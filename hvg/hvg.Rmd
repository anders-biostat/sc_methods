---
title: "Find Highly Variable Genes"
output:
  html_document:
    df_print: paged
    toc: yes
    number_sections: true
    toc_float: true
    self_contained: true
    mathjax: default
    code_download: true
---




# Load Packages

```{r}
library(ggplot2)
library(pbmcapply) # parallelization
library(quantreg)  # quantile regression with rq.wfit
library(parallelDist)
```



# Load data
On the CITEseq data, we play around with HVGs (highly variable genes), i.e.
superpoissonian genes.

```{r loadCITE}
citeDIR <- "~/sds/sd17l002/p/scRNAseq_datasets/CITEseq_NatMethods_2017/data/"
rna_file <- paste0(citeDIR, "GSE100866_CBMC_8K_13AB_10X-RNA_umi.csv.gz")
adt_file <- paste0(citeDIR, "GSE100866_CBMC_8K_13AB_10X-ADT_umi.csv.gz")

rawC <- as.matrix(read.csv(gzfile(rna_file), row.names = 1))
prot <- as.matrix(read.csv(gzfile(adt_file), row.names = 1))

# exclude mouse cells:
is_mouse <- colSums(rawC[grepl("MOUSE", rownames(rawC)),]) / colSums(rawC) > .1
rawC <- rawC[grepl("HUMAN", rownames(rawC)), ! is_mouse]
prot <- prot[, ! is_mouse]
```




# RNA data: normalization

```{r}
normC_anscombe <- apply( rawC, 2, function(col) {
  (sqrt(col+3/8)-sqrt(3/8)) / sqrt(sum(col))
})

normC <- log1p(t( t(rawC) * 2300 / colSums(rawC)))

```




# Protein data: normalization and Cell identities
```{r}
# simplifying Seurat's code to compute Centered Log Ratio (CLR):
norm_prot <- apply(prot, 1, function(x) {
  log1p( (x) /
                          (exp(sum(log1p((x)[x > 0]), na.rm = TRUE)/length(x + 1))) ) })


 # exclude NK and monos:
  plot(norm_prot[, "CD11c"], norm_prot[, "CD56"], pch =20, cex=.1, 
       main = "CITEseqs protein data\nto exclude NKs (CD56) and myeloid cells (CD11c)")
  lin_neg <- norm_prot[, "CD56"] < 1.1   & norm_prot[, "CD11c"] < 1 
 
# exclude erythrocytes. No good protein marker available, we use RNA for it:
  not_ery <- normC["HUMAN_HBB", ]  < .05  &
             normC["HUMAN_HBG2", ] < .05 &
             normC["HUMAN_HBA1", ] < .04 
# same for platelets / megakaryocytes:
  not_platelet <- normC["HUMAN_GP9", ] < .005 &
                  normC["HUMAN_PF4", ] < .005 &
                  normC["HUMAN_PPBP", ] < .005 
  
  
# B and T cells:
  has_B_markers <- norm_prot[, "CD19"] > 2  &  norm_prot[, "CD3"] < .5
  has_T_markers <- norm_prot[, "CD19"] < 1.5  &  norm_prot[, "CD3"] > 1
  plot(norm_prot[, "CD3"], norm_prot[, "CD19"], pch =20, col =
         rgb(has_B_markers, has_T_markers, .7 *lin_neg),
       main = "B and T cells in red and green\nCD56-CD11c- in blue\nPick cells with mixture colors")

  
    
  isBcell <- has_B_markers & lin_neg & not_ery & not_platelet
  isTcell <- has_T_markers & lin_neg & not_ery & not_platelet

  
  
  has_CD4 <- norm_prot[, "CD4"] > 1  &  norm_prot[, "CD8"] < 1
  has_CD8 <- norm_prot[, "CD4"] < .5  &  norm_prot[, "CD8"] > 3
  isCD4Tcell <- isTcell & has_CD4
  isCD8Tcell <- isTcell & has_CD8
   
  plot(norm_prot[, "CD4"], norm_prot[, "CD8"], pch=20, cex=.4,
       col = rgb(0, isCD4Tcell, isCD8Tcell), 
       main = "CD4 and CD8 T cells\nselected amongst T cells picked above")  
  
  
Ts <- isCD4Tcell | isCD8Tcell
```




# VMRstats function
```{r}
## bug report for below function(s):
  #
  #  This function is terrible coding practice; rewrite it completely!
  #
  #  rowMeans(rawC) and allM is redundant computation, that's uncritical but not elegant
  #
  #  everytime you run it, different genes get selected due to stochastic poisson.
  #  We'll probably find an analytic solution eventually anyways that should fix it.

                                                  ##

VMRstats <- function(mat = rawC, nbin_qreg = 75) {
  cs <- colSums(mat) 
  
  # Simon wants to filter out low-abundance genes. A very conservative cutoff is
  # the mean resulting from 1 small cell (nUMI = 500) having an UMI of 1, the rest 0s:
  threshold <- mean(c(1/500,  rep(0, ncol(mat)-1)))
    print("Simulate Poisson Counts...")
  # estimate each gene's true mean and var - for this we normalize with library size:
    normC <- t( t(mat) / cs)
    normC <- normC[rowMeans(normC) > threshold,]
    allM <- apply(normC, 1, mean)
    allV <- apply(normC, 1, var)
  
  
  # simulate poisson raw counts relevant to our data (i.e. using allM):
    nc <- ncol(mat)
    pCounts <- do.call(rbind,
                       pbmclapply(allM, function(mu) {rpois(nc, lambda = mu * cs)},
                                  mc.cores=4) )
    # superfluous, here because I used to punch these into Seurat:
    rownames(pCounts) <- paste0("poisson_", 1:nrow(pCounts))
    colnames(pCounts) <- paste0("Cell_", 1:ncol(pCounts))
  
    print("Normalize Poisson Counts...")
  # as with real counts:
    normP <- t( t(pCounts) / cs)
    allMP <- apply(normP, 1, mean)
    allVP <- apply(normP, 1, var)
    # x,y for ease of typing. threshold also prevents -Inf values, defensive progr.:
    x <- log(allMP[allMP > threshold])
    y <- log(allVP[allMP > threshold] / allMP[allMP > threshold])
  
  print("Fit VMR-mean relationship...")
  # going over all x takes long, so we fit the VMR-mean relationship in bins:
  xrange <- range(x) # log(c(max(1e-9, min(allM), min(allMP)), max(max(allM), max(allMP))))
  bin_x <- seq(xrange[1], xrange[2], length.out = nbin_qreg)
  # random poisson by chance might not cover the extreme values:
  bin_x <- c(log(min(allM)), bin_x, log(max(allM)))
  
  # Simon's local quantile fit (faster using matrix notation, `mm`)
  mm <- cbind( Intercept = 1, X = x, X2 = x^2 )
  yfit <- do.call(rbind, pbmclapply( bin_x, function(xp)
    {
        fit        <- rq.wfit( mm, y, .75, dnorm( x, mean=xp, sd=2 ) )
        fit_median <- rq.wfit( mm, y, .5, dnorm( x, mean=xp, sd=2 ) )
        c(bin_x = xp,
          q75= fit$coefficients %*% c( 1, xp, xp^2 ),
          median = fit_median$coefficients %*% c( 1, xp, xp^2 ))
    } )  )
  
  # interpolate Poisson median and quantile for each gene: 
  all_y<- do.call(data.frame,
                 c( 
                   poisson_medians <- approx(x = yfit[, "bin_x"],
                                             y = yfit[, "median"],
                                             xout = log(allM)),
                   poisson_q75     <- approx(x = yfit[, "bin_x"],
                                             y = yfit[, "q75"],
                                             xout = log(allM)) ) )
  # summarize final result:
  df <- data.frame(
      log_Gene_Mean         = all_y[, 1],
      log_Gene_VMR          = log(allV / allM),
      log_PoissonVMR_median = all_y[, 2],
      log_PoissonVMR_q75    = all_y[, 4],
      IQR_of_logged_VMR         = 2 * (all_y[, 4] - all_y[, 2]),
      row.names = rownames(all_y)
    )
  df$aboveP  <- (df$log_Gene_VMR - df$log_PoissonVMR_median) /
    df$IQR_of_logged_VMR
  df <- df[order(df$aboveP, decreasing = TRUE), ]
  
  return(df)
} 

vmrPlot <- function(vmrstat) {
    plot(vmrstat$log_Gene_Mean, vmrstat$log_Gene_VMR, pch = 20, cex=.1, 
         xlab = "log( estim_mean )", ylab = "log( estim_var / estim_mean )")
    points(vmrstat$log_Gene_Mean,vmrstat$log_PoissonVMR_median, pch = ".", col = "orange")
    points(vmrstat$log_Gene_Mean,vmrstat$log_PoissonVMR_q75, pch = ".", col = "orange")
  }
  
vmrPoints <- function(vmrstat, sel) {# sel is a true/false boolean vector selecting genes
    points(vmrstat[sel, "log_Gene_Mean"], vmrstat[sel, "log_Gene_VMR"],
           col="red", pch=20)
    }

```

# Other Functions
```{r}

anscNorm <- function(rawCounts) {
      apply( rawCounts, 2, function(col) {
                     (sqrt(col+3/8)-sqrt(3/8)) / sqrt(sum(col)) } )
} 




getUpper <- function(mat) mat[upper.tri(mat, diag = FALSE)]
 

sqDeviation <- function(PCvector) {
  # For a given numerical vector, returns a matrix
  # with squared deviations of amongst all its elements.
  # Taking this matrix's square root is the 1D Euclidean Distance.
  sapply(PCvector, function(entry) (entry - PCvector) * (entry - PCvector))
 }
 
NNindices <- function(dists = as.matrix(dist(matrix(1:9,3))), n_neighbors = 10) {
  # get n nearest neighbors' indices. 
  NN = t(apply(dists, 1,   
           function(ds) head(order(ds), n = n_neighbors)
             ) )
  return(NN)
 }

 
  
stratNs <- function(NN, ct) {
# Stratify neighborhoods: for each cell (row) in NN, count neighborhoods it 
# belongs to.
  # NN: is a matrix with one row per cell (neighborhood), filled with the
  #     indices of that cell's
  #     nearest neighbors (e.g. top 10, top 30, ...).
  # ct: is a character vector with the celltype of each cell, in the same order
  #     as the rows in NN.
  # value: 
  cbind(ID = 1:nrow(NN), 
    sapply(unique(ct), function(type)
      { as.numeric( table( factor( NN[ ct == type,  ], levels = 1:nrow(NN) ) ) ) 
    })
  )
}


```





# Plausible gene means and sizefactors

Plausible gene means inspired by CITEseq dataset
When simulating scRNAseq data
(e.g. as Poisson counts for homogeneous populations),
it is useful to cover biologically relevant ranges of gene expression magnitude.
Here are "plausible gene means", meaning they could actually be raw counts of
~ 1000 genes observed for a cell with average library size
(libsize here means nUMI aka colSums)
in a 10X scRNAseq dataset.
These can be used directly as Poisson rates to simulate data, or they can be
adjusted with sizefactors centered around 1 (1 representing the cell with average sequencing depth).

```{r plausible_means}
plausible_means <- c(  
    runif(575, min = .5, max = 1),
    runif(267, min = 1,  max = 2),
    runif(190, min = 2,  max = 10),
    runif(70, min = 10, max = 40)  )
# compare to citeseq by loading rawC as in hvg.Rmd, followed by:
# cs <- colSums(rawC)
# normC <- t( t(rawC) *mean(cs)/ cs)
# table(round(rowMeans(normC)))
# rms <- rowMeans(normC)
# plot(table(round(plausible_means, digits = 1 )), main = "Simulated means", ylab = "Number of Genes")
# plot(table(round(rms[rms < 40 & rms > .5], digits = 1)), main = "CITEseq means (> .5 UMIs)", ylab="Number of Genes")
```

Turns out using the f-Distribution we can simulate acceptable sizefactors
around 1 for scRNAseq data.
```{r plausible_sf}
plausible_sf <- rf(1000,20,22, 0)
# This is where the f-Distribution idea is motivated from:
#  # load the citeseq dataset first, then you can get the sizefactors:
#  sf <- colSums(rawC) / mean(colSums(rawC))
#  hist(sf, breaks = 0:150 * .1)
#  
#  # let's fit an f-Distribution to that. With guessing parameters, it'd look like this:
#  x <- seq(0, 20, length.out = 1000)
#  hist(sf, breaks = 0:150 * .1, freq  = F); lines(x, df(x, df1=3, df2=2, ncp = 0), col="blue")
#  
#  # lets find maximum likelihood estimates:
#  optim(c(2, 5, 3), function(params) -sum(df(sf, df1 =  params[1],
#                                             df2 =  params[2], ncp = 
#                                               params[3], log = T) ) )
#  
#  hist(sf, breaks = 0:150 * .1, freq  = F); lines(x, df(x,20,22, 0), col="blue")
```






# Savepoint

```{r}
# save.image(file = "/home/felix/PhDother/scAnalysis/sc_methods/hvg/savepoint/start_hvg.rda")
library(ggplot2)
library(pbmcapply) # parallelization
library(quantreg)  # quantile regression with rq.wfit
library(pheatmap)
library(parallelDist)
library(tidyverse)
library(Rtsne)
load(file = "/home/felix/PhDother/scAnalysis/sc_methods/hvg/savepoint/start_hvg.rda")


normC <- log1p(t( t(rawC) / colSums(rawC)))
```






# Curse of dimensionality
Here I will pick more and more HVGs and/or PCs to show the distances within
and between CD4 and CD8 T cells become better and worse.


## Colour scheme
Useful to pick colors for CD4 and CD8 
once and then use them consistently throughout this script.
```{r}
library(RColorBrewer)
dark2 <- brewer.pal(8, "Dark2")
myCols <- scale_colour_manual(values = c("CD4" = dark2[2], "CD8" = dark2[5]))


# for expression heatmap:
colPal <- colorRampPalette(rev(brewer.pal(5, "RdBu")))(10)
```

## Select cells (250+250)
I want the groups (CD4 and CD8) to be even, so I draw 250 CD4 T cells (out of
almost 3000). Note that CD8 T  cells **systematically have higher library sizes**
than CD4 T cells. To investigate the effect of this, I draw twice: once randomly,
and once so that libsizes roughly match.

Convention: I'll always list the 250 CD4 cells first, followed by 250 CD8 T cells:
```{r}
ct <- c( rep("CD4", 250), rep("CD8", sum(isCD8Tcell)) )
```


1. Randomly draw 250 CD4 cells:
```{r}
selectFew <- c(sample(which(isCD4Tcell), 250), which(isCD8Tcell))
# order by celltype, then by libsize:
selectFew <- selectFew[ order(ct, colSums(rawC[, selectFew]), decreasing = F) ]
```
2. match library sizes of selected CD4 cells to the 250 CD8 T cells
in the CITEseq dataset:
```{r}
cs <- colSums(rawC)
# we mask all non-CD4 cells by adding huge numbers:
dummycs <- cs; dummycs[! isCD4Tcell ] <- dummycs[ !isCD4Tcell] + 32000
# for each CD8 cell, we'll put the CD4 cell with most similar library size
# into the first column of the `simil` matrix, the second most similar into
# the second column, etc. The first column has duplicates, so them alone would
# not be 250 cells. But taking the first two columns together we have enough
# unique CD4 T cells to draw 250 from them:
simil <- t(sapply(cs[isCD8Tcell], function(libsize) {
  order(abs(libsize - dummycs), decreasing = FALSE)
}))
ids <- sample( unique(as.vector(simil[, 1:2])),  250 )
ids <- ids[order(colSums(rawC[, ids]), decreasing = T)] # sorted by libsize can help to find artifacts
selecteven <- c(ids, which(isCD8Tcell))
```

You see here that it makes quite some difference:
```{r cd4_cd8_libsizes, fig.height=6, fig.width=9}
plot_grid(
   data.frame(libsize = colSums(rawC[, selectFew]), Celltype = ct) %>%
  ggplot(aes(Celltype, libsize, col = Celltype))+
  geom_violin(draw_quantiles = c(.5)) +
  geom_jitter() + ylim(c(0,8500)) +
  ggtitle("250 random CD4 T cells\nRcode: selectFew") +
  myCols + theme(legend.position = "none"),
  
  data.frame(libsize = colSums(rawC[, selecteven]), Celltype = ct) %>%
  ggplot(aes(Celltype, libsize, col = Celltype))+
  geom_violin(draw_quantiles = c(.5)) +
  geom_jitter() +ylim(c(0,8500)) +
    ggtitle("Samples for even library sizes\nRcode: selecteven") +
    myCols + theme(legend.position = "none"),
  
  ncol = 2
)
```



## Filter Genes [vfew, veven]
I have observed that genes that are only >0 in 1 or 2 cells are captured by
PC1 after regressing library size, simply because regression changes the 0s
into a downward-tilted line (see powerpoint presentation).

To be fair, I'll repeat these findings after filtering genes by their
variance/expression, for this I use our VMR on raw counts:

```{r tcellVMR, results= hide}
vfew <- VMRstats(rawC[, selectFew])
veven<- VMRstats(rawC[, selecteven])

universe <- unique(
  c(rownames(vfew)[vfew$aboveP > 1 & vfew$log_Gene_Mean > -12] ,
    rownames(veven)[veven$aboveP > 1 & veven$log_Gene_Mean > -12] ) 
  )
```





## Normalize data [normFew, normeven]
I will compare euclidean distances with increasing numbers of features: 
  normalized gene expression directly, 
  PCs on normalized genes, 
  PCs on regressed (nUMI, ...) genes.

Normalize by hand (selectFew and selecteven):
```{r}
normFew  <- log( t(t(rawC[, selectFew]) * median(colSums(rawC[, selectFew])) /
                  colSums(rawC[, selectFew])  ) + 1 ) 

normeven <- log( t(t(rawC[, selecteven]) * median(colSums(rawC[, selecteven])) /
                  colSums(rawC[, selecteven])  ) + 1 ) 
```


Normalize with seurat (selectFew):
```{r}
library(Seurat)
s <- CreateSeuratObject(rawC[, selectFew])

mito <- rownames(s@data)[grepl("HUMAN_MT-", rownames(s@data))]
s   <- AddMetaData(s,
                   metadata = colSums(s@raw.data[mito, ]) / colSums(s@raw.data),
                   col.name = "percent.mito")
s <- NormalizeData(s)
s <- FindVariableGenes(s)


##    use genes after filtering only
noR <- ScaleData(s, genes.use = universe, vars.to.regress = NULL)
R   <- ScaleData(s, genes.use = universe, vars.to.regress = 
                   c("nUMI", "percent.mito"))


# extract normalized counts:
seurat_norm <- noR@scale.data
seurat_regr <-   R@scale.data
```

## Compute PCA
I've seen that regression introduces artifactual genes into PC1 if you use
all genes, so here I'll only use filtered genes ('universe'):
```{r}
pcFew  <- prcomp((t(normFew[universe, ])), center = TRUE, scale. = FALSE)
hvg1 <- rownames(vfew)[vfew$aboveP > 4]
hvg1 <- hvg1[hvg1 %in% universe]
pcFew_hvg1  <- prcomp((t(normFew[ hvg1, ])), center = TRUE, scale. = FALSE)

pc_seurat_norm <- prcomp(t(noR@scale.data[universe, ]),
                         center = TRUE , scale. = FALSE)

pc_seurat_regr <- prcomp(t(  R@scale.data[universe, ]),
                         center = TRUE , scale. = FALSE)

# for even library sizes:
pceven<- prcomp((t(normeven[universe, ])), center = TRUE, scale. = FALSE)
```

## [function:] correctNeighbors function as readout
```{r}

correctNeighbors <- function(prcompObj = NULL,
                             celltypeInfo = NULL,
                             n_neighbors = 31) {
  # initialize variables:
  sqDs <- matrix(0, nrow = nrow(prcompObj$x), ncol = nrow(prcompObj$x))    
  results <- tibble()
  # loop through all PCs:
  for(p in 1:ncol(prcompObj$x)) {
   if(p %% 10 == 0) print(p)
   sqDs <- sqDs + sqDeviation(prcompObj$x[, p])
   nns  <- NNindices(as.matrix(sqrt(sqDs)), n_neighbors = n_neighbors)
   correct_assignments <- apply(nns, 1, function(ids)
        sum(celltypeInfo[ids[2:ncol(nns)]] == celltypeInfo[ids[1]]))
   tmp <- tibble(  correct_assignments, Celltype = celltypeInfo,
                         PCs = p) %>% # number of PC
     group_by(Celltype, PCs) %>%
     summarise(Mean_correctNeighbors = mean(correct_assignments))  
   results <- bind_rows(results, tmp) 
  } 
  return(results)
}
```

## [function:] sepScores, ct_boxplot
```{r}
sepScores <- function(cellInfo = factor(sort(rep(1:2, 250))),
                      pca, pcs=1:6, ...) {
 # compute separation scores:
  is_A <- cellInfo == unique(cellInfo)[1] 
  sepScores <- sapply(pcs, function(i) {
        pc <- pca$x[, i] 
        pc_sep <- ( mean(pc[is_A]) - mean(pc[!is_A]) )  /  sum( sd(pc[is_A]), sd(pc[!is_A])   )
        pc_sep
        })
  names(sepScores) <- paste0("PC", pcs)
 # outut a plot:
  ys <- max(1.5, max(abs(sepScores)))
  plot(pcs, abs(sepScores), ylim = c(-.1, ys), pch=20, ...);
  abline(h=0, col = adjustcolor("black", alpha.f = .4))
  # end
 return(sepScores)
}

ct_boxplot <- function(cellInfo = factor(sort(rep(1:2, 250))),
                       pca, pcs=1:6, plot_cols = 3, plot_rows=2) {
     par(mfrow = c(plot_rows, plot_cols))
    sapply(pcs, function(i) {
      scores <- pca$x[, i]
      plot(cellInfo, scores, main = paste0("PC", i))
    }); par(mfrow = c(1, 1))
}
```




## Playground NN readout
I change the below codechunk again and again to generate plots - these 
plots are pasted into a powerpoint presentation.


```{r message=FALSE, warning=FALSE, silent=TRUE}
tbl_noR <- correctNeighbors(pc_seurat_norm, ct, 31)
tbl_R <- correctNeighbors(pc_seurat_regr, ct, 31)

tbl_1 <- tbl_R
```
Plot correctly assigned nearest neighbors:
```{r fig.height=3.5, fig.width=10}
cowplot::plot_grid(
     ggplot(tbl_1) + geom_point(aes(PCs, Mean_correctNeighbors, col=Celltype))+
     ylim(0, 30) +  myCols + theme(legend.position = "top"),
     
     data.frame(libsize = colSums(rawC[, selectFew]), Celltype = ct) %>% 
     ggplot(aes(Celltype, libsize, col = Celltype))+geom_violin(draw_quantiles = c(.5)) +
     geom_jitter(size=.3)+myCols + theme(legend.position = "none"),
     
     ncol = 2, 
     rel_widths = c(3,1)
)
```




## Effect of regression
I want to look Gene-Libsize scatter plots, and start with genes that are contributing
most to PCs and thus will influence all further analysis.
```{r}
s_R <- RunPCA(R, pc.genes = universe )

Gs <- 
  apply(s_R@dr$pca@gene.loadings, 2, function(PC) {
    order(abs(PC), decreasing = TRUE)[1:5] 
  })


PC = 2
topgenes <- rownames(s_R@dr$pca@gene.loadings)[Gs[, PC]]

 data.frame(libsize = colSums(R@raw.data),  t( R@raw.data[topgenes, ] )) %>%
   gather(key = "Gene", value ="Expression", -libsize) %>% 
   ggplot() + geom_point(aes(libsize, Expression), size=.4) + facet_wrap( ~ Gene, scale = "free_y" )
```

## Which genes to regress?
Lowly expressed genes will get heavily biased by regressing out nUMI, because
all zeros end up as a straight line with negative slope.
Perhaps we can spot and exclude these genes, and perhaps even genes where
the dependency is real but non-linear, by comparing Pearson and Spearman 
correlations:

```{r}
pcor <- cor(t(s@raw.data), colSums(s@raw.data) )
scor <- cor(t(s@raw.data), colSums(s@raw.data), method = "spearman" )

plot(scor[,1], pcor[,1], pch=20, cex=.4, col = adjustcolor("black", alpha.f = .1),
     main = "Cor(Gene_x, nUMI)\nred: Genes with exactly 1 UMI in entire dataset\n10X data (500 T cells from CBMCs)",
     xlab = "Spearman Correlation (with nUMI)", ylab = "Pearson Correlation (with nUMI")
oneExpressor <- rowSums(s@raw.data) == 1
points(scor[oneExpressor,1], pcor[oneExpressor,1], pch = 20, col="red")



```














# ---------------------------------------
# Old parts of script, delete soon
# ---------------------------------------

## B vs T cells: all ok

```{r}
# cell types in alphabetical order (otherwise doesn't match ordered selectBT):
ctBT <- c(  rep("B", sum(isBcell)),  rep("T", 500) )
selectBT <- c(which(isBcell), sample(which(Ts), 500) )
selectBT <- selectBT[ order(ctBT, colSums(rawC[, selectBT]))]


normBT <- log( t(t(rawC[, selectBT]) * median(colSums(rawC[, selectBT])) /
                  colSums(rawC[, selectBT])  ) + 1 )

# preselect genes very coarsely for speedup in PCA:
vmrBT <- VMRstats(rawC[, selectBT])
coarse_pickBT <- rownames( vmrBT[vmrBT$log_Gene_VMR > vmrBT$log_PoissonVMR_median,] )

pcBT<- prcomp((t(normBT[ coarse_pickBT, ])), center = TRUE, scale. = FALSE)
```


```{r}
 sqDs <- matrix(0, nrow = nrow(pcBT$x), ncol = ncol(pcBT$x))    
 BT_DF <- tibble()
for(p in 1:ncol(pcBT$x)) {
 if(p %% 10 == 0) print(p)
 sqDs <<- sqDs + sqDeviation(pcBT$x[, p])
 nns  <- NNindices(as.matrix(sqrt(sqDs)), n_neighbors = 31)
 # within NNs we can now ask for interesting statistics:
 correct_assignments <- apply(nns, 1, function(ids)
      sum(ctBT[ids[2:ncol(nns)]] == ctBT[ids[1]]))
 tmp <- tibble(  correct_assignments, Celltype = ctBT,
                       PCs = p) %>% # number of PC
   group_by(Celltype, PCs) %>%
   summarise(Mean_correctNeighbors = mean(correct_assignments))  
 BT_DF <<- bind_rows(BT_DF, tmp) 
}

p_BT<- ggplot(BT_DF) + geom_point(aes(PCs, Mean_correctNeighbors, colour = Celltype))
p_BT 

BTnn10 <- NNindices( as.matrix(dist(pcBT$x[, 1:10])), n_neighbors = 31)
as.data.frame(stratNs(BTnn10, ctBT)) %>%
  gather("Ntype", "Neighborhoods", -ID) %>%
  ggplot()+geom_point(aes(ID, Neighborhoods, col = Ntype))

BTnn400 <- NNindices( as.matrix(dist(pcBT$x[, 1:400])), n_neighbors = 31)
as.data.frame(stratNs(BTnn400, ctBT)) %>%
  gather("Ntype", "Neighborhoods", -ID) %>%
  ggplot()+geom_point(aes(ID, Neighborhoods, col = Ntype))


plot( as.numeric(table(BTnn10)), pch = 20, col = 3 + (ctBT=="T"),
      xlab = "B cells (green), T cells (blue), ordered by library size",
      ylab = "Dots: N30 neighborhoods. Line: libsize (A.U.)", 
      main = "265 B cells and 500 T cells\nDistances on first 10 PCs")
lines( colSums(rawC[, selectBT]) / 100)
```













## CD4 vs CD8 T cells - no regressing

For an increasing number of genes, the below chunk computes euclidean distance
between cells. Depending on which line is commented in or out, it computes it
either directly on normalized expression or on the first 30 PCs. 
Overall we see that varying number of input genes into PCA does not change much:
```{r}
sel <- c(which(isCD4Tcell), which(isCD8Tcell))
identities <- c( rep("CD4", sum(isCD4Tcell)), rep("CD8", sum(isCD8Tcell)) )

vmr <- VMRstats(rawC[, sel] )
norm2 <- log( t(t(rawC[, sel]) * median(colSums(rawC[, sel])) /
                  colSums(rawC[, sel])  ) + 1 )



for(ng in 2^(5:14)) {
  print(ng)
  fast <- ifelse(ng > 500, TRUE, FALSE)
  #########  eucl. distance on PCs:
  # d <- pcDist(norm2[rownames(head(vmr, n = ng)), ], do.fast = fast)
  #########  eucl. distance directly on gene expression (very poor performance):
  d <- parDist(t(norm2[rownames(head(vmr, n = ng)), ]) )
  
  nns <- NNindices(as.matrix(d), n_neighbors = 11 )
  correct_assignments <- apply(nns, 1, function(ids)
    sum(identities[ids[2:11]] == identities[ids[1]]))
  cdf <- (cbind.data.frame(correct_assignments, identities))
  cdf$correct_assignments <- factor(cdf$correct_assignments, levels = 0:10)
  
  cd4 <-  data.frame(table(cdf[cdf$identities == "CD4",
                        "correct_assignments"]), Celltype = "CD4")
  cd8 <-  data.frame(table(cdf[cdf$identities == "CD8",
                        "correct_assignments"]), Celltype = "CD8")
  cdf <- data.frame(rbind(cd4, cd8),
             prop = c( cd4$Freq / sum(identities == "CD4"),
                       cd8$Freq / sum(identities == "CD8") ))
  
    
  # output nice plots:
   png(paste0(
        "/home/felix/Dropbox/PhD/Meetings-Talks-etc/labM_Anders/7_hvg_vmr_CD4CD8/",
        "lognormalization_euclidean/" ,      
        "NNs_PCs1to30_hvgs1to", ng, ".png"), width = 1000, height = 600, units = "px")
  # par(mfrow = c(1, 2)) 
  # plot(vmr$log_Gene_Mean, vmr$log_Gene_VMR, pch = 20, cex=.1, col = "#00000040",
  #      ylab = "log_VMR ( log(var/mu)", xlab = "Average expression ( log(mu) )",
  #      main = ng) 
  # points(vmr$log_Gene_Mean[1:ng], vmr$log_Gene_VMR[1:ng], pch=20, col = "red")
   
  print(ggplot(cdf) + geom_bar(aes(Var1, prop, fill = Celltype),stat = "identity", position = "dodge") + xlab("NNs of same cell type")+ ylab("Proportion") +
    ggtitle(ng))
   # deprecated plot:
   # hist(ns, xlab = "CD4 T cells amongst 100 NNs", main = paste0("CD4 and CD8 T cells\n", ng),
   #      breaks = -.5+0:101, ylim = c(0, 800) )
   dev.off()
 
}
```

## CoD on PCs (3000 cells)
Curse of dimensionality (CoD): as we increase the PCs, does euclidean distance
deteriorate?

```{r}
sel <- c(which(isCD4Tcell), which(isCD8Tcell))
identities <- c( rep("CD4", sum(isCD4Tcell)), rep("CD8", sum(isCD8Tcell)) )

vmr <- VMRstats(rawC[, sel] )
norm2 <- log( t(t(rawC[, sel]) * median(colSums(rawC[, sel])) /
                  colSums(rawC[, sel])  ) + 1 )


# compute PCA on (almost) all genes (minus exremely noisy ones, for speed):
  coarse_pick <- rownames( vmr[vmr$log_Gene_VMR > vmr$log_PoissonVMR_median,] )
  pc  <- prcomp((t(norm2[ coarse_pick, ])), center = TRUE, scale. = FALSE)
  # I used to scale and center here but scaling is debatable.
  # Note that irlba's implementation in Seurat does both.
```


```{r}
sqDs <- matrix(0, nrow = nrow(pc$x), ncol = ncol(pc$x))    
T_DF <- tibble()
for(p in 1:1000) {
 sqDs <<- sqDs + sqDeviation(pc$x[, p])

 if(p %% 50 ==0 | p < 50) { 
   print(p)
  nns  <- NNindices(as.matrix(sqrt(sqDs)), n_neighbors = 31)
  correct_assignments <- apply(nns, 1, function(ids)
      sum(identities[ids[2:ncol(nns)]] == identities[ids[1]]))
  tmp <- tibble(  correct_assignments, Celltype = identities,
                       PCs = p) %>% # number of PC
   group_by(Celltype, PCs) %>%
   summarise(Mean_correctNeighbors = mean(correct_assignments))  
 T_DF <<- bind_rows(T_DF, tmp) 
 }
}

 
p_T <- ggplot(T_DF) + geom_point(aes(PCs, Mean_correctNeighbors, colour = Celltype)) +
  ggtitle("2768 CD4 T cells, 250 CD8 T cells")
```
There are roughly 10x more CD4 
than CD8 T cells. So without any transcriptomic differences, we'd expect the
red line around 29 and the blue line around 1. So I suppose this is not bad.

```{r}
Tnn8<- NNindices( as.matrix(dist(pc$x[, 1:8])), n_neighbors = 31)
as.data.frame(stratNs(Tnn8, identities)) %>%
  gather("Ntype", "Neighborhoods", -ID) %>%
  ggplot()+geom_point(aes(ID, Neighborhoods, col = Ntype))



Tnn250<- NNindices( as.matrix(dist(pc$x[, 1:250])), n_neighbors = 31)
as.data.frame(stratNs(Tnn250, identities)) %>%
  gather("Ntype", "Neighborhoods", -ID) %>%
  ggplot()+geom_point(aes(ID, Neighborhoods, col = Ntype))
```



## CoD on PCs (250 + 250 cells)


### More PCs, popular neighbors and nUMI
We can see that at one point, the more PCs we include in the distance computation,
the less resolution we have to separate CD4 and CD8 T cells.
```{r}
sqDs <- matrix(0, nrow = nrow(pcFew$x), ncol = ncol(pcFew$x))    
few_DF <- tibble()
for(p in 1:500) {
 if(p %% 10 == 0) print(p)
 sqDs <<- sqDs + sqDeviation(pcFew$x[, p])
 nns  <- NNindices(as.matrix(sqrt(sqDs)), n_neighbors = 31)
 correct_assignments <- apply(nns, 1, function(ids)
      sum(ctFew[ids[2:ncol(nns)]] == ctFew[ids[1]]))
 tmp <- tibble(  correct_assignments, Celltype = ctFew,
                       PCs = p) %>% # number of PC
   group_by(Celltype, PCs) %>%
   summarise(Mean_correctNeighbors = mean(correct_assignments))  
 few_DF<<- bind_rows(few_DF, tmp) 
}

p_few <- ggplot(few_DF) + geom_point(aes(PCs, Mean_correctNeighbors, colour = Celltype))+
    ggtitle("(Almost) all genes -> PCA on 250 CD4 and 250 CD8 T cells\n-> eucl. distance on n PCs") + ylim(c(0,30))
p_few + geom_vline(xintercept = 9)  


tsneFew_9 <- Rtsne(pcFew$x[, 1:9])
plot(tsneFew_9$Y, pch =20, col = 2+ .5 * incFew, main = "250/250 CD4/CD8 T cells\nPCs 1-9")

tsneFew_25 <- Rtsne(pcFew$x[, 1:25])
plot(tsneFew_25$Y, pch =20, col = 2+ .5 * incFew, main = "250/250 CD4/CD8 T cells\nPCs 1-25")

tsneFew_200 <- Rtsne(pcFew$x[, 1:200])
plot(tsneFew_200$Y, pch =20, col = 2+ .5 * incFew, main = "250/250 CD4/CD8 T cells\nPCs 1-200")
```



```{r}
Fewnn4 <- NNindices( as.matrix(dist(pcFew$x[, 1:4])), n_neighbors = 31)
as.data.frame(stratNs(Fewnn4, ctFew)) %>%
  gather("Ntype", "Neighborhoods", -ID) %>%
  ggplot()+geom_point(aes(ID, Neighborhoods, col = Ntype))


Fewnn100 <- NNindices( as.matrix(dist(pcFew$x[, 1:100])), n_neighbors = 31)
as.data.frame(stratNs(Fewnn100, ctFew)) %>%
  gather("Ntype", "Neighborhoods", -ID) %>%
  ggplot()+geom_point(aes(ID, Neighborhoods, col = Ntype))

data.frame(celltype = ctFew, libsize = colSums(rawC[, selectFew])) %>% ggplot(aes(celltype, libsize)) + geom_violin() + geom_jitter()
```

We see that deeply sequenced cells (with high nUMI) are popular neighbors.
In fact, > 60 % of cells are no one's nearest neighbors, whereas the one or two
dozen cells with highest sequencing coverage are neighbors of 80 or 90 % of the cells.















### Regress out nUMI

```{r}
seur <- CreateSeuratObject(rawC[, selectFew],
         meta.data = data.frame(Celltype = ctFew,
                                row.names = colnames(rawC[, selectFew])))
seur@data <- normFew
# seur@var.genes <- coarse_pickFew
seur <- ScaleData(object = seur, vars.to.regress = c("nUMI"),
                  genes.use = coarse_pickFew, model.use = "negbinom")
pcReg_nb  <- prcomp((t( seur@scale.data) ), center = TRUE, scale. = FALSE)

seur <- ScaleData(object = seur, vars.to.regress = c("nUMI"),
                  genes.use = coarse_pickFew, model.use = "linear")
pcReg_l <- prcomp((t( seur@scale.data) ), center = TRUE, scale. = FALSE)

# tReg <- Rtsne(pcReg$x[, 1:150])

 sqDs <- matrix(0, nrow = 500, ncol = 500)    
 nbinom_DF <- tibble()
for(p in 1:500) {
 if(p %% 10 == 0) print(p)
 sqDs <<- sqDs + sqDeviation(pcReg_nb$x[, p])
 nns  <- NNindices(as.matrix(sqrt(sqDs)), n_neighbors = 31)
 correct_assignments <- apply(nns, 1, function(ids)
      sum(ctFew[ids[2:ncol(nns)]] == ctFew[ids[1]]))
 tmp <- tibble(  correct_assignments, Celltype = ctFew,
                       PCs = p) %>% # number of PC
   group_by(Celltype, PCs) %>%
   summarise(Mean_correctNeighbors = mean(correct_assignments))  
 nbinom_DF <<- bind_rows(nbinom_DF, tmp) 
}

 p_nbinom <- ggplot(nbinom_DF) + geom_point(aes(PCs, Mean_correctNeighbors, colour = Celltype))+ ylim(c(0,30))

 
 
 
sqDs <- matrix(0, nrow = 500, ncol = 500)    
linear_DF <- tibble()
for(p in 1:500) {
 if(p %% 10 == 0) print(p)
 sqDs <<- sqDs + sqDeviation(pcReg_l$x[, p])
 nns  <- NNindices(as.matrix(sqrt(sqDs)), n_neighbors = 31)
 correct_assignments <- apply(nns, 1, function(ids)
      sum(ctFew[ids[2:ncol(nns)]] == ctFew[ids[1]]))
 tmp <- tibble(  correct_assignments, Celltype = ctFew,
                       PCs = p) %>% # number of PC
   group_by(Celltype, PCs) %>%
   summarise(Mean_correctNeighbors = mean(correct_assignments))  
 linear_DF <<- bind_rows(linear_DF, tmp) 
}

 p_linear <- ggplot(linear_DF) + geom_point(aes(PCs, Mean_correctNeighbors, colour = Celltype)) + ylim(0,30)

```


```{r}
d <- dist(pcReg$x[, 1:20])
nns <- NNindices(d, n_neighbors = 31)

plot(1:500, table(nns), main = "How often is each cell amongst 30 NNs of all other cells?\n(1:250 are CD4, 251:500 are CD8")
```








### More Celltypes, less resolution

PCA and then cell-cell distances, this time including all cells from the dataset



```{r}
# Takes a ~ 5 GB of memory:
normCells <- log( t(t(rawC) * median(colSums(rawC)) /
                  colSums(rawC)  ) + 1 )


# compute PCA on (almost) all genes (minus exremely noisy ones, for speed):
vmrCells <- VMRstats( rawC )
coarse_pickCells<- rownames( vmrCells[vmrCells$log_Gene_VMR > vmrCells$log_PoissonVMR_median,] )

# takes ~ 10 GB of memory, runs > 25 min
# pcCells<- prcomp((t(normCells[ coarse_pickCells, ])), center = TRUE, scale. = TRUE)

```
Distances for these cells:
```{r}

sqDs <- matrix(0, nrow = 500, ncol = 500)    
 cells_DF <- tibble()
for(p in 1876:8005) {
 if(p %% 10 == 0) print(p)
 sqDs <<- sqDs + sqDeviation(pcCells$x[selectFew, p])

 #if(p %% 50 ==0 | p < 50) { 
  nns  <- NNindices(as.matrix(sqrt(sqDs)), n_neighbors = 31)
  correct_assignments <- apply(nns, 1, function(ids)
      sum(ctFew[ids[2:ncol(nns)]] == ctFew[ids[1]]))
  tmp <- tibble(  correct_assignments, Celltype = ctFew,
                       PCs = p) %>% # number of PC
   group_by(Celltype, PCs) %>%
   summarise(Mean_correctNeighbors = mean(correct_assignments))  
 cells_DF <<- bind_rows(cells_DF, tmp) 
 #}
}

ggplot(cells_DF) + geom_point(aes(PCs, Mean_correctNeighbors, colour = Celltype))+
  ggtitle("(Almost) all genes -> PCA on all 8005 cells\n-> eucl. distance between 250 CD4 and 250 CD8 T cells on n PCs") + ylim(c(0,30)) + 
  xlim(c(0,500))
```


### Celltype proportion

First we pick 250 other CD4 T cells and see how much the result differs from above,
to control for heterogeneity within the CD4 cells:
```{r}
furtherCD4 <- sample(setdiff(which(isCD4Tcell), selectFew), 250)

selectFurther <- c(furtherCD4, which(isCD8Tcell))




# find exremely noisy genes, those we exclude in PCA for speed):
vmrFurther <- VMRstats(rawC[, selectFurther] )
normFurther <- log( t(t(rawC[, selectFurther]) * median(colSums(rawC[, selectFurther])) /
                  colSums(rawC[, selectFurther])  ) + 1 )
coarse_pickFurther <- rownames(
    vmrFurther[ vmrFurther$log_Gene_VMR > vmrFurther$log_PoissonVMR_median, ] )
  
  
pcFurther<- prcomp((t(normFurther[ coarse_pickFurther, ])), center = TRUE, scale. = FALSE)
 
```

let's show the 250 other CD4 picked here give the same results as above:
```{r}
sqDs <- matrix(0, nrow = 500, ncol = 500)    
further_DF <- tibble()
for(p in 1:500) {
 if(p %% 10 == 0) print(p)
 sqDs <<- sqDs + sqDeviation(pcFurther$x[, p])

 #if(p %% 50 ==0 | p < 50) { 
  nns  <- NNindices(as.matrix(sqrt(sqDs)), n_neighbors = 31)
  correct_assignments <- apply(nns, 1, function(ids)
      sum(ctFew[ids[2:ncol(nns)]] == ctFew[ids[1]])) # ctFew still valid for further CD4
  tmp <- tibble(  correct_assignments, Celltype = ctFew,
                       PCs = p) %>% # number of PC
   group_by(Celltype, PCs) %>%
   summarise(Mean_correctNeighbors = mean(correct_assignments))  
 further_DF <<- bind_rows(further_DF, tmp) 
 #}
}
p_further <- ggplot(further_DF) + geom_point(aes(PCs, Mean_correctNeighbors, colour = Celltype))+
    ggtitle("(Almost) all genes -> PCA on 250 __other__ CD4 and 250 CD8 T cells\n-> eucl. distance on n PCs") + ylim(c(0,30))
cowplot::plot_grid(p_few, p_further, nrow = 2)
```

This shows they're highly similar. We can now investigate what happens when we do
500 CD4 and 250 CD8 T cells:

```{r}
selectProp <- c(furtherCD4, selectFew)
# selectProp now has indices of 500 CD4 cells followed by those of 250 CD8 cells.
# However, I only take __250/250__ here as well, as I want to read out the same
# cells as above:
ctProp     <- c( rep("CD4", 250), rep("CD8", sum(isCD8Tcell)) ) 

# find exremely noisy genes, those we exclude in PCA for speed):
vmrProp  <- VMRstats(rawC[, selectProp] )
normProp <- log( t(t(rawC[, selectProp]) * median(colSums(rawC[, selectProp])) /
                  colSums(rawC[, selectProp])  ) + 1 )
coarse_pickProp <- rownames(
    vmrProp[ vmrProp$log_Gene_VMR > vmrProp$log_PoissonVMR_median, ] )
  
  
pcProp <- prcomp((t(normProp[ coarse_pickProp, ])), center = TRUE, scale. = FALSE)
```

```{r}
sqDs <- matrix(0, nrow = 500, ncol = 500)    
prop_DF <- tibble()
for(p in 1:750) {
 if(p %% 10 == 0) print(p)
 sqDs <<- sqDs + sqDeviation(pcProp$x[251:750, p])
 nns  <- NNindices(as.matrix(sqrt(sqDs)), n_neighbors = 31)
  correct_assignments <- apply(nns, 1, function(ids)
      sum(ctProp[ids[2:ncol(nns)]] == ctProp[ids[1]]))
  tmp <- tibble(  correct_assignments, Celltype = ctProp,
                       PCs = p) %>% # number of PC
   group_by(Celltype, PCs) %>%
   summarise(Mean_correctNeighbors = mean(correct_assignments))  
 prop_DF <<- bind_rows(prop_DF, tmp) 
}


p_prop <- ggplot(prop_DF) + geom_point(aes(PCs, Mean_correctNeighbors, colour = Celltype))+
    ggtitle("(Almost) all genes -> PCA on 500 CD4 and 250 CD8 T cells\n-> eucl. distance between same 250/250 cells as above on n PCs") + ylim(c(0,30))
cowplot::plot_grid(p_few, p_prop, nrow = 2)
```






## Kolmogorow-Smirnow statistics

### Idea
Let's walk up the density histograms from the left side and in a cumsum, we 
count upward when we encounter a CD8 cell and downward when we encounter a
CD4 cell.
Illustration:
```{r}
# pc is object computed in CoD on PCs
d <- dist(pcFew$x[, 1:30])


id_g = which(rownames(pcFew$x) == "GGAACTTAGGTGTGGT")
ggplot( data.frame(Celltype = ctFew[- id_g], Distance = as.matrix(d)[ id_g, -id_g]) ) + geom_histogram( aes(Distance, y = ..density.., fill = Celltype) , binwidth = 1, alpha = .4, position = "identity") + ggtitle("Good CD8 cell")

id_b = which(rownames(pcFew$x) == "CCGTTCACAGGCGATA")
ggplot( data.frame(Celltype = ctFew[- id_b], Distance = as.matrix(d)[ id_b, -id_b]) ) + geom_histogram( aes(Distance, y = ..density.., fill = Celltype) , binwidth = 1, alpha = .4, position = "identity") + ggtitle("Bad CD8 cell")
```


```{r}
n_PCs <- 30
d <- dist(pcFew$x[, 1:n_PCs])


id <- id_g
do <- order(as.matrix(d)[id, ])
x <- cumsum(incFew[do])
plot( cumsum(incFew[do]), xlab = "NN          <---->         furthest cells" , main = "CD8 enrichment for bad CD8 cell", ylim = c(0, 120))

```




```{r}
n_PCs <- 30
d <- dist(pcFew$x[, 1:n_PCs])

# we can use statistical test as well, but not sure that's sound:
ks.test(x = which(ctFew[do] == "CD8"), 
             y = which(ctFew[do] == "CD4") )

# for a given distance matrix d, compute nearest neighbor enrichments
    enrichments <- t(sapply(1:500, function(id) {
      
     do <- order(as.matrix(d)[id, ])
     x <- cumsum(incFew[do])
     return( c(ES = x[which.max(abs(x))],
               NN_at_max = which.max(abs(x))) )
     
    }))
# for a given distance matrix d, compute Kolmogorov-Smirnov test

ksresults <- t( sapply(1:500, function(id) {
    do <- order(as.matrix(d)[id, ])
    ks <- ks.test(x = which(ctFew[do] == "CD8"), 
                  y = which(ctFew[do] == "CD4") )
    c(D_statistic = unname(ks$statistic), pval = ks$p.value)
}) )

    
df <- data.frame(PCs = n_PCs, Celltype = ctFew, increment = incFew,
               enrichments, ksresults)


df %>% ggplot() + geom_jitter(aes(Celltype, ES, col = Celltype)) + ggtitle("CD8 enrichment\nfor 250:250 CD4:CD8 T cells, 1:30 PCs")

df %>% ggplot() + geom_point(aes(ES, NN_at_max, col = Celltype)) + ggtitle("Late peak for false-positive CD4s")
```










### enrichments Functions

```{r}


enrichments <- function(d, increms = c(rep(-1, 250), rep(1, 250))) {
# for a given distance matrix d of class 'dist' or 'matrix', compute nearest neighbor enrichments
     if(inherits(d, "dist")) d <- as.matrix(d)
     enr <- t(sapply(1:ncol(d), function(id) {
                do <- order(d[id, ])
                x <- cumsum(increms[do])
                return( c(ES = x[which.max(abs(x))],
                          NN_at_ES= which.max(abs(x))) )
              })
           )
     return( data.frame(enr, increments = increms) )
}

```


### 250:250

```{r}

 sqDs <- matrix(0, nrow = 500, ncol = 500)    
 res_250.250 <- tibble()
for(n_PCs in 1:500) {
 if(n_PCs %% 10 == 0) print(n_PCs)
  # add squared deviations of the current PC:
  sqDs <<- sqDs + sqDeviation(pcFew$x[, n_PCs])
  # compute enrichments:
  d <- sqrt(sqDs)
  foo <- data.frame(enrichments(d, incFew),
                    Celltype = ctFew)
  # summarize enrichment readouts for current number of PCs:
  foo <- foo %>%
    mutate(ES_sign = ifelse(ES>0, "positive", "negative")) %>% group_by(Celltype, ES_sign) %>% 
    summarise(Mean = mean(ES),
              sd=sd(ES),
              Number = n()) %>%
    mutate(PCs = n_PCs,
           correlation_ES_celltype = cor(foo$increments, foo$ES)) %>%
    select(PCs, correlation_ES_celltype, everything())
  
  res_250.250 <<- bind_rows(res_250.250, foo) 
}

 
 
 
p_250.250 <- ggplot(res_250.250) + geom_point(aes(PCs, correlation_ES_celltype))
p_250.250
p_250.250  + geom_vline(xintercept = 35) + 
             geom_vline(xintercept = 12) +
             geom_vline(xintercept = 65)


cowplot::plot_grid( 
   ggplot( data.frame( enrichments(dist(pcFew$x[, 1:12])), 
                       Celltype = ctFew) ) +
           geom_jitter(aes(Celltype, ES, col = Celltype)) + theme(legend.position="none") +
           ggtitle("PCs 1-12"),
   
   ggplot( data.frame( enrichments(dist(pcFew$x[, 1:35])), 
                       Celltype = ctFew) ) +
           geom_jitter(aes(Celltype, ES, col = Celltype)) + theme(legend.position="none") +
           ggtitle("PCs 1-35"),
   
   
   ggplot( data.frame( enrichments(dist(pcFew$x[, 1:65])), 
                       Celltype = ctFew) ) +
           geom_jitter(aes(Celltype, ES, col = Celltype)) + theme(legend.position="none") +
           ggtitle("PCs 1-65"),
   ncol = 3)
```
















# Unfinished





## CoD on Cells
Curse of dimensionality (CoD): let's take all cells, this should also make
it harder to tell CD4 and CD8 T cells apart.

```{r}
# vmr <- VMRstats(rawC )
norm3 <- log( t(t(rawC) * median(colSums(rawC)) /
                  colSums(rawC)  ) + 1 )
library(irlba)
pc <- irlba::irlba(t(scale(t(norm3[rownames(vmr), ]))),
                         nv = 40)
embeddings <- pc$v %*% diag(pc$d)


d  <- dist(embeddings[, 1:28])
tmp <- Rtsne::Rtsne(d, is_distance = TRUE)  
plot(tmp$Y, pch = 20, col = rgb(.5+.5*isCD4Tcell, .5+.5*isCD8Tcell, 0),
     asp=1)

for(np in c(1:39)) {
  print(np)
  
  d <- dist(embeddings[, 1:np])
  
  nn4 <- NNindices(as.matrix(d)[isCD4Tcell, ] )
  nn4 <- apply(nn4, 1, function(drow) sum(isCD4Tcell[drow]))
  
  nn8 <- NNindices(as.matrix(d)[isCD8Tcell, ] )
  nn8 <- apply(nn8, 1, function(drow) sum(isCD8Tcell[drow]))
 
   
  df <- rbind(data.frame(Celltype = "CD4", NNsameType = unname(nn4) ),
               data.frame(Celltype = "CD8", NNsameType = unname(nn8) ) )
  df$NNsameType =  factor(df$NNsameType, levels = 0:10)
  
  
  
  cd4 <-  data.frame(table(df[df$Celltype== "CD4",
                        "NNsameType"]), Celltype = "CD4")
  cd8 <-  data.frame(table(df[df$Celltype== "CD8",
                        "NNsameType"]), Celltype = "CD8")
  cdf <- data.frame(rbind(cd4, cd8),
             prop = c( cd4$Freq / sum(isCD4Tcell),
                       cd8$Freq / sum(isCD8Tcell) ))
  
    
  # output nice plots:
   png(paste0(
        "/home/felix/Dropbox/PhD/Meetings-Talks-etc/labM_Anders/7_hvg_vmr_CD4CD8/",
        "lognormalization_allGenes_allCells_varyPC/" ,      
        "NNs_lognormAllGenes_PCs1to", np, ".png"), width = 1000, height = 600, units = "px")
  # par(mfrow = c(1, 2)) 
  # plot(vmr$log_Gene_Mean, vmr$log_Gene_VMR, pch = 20, cex=.1, col = "#00000040",
  #      ylab = "log_VMR ( log(var/mu)", xlab = "Average expression ( log(mu) )",
  #      main = ng) 
  # points(vmr$log_Gene_Mean[1:ng], vmr$log_Gene_VMR[1:ng], pch=20, col = "red")
   
  print(ggplot(cdf) + geom_bar(aes(Var1, prop, fill = Celltype),stat = "identity", position = "dodge") + xlab("NNs of same cell type")+ ylab("Proportion") +
    ggtitle(np))
   # deprecated plot:
   # hist(ns, xlab = "CD4 T cells amongst 100 NNs", main = paste0("CD4 and CD8 T cells\n", ng),
   #      breaks = -.5+0:101, ylim = c(0, 800) )
   dev.off()
 
}
```




















## CD4 vs CD8 T cells - regress nUMIs

```{r}
library(Seurat)
s <- MakeSparse(CreateSeuratObject(rawC[, c(which(isCD4Tcell), which(isCD8Tcell))]))
s <- NormalizeData(s)
s <- FindVariableGenes(s, display.progress = F)

s <- ScaleData(object = s, vars.to.regress = c("nUMI"),
               genes.use = rownames(vmr[1:4100,]), use.umi = T,
               model.use = "negbinom", do.par = T, num.cores = 4, display.progress = T)
```


# Savepoint

```{r}
# save.image(file = "/home/felix/PhDother/scAnalysis/sc_methods/hvg/savepoint/messySeurat_s.rda")
library(ggplot2)
library(pbmcapply) # parallelization
library(quantreg)  # quantile regression with rq.wfit
library(parallelDist)
library(tidyverse)
library(Rtsne)
load(file = "/home/felix/PhDother/scAnalysis/sc_methods/hvg/savepoint/messySeurat_s.rda")
```







```{r}

for(ng in 2^(6:14)) {
  print(ng)
 seurat <- RunPCA(s, pc.genes = rownames(head(vmr, n = ng)), pcs.compute = 30,
            do.print = F)
   d    <- parallelDist::parDist(seurat@dr$pca@cell.embeddings[, 1:30],
                                  threads = 4)
 
  nns <- NNindices(as.matrix(d), n_neighbors = 11 )
  correct_assignments <- apply(nns, 1, function(ids)
    sum(identities[ids[2:11]] == identities[ids[1]]))
  cdf <- (cbind.data.frame(correct_assignments, identities))
  # below, table will note empty levels only if it's a factor:
  cdf$correct_assignments <- factor(cdf$correct_assignments, levels = 0:10)
  
  cd4 <-  data.frame(table(cdf[cdf$identities == "CD4",
                        "correct_assignments"]), Celltype = "CD4")
  cd8 <-  data.frame(table(cdf[cdf$identities == "CD8",
                        "correct_assignments"]), Celltype = "CD8")
  cdf <- data.frame(rbind(cd4, cd8),
             prop = c( cd4$Freq / sum(identities == "CD4"),
                       cd8$Freq / sum(identities == "CD8") ))
  
    
  # output nice plots:
  fn <- paste0(
        "/home/felix/Dropbox/PhD/Meetings-Talks-etc/labM_Anders/7_hvg_vmr_CD4CD8/",
        "withRegressingUMIs/",
        "NNs_PCs1to30_hvgs1to", ng, ".png")
  # png(fn, width = 1000, height = 800, units = "px")
  # par(mfrow = c(1, 2)) 
  # plot(vmr$log_Gene_Mean, vmr$log_Gene_VMR, pch = 20, cex=.1, col = "#00000040",
  #      ylab = "log_VMR ( log(var/mu)", xlab = "Average expression ( log(mu) )",
  #      main = ng) 
  # points(vmr$log_Gene_Mean[1:ng], vmr$log_Gene_VMR[1:ng], pch=20, col = "red")
   
  ggplot2::ggsave(fn, ggplot(cdf) + geom_bar(aes(Var1, prop, fill = Celltype),stat = "identity", position = "dodge") + xlab("NNs of same cell type")+ ylab("Proportion")
  )
   # deprecated plot:
   # hist(ns, xlab = "CD4 T cells amongst 100 NNs", main = paste0("CD4 and CD8 T cells\n", ng),
   #      breaks = -.5+0:101, ylim = c(0, 800) )
   #dev.off()
 
}
```































# Play zone





## Covariance of Poisson estimators
```{r}
# My VMR-over-mean plot implicitly estimates the true variance and mean of each
# gene, and the vmr as well. Sveta has come up with formulas for the variances
# of the first two estimators (mean_hat and var_hat), and from these I can compute
# the variance of the vmr estimator. This follows the formula
#
#      var(log_vmr) = var(log_var) + var(-log_mean) + 2 * cov(log_var, -log_mean)
#
# It is interesting for me to see whether the covariance of log_var and log_mean
# is 0 as hoped, or if it's above.

meanvar_correl <- do.call(rbind, lapply(exp(-5:10), function(x) {

cors <- t(replicate(5, {
  mv <- t(replicate(100, expr = {
  p <- rpois(8000, x)
  c(mean(p), var(p)) } ))
  c( lambda = x, varmean_cov = cov(log(mv[,1]), log(mv[,2]) ), var_of_var = var(log(mv[, 2])),
     var_of_mean = var(log(mv[, 1])))
}))

}) )

plot(meanvar_correl, log = "x")
```

Let's try this again with realistic values, stemming from the CITEseq dataset:
```{r}
# size factors:
s_j <- colSums(rawC[, Ts])
# means (after colsum normalization):
mu_i <- rowMeans( t( t(rawC[, Ts]) / s_j))

m <- 0.008

  p <- rpois(length(s_j), lambda = m * s_j)
  p <- p / s_j
  c(mean = mean(p), var = var(p))

```









## Sveta's variance of Poisson-variance
she derived the formula for variance of variance when we include size factors in
our poisson models. Ultimately this will be helpful in picking HVGs as we do not
have to rely on poisson simulations anymore, so here I simulate data to see whether
the formula is correct.


I simulate poisson counts:
```{r}
cs <- colSums(rawC[, Ts]) 
# estimate each gene's true mean and var - for this we normalize with library size:
estMeans <- apply(t( t(rawC[ rowMeans(rawC[, Ts]) > 0.00530, Ts]) / cs), 1, mean)
estMeans <- sample(estMeans, 1000)

# simulate poisson raw counts relevant to our data (i.e. using allM):
    nc <- ncol(rawC[, Ts])
   
    
    poisson_replicates <- replicate(100, { 
    singlePoisson <- do.call(rbind,
                           lapply(estMeans, function(mu) rpois(nc, lambda = mu * cs))
                           )
    
    apply( t(singlePoisson) / cs, 2, var)} )
    
    
    
# Sveta's variance estimation:
    var_theo <- function(mu, colsums = colSums(rawC[, Ts])) {
     N   <- length(colsums)
     psi <- sum(1/colsums) / N
     
     vt <- 2/(N-1)/(N-1) * mu^2 * psi^2 +
           (2*N-4)/(N*(N-1)^2) * mu * mu * sum(1/(colsums^2)) +
           mu / N / N * sum(1/(colsums^3))
    }


plot(apply(poisson_replicates, 1, var), var_theo(estMeans, cs), log = "xy")

all.equal(apply(poisson_replicates, 1, var), var_theo(estMeans, cs))


```

```{r}
poisson_sds <- sqrt( var_theo(exp(v$log_Gene_Mean), colSums(rawC[, Ts])) )

plot(v$log_Gene_Mean, v$log_Gene_VMR, pch = 20, cex=.1, col = "#00000090")
lines(v$log_Gene_Mean, v$log_PoissonVMR_median,
      lwd = 1.5, col = "orange")
lines(v$log_Gene_Mean, )
```


































# weighted distance

```{r}
distanceDF <- function(dist, idx1, idx2, name1, name2) {
  rbind(
    # both groups amongst themselves (only keep upper half of matrix)
    data.frame(Group = paste(name1, name1, sep = "_"),
               Distance = getUpper( as.matrix(dist)[idx1, idx1])),
    data.frame(Group = paste(name2, name2, sep = "_"),
               Distance = getUpper( as.matrix(dist)[idx2, idx2])),
    # between the two groups (keep all values)
    data.frame(Group = paste(name1, name2, sep = "_"),
               Distance = as.numeric( as.matrix(dist)[idx1, idx2]))
  )
}


histo_distance_group <- function(distanceDF){
  ggplot(distanceDF,
       aes(x = Distance, stat(density), fill = Group)) +
       geom_histogram(position = "identity", alpha = .6, binwidth = 1) }



```



Simon and I play around with this but find it's not very powerful
yet.
```{r}

mat <- rawC[, Ts]
cs  <- colSums(mat)
mat <- mat[rownames(mat) %in% names(hvgs_T$hvg_2IQR) ,]

w <- apply(mat, 1, function(k) 1 / mean(1/ (k+.5)))

plot(w, col =
(1 + rownames(mat) %in% names(hvgs_T$hvg_2IQR) ))

d <- parDist(t(sqrt(w) * log( t(t(mat + .5) / cs)) ),
               threads = 4 )

ggplot(distanceDF_Tcells(d),
       aes(x = Distance, stat(density), fill = Group)) +
       geom_histogram(position = "identity", alpha = .6, binwidth = 1)

```








# Seurat's means and disps

FindVariableGenes by default uses ExpMean and LogVMR, which effectively do this
with the lognormalized values x:

```{r}
x  <- s@data["poisson_1305", ]
expMean <-  log(
              x = mean(x = exp(x = x) - 1) + 1 )

disp    <-  log(
              x = var(x = exp(x = x) - 1) / mean(x = exp(x = x) - 1) )



xf <- pCounts["poisson_1305", ]  / cs 

```

I find this worrying.



# End of Script

```{r}
devtools::session_info()
```


