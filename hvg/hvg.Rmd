---
title: "Find Highly Variable Genes"
output:
  html_document:
    df_print: paged
    toc: yes
    number_sections: true
    toc_float: true
    self_contained: true
    mathjax: default
    code_download: true
---




# Load Packages

```{r}
library(ggplot2)
library(pbmcapply) # parallelization
library(quantreg)  # quantile regression with rq.wfit
library(parallelDist)
```



# Load data
On the CITEseq data, we play around with HVGs (highly variable genes), i.e.
superpoissonian genes.

```{r loadCITE}
citeDIR <- "~/sds/sd17l002/p/scRNAseq_datasets/CITEseq_NatMethods_2017/data/"
rna_file <- paste0(citeDIR, "GSE100866_CBMC_8K_13AB_10X-RNA_umi.csv.gz")
adt_file <- paste0(citeDIR, "GSE100866_CBMC_8K_13AB_10X-ADT_umi.csv.gz")

rawC <- as.matrix(read.csv(gzfile(rna_file), row.names = 1))
prot <- as.matrix(read.csv(gzfile(adt_file), row.names = 1))

# exclude mouse cells:
is_mouse <- colSums(rawC[grepl("MOUSE", rownames(rawC)),]) / colSums(rawC) > .1
rawC <- rawC[grepl("HUMAN", rownames(rawC)), ! is_mouse]
prot <- prot[, ! is_mouse]
```




# RNA data: normalization

```{r}
normC_anscombe <- apply( rawC, 2, function(col) {
  (sqrt(col+3/8)-sqrt(3/8)) / sqrt(sum(col))
})

normC <- log1p(t( t(rawC) * 2300 / colSums(rawC)))
###        DEPRECATED!!!             ###
###        delete 2300! it's only    ###
###        here to not break         ###
###        reproducibility with next ###
###        section.                  ###
###        Savepoint below fixes it  ###
```




# Protein data: normalization and Cell identities
```{r}
# simplifying Seurat's code to compute Centered Log Ratio (CLR):
norm_prot <- apply(prot, 1, function(x) {
  log1p( (x) /
                          (exp(sum(log1p((x)[x > 0]), na.rm = TRUE)/length(x + 1))) ) })


 # exclude NK and monos:
  plot(norm_prot[, "CD11c"], norm_prot[, "CD56"], pch =20, cex=.1, 
       main = "CITEseqs protein data\nto exclude NKs (CD56) and myeloid cells (CD11c)")
  lin_neg <- norm_prot[, "CD56"] < 1.1   & norm_prot[, "CD11c"] < 1 
 
# exclude erythrocytes. No good protein marker available, we use RNA for it:
  not_ery <- normC["HUMAN_HBB", ]  < .05  &
             normC["HUMAN_HBG2", ] < .05 &
             normC["HUMAN_HBA1", ] < .04 
# same for platelets / megakaryocytes:
  not_platelet <- normC["HUMAN_GP9", ] < .005 &
                  normC["HUMAN_PF4", ] < .005 &
                  normC["HUMAN_PPBP", ] < .005 
  
  
# B and T cells:
  has_B_markers <- norm_prot[, "CD19"] > 2  &  norm_prot[, "CD3"] < .5
  has_T_markers <- norm_prot[, "CD19"] < 1.5  &  norm_prot[, "CD3"] > 1
  plot(norm_prot[, "CD3"], norm_prot[, "CD19"], pch =20, col =
         rgb(has_B_markers, has_T_markers, .7 *lin_neg),
       main = "B and T cells in red and green\nCD56-CD11c- in blue\nPick cells with mixture colors")

  
    
  isBcell <- has_B_markers & lin_neg & not_ery & not_platelet
  isTcell <- has_T_markers & lin_neg & not_ery & not_platelet

  
  
  has_CD4 <- norm_prot[, "CD4"] > 1  &  norm_prot[, "CD8"] < 1
  has_CD8 <- norm_prot[, "CD4"] < .5  &  norm_prot[, "CD8"] > 3
  isCD4Tcell <- isTcell & has_CD4
  isCD8Tcell <- isTcell & has_CD8
   
  plot(norm_prot[, "CD4"], norm_prot[, "CD8"], pch=20, cex=.4,
       col = rgb(0, isCD4Tcell, isCD8Tcell), 
       main = "CD4 and CD8 T cells\nselected amongst T cells picked above")  
  
  
Ts <- isCD4Tcell | isCD8Tcell
```

# Select CD4 and CD8 T cells



## Colour scheme
Useful to pick colors for CD4 and CD8 
once and then use them consistently throughout this script.
```{r}
library(RColorBrewer)
dark2 <- brewer.pal(8, "Dark2")
myCols <- scale_colour_manual(values = c("CD4" = dark2[2], "CD8" = dark2[5]))


# for expression heatmap:
colPal3 <- colorRampPalette(rev(brewer.pal(5, "RdBu")))(10)
colPal2 <- c("grey", colorRampPalette((brewer.pal(9, "YlOrRd")))(100) )
```

## Select cells (250+250)
I want the groups (CD4 and CD8) to be even, so I draw 250 CD4 T cells (out of
almost 3000). Note that CD8 T  cells **systematically have higher library sizes**
than CD4 T cells. To investigate the effect of this, I draw twice: once randomly,
and once so that libsizes roughly match.

Convention: I'll always list the 250 CD4 cells first, followed by 250 CD8 T cells:
```{r}
ct <- c( rep("CD4", 250), rep("CD8", sum(isCD8Tcell)) )
```


1. Randomly draw 250 CD4 cells:
```{r}
selectFew <- c(sample(which(isCD4Tcell), 250), which(isCD8Tcell))
# order by celltype, then by libsize:
selectFew <- selectFew[ order(ct, colSums(rawC[, selectFew]), decreasing = F) ]
```
2. match library sizes of selected CD4 cells to the 250 CD8 T cells
in the CITEseq dataset:
```{r}
cs <- colSums(rawC)
# we mask all non-CD4 cells by adding huge numbers:
dummycs <- cs; dummycs[! isCD4Tcell ] <- dummycs[ !isCD4Tcell] + 32000
# for each CD8 cell, we'll put the CD4 cell with most similar library size
# into the first column of the `simil` matrix, the second most similar into
# the second column, etc. The first column has duplicates, so them alone would
# not be 250 cells. But taking the first two columns together we have enough
# unique CD4 T cells to draw 250 from them:
simil <- t(sapply(cs[isCD8Tcell], function(libsize) {
  order(abs(libsize - dummycs), decreasing = FALSE)
}))
ids <- sample( unique(as.vector(simil[, 1:2])),  250 )
ids <- ids[order(colSums(rawC[, ids]), decreasing = T)] # sorted by libsize can help to find artifacts
selecteven <- c(ids, which(isCD8Tcell))
```

You see here that it makes quite some difference:
```{r cd4_cd8_libsizes, fig.height=6, fig.width=9}
plot_grid(
   data.frame(libsize = colSums(rawC[, selectFew]), Celltype = ct) %>%
  ggplot(aes(Celltype, libsize, col = Celltype))+
  geom_violin(draw_quantiles = c(.5)) +
  geom_jitter() + ylim(c(0,8500)) +
  ggtitle("250 random CD4 T cells\nRcode: selectFew") +
  myCols + theme(legend.position = "none"),
  
  data.frame(libsize = colSums(rawC[, selecteven]), Celltype = ct) %>%
  ggplot(aes(Celltype, libsize, col = Celltype))+
  geom_violin(draw_quantiles = c(.5)) +
  geom_jitter() +ylim(c(0,8500)) +
    ggtitle("Samples for even library sizes\nRcode: selecteven") +
    myCols + theme(legend.position = "none"),
  
  ncol = 2
)
```




# VMRstats function
```{r}
## bug report for below function(s):
  #
  #  This function is terrible coding practice; rewrite it completely!
  #
  #  rowMeans(rawC) and allM is redundant computation, that's uncritical but not elegant
  #
  #  everytime you run it, different genes get selected due to stochastic poisson.
  #  We'll probably find an analytic solution eventually anyways that should fix it.

                                                  ##

VMRstats <- function(mat = rawC, nbin_qreg = 75) {
  cs <- colSums(mat) 
  
  # Simon wants to filter out low-abundance genes. A very conservative cutoff is
  # the mean resulting from 1 small cell (nUMI = 500) having an UMI of 1, the rest 0s:
  threshold <- mean(c(1/500,  rep(0, ncol(mat)-1)))
    print("Simulate Poisson Counts...")
  # estimate each gene's true mean and var - for this we normalize with library size:
    normC <- t( t(mat) / cs)
    normC <- normC[rowMeans(normC) > threshold,]
    allM <- apply(normC, 1, mean)
    allV <- apply(normC, 1, var)
  
  
  # simulate poisson raw counts relevant to our data (i.e. using allM):
    nc <- ncol(mat)
    pCounts <- do.call(rbind,
                       pbmclapply(allM, function(mu) {rpois(nc, lambda = mu * cs)},
                                  mc.cores=4) )
    # superfluous, here because I used to punch these into Seurat:
    rownames(pCounts) <- paste0("poisson_", 1:nrow(pCounts))
    colnames(pCounts) <- paste0("Cell_", 1:ncol(pCounts))
  
    print("Normalize Poisson Counts...")
  # as with real counts:
    normP <- t( t(pCounts) / cs)
    allMP <- apply(normP, 1, mean)
    allVP <- apply(normP, 1, var)
    # x,y for ease of typing. threshold also prevents -Inf values, defensive progr.:
    x <- log(allMP[allMP > threshold])
    y <- log(allVP[allMP > threshold] / allMP[allMP > threshold])
  
  print("Fit VMR-mean relationship...")
  # going over all x takes long, so we fit the VMR-mean relationship in bins:
  xrange <- range(x) # log(c(max(1e-9, min(allM), min(allMP)), max(max(allM), max(allMP))))
  bin_x <- seq(xrange[1], xrange[2], length.out = nbin_qreg)
  # random poisson by chance might not cover the extreme values:
  bin_x <- c(log(min(allM)), bin_x, log(max(allM)))
  
  # Simon's local quantile fit (faster using matrix notation, `mm`)
  mm <- cbind( Intercept = 1, X = x, X2 = x^2 )
  yfit <- do.call(rbind, pbmclapply( bin_x, function(xp)
    {
        fit        <- rq.wfit( mm, y, .75, dnorm( x, mean=xp, sd=2 ) )
        fit_median <- rq.wfit( mm, y, .5, dnorm( x, mean=xp, sd=2 ) )
        c(bin_x = xp,
          q75= fit$coefficients %*% c( 1, xp, xp^2 ),
          median = fit_median$coefficients %*% c( 1, xp, xp^2 ))
    } )  )
  
  # interpolate Poisson median and quantile for each gene: 
  all_y<- do.call(data.frame,
                 c( 
                   poisson_medians <- approx(x = yfit[, "bin_x"],
                                             y = yfit[, "median"],
                                             xout = log(allM)),
                   poisson_q75     <- approx(x = yfit[, "bin_x"],
                                             y = yfit[, "q75"],
                                             xout = log(allM)) ) )
  # summarize final result:
  df <- data.frame(
      log_Gene_Mean         = all_y[, 1],
      log_Gene_VMR          = log(allV / allM),
      log_PoissonVMR_median = all_y[, 2],
      log_PoissonVMR_q75    = all_y[, 4],
      IQR_of_logged_VMR         = 2 * (all_y[, 4] - all_y[, 2]),
      row.names = rownames(all_y)
    )
  df$aboveP  <- (df$log_Gene_VMR - df$log_PoissonVMR_median) /
    df$IQR_of_logged_VMR
  df <- df[order(df$aboveP, decreasing = TRUE), ]
  
  return(df)
} 

vmrPlot <- function(vmrstat) {
    plot(vmrstat$log_Gene_Mean, vmrstat$log_Gene_VMR, pch = 20, cex=.1, 
         xlab = "log( estim_mean )", ylab = "log( estim_var / estim_mean )")
    points(vmrstat$log_Gene_Mean,vmrstat$log_PoissonVMR_median, pch = ".", col = "orange")
    points(vmrstat$log_Gene_Mean,vmrstat$log_PoissonVMR_q75, pch = ".", col = "orange")
  }
  
vmrPoints <- function(vmrstat, sel) {# sel is a true/false boolean vector selecting genes
    points(vmrstat[sel, "log_Gene_Mean"], vmrstat[sel, "log_Gene_VMR"],
           col="red", pch=20)
    }

```

# Other Functions
```{r}

anscNorm <- function(rawCounts) {
      apply( rawCounts, 2, function(col) {
                     (sqrt(col+3/8)-sqrt(3/8)) / sqrt(sum(col)) } )
} 




getUpper <- function(mat) mat[upper.tri(mat, diag = FALSE)]
 

sqDeviation <- function(PCvector) {
  # For a given numerical vector, returns a matrix
  # with squared deviations of amongst all its elements.
  # Taking this matrix's square root is the 1D Euclidean Distance.
  sapply(PCvector, function(entry) (entry - PCvector) * (entry - PCvector))
 }
 
NNindices <- function(dists = as.matrix(dist(matrix(1:9,3))), n_neighbors = 10) {
  # get n nearest neighbors' indices. 
  NN = t(apply(dists, 1,   
           function(ds) head(order(ds), n = n_neighbors)
             ) )
  return(NN)
 }

 
  
stratNs <- function(NN, ct) {
# Stratify neighborhoods: for each cell (row) in NN, count neighborhoods it 
# belongs to.
  # NN: is a matrix with one row per cell (neighborhood), filled with the
  #     indices of that cell's
  #     nearest neighbors (e.g. top 10, top 30, ...).
  # ct: is a character vector with the celltype of each cell, in the same order
  #     as the rows in NN.
  # value: 
  cbind(ID = 1:nrow(NN), 
    sapply(unique(ct), function(type)
      { as.numeric( table( factor( NN[ ct == type,  ], levels = 1:nrow(NN) ) ) ) 
    })
  )
}


```





# Plausible gene means and sizefactors

Plausible gene means inspired by CITEseq dataset
When simulating scRNAseq data
(e.g. as Poisson counts for homogeneous populations),
it is useful to cover biologically relevant ranges of gene expression magnitude.
Here are "plausible gene means", meaning they could actually be raw counts of
~ 1000 genes observed for a cell with average library size
(libsize here means nUMI aka colSums)
in a 10X scRNAseq dataset.
These can be used directly as Poisson rates to simulate data, or they can be
adjusted with sizefactors centered around 1 (1 representing the cell with average sequencing depth).

```{r plausible_means}
plausible_means <- c(  
    runif(575, min = .5, max = 1),
    runif(267, min = 1,  max = 2),
    runif(190, min = 2,  max = 10),
    runif(70, min = 10, max = 40)  )
# compare to citeseq by loading rawC as in hvg.Rmd, followed by:
# cs <- colSums(rawC)
# normC <- t( t(rawC) *mean(cs)/ cs)
# table(round(rowMeans(normC)))
# rms <- rowMeans(normC)
# plot(table(round(plausible_means, digits = 1 )), main = "Simulated means", ylab = "Number of Genes")
# plot(table(round(rms[rms < 40 & rms > .5], digits = 1)), main = "CITEseq means (> .5 UMIs)", ylab="Number of Genes")
```

Turns out using the f-Distribution we can simulate acceptable sizefactors
around 1 for scRNAseq data.
```{r plausible_sf}
plausible_sf <- rf(1000,20,22, 0)
# This is where the f-Distribution idea is motivated from:
#  # load the citeseq dataset first, then you can get the sizefactors:
#  sf <- colSums(rawC) / mean(colSums(rawC))
#  hist(sf, breaks = 0:150 * .1)
#  
#  # let's fit an f-Distribution to that. With guessing parameters, it'd look like this:
#  x <- seq(0, 20, length.out = 1000)
#  hist(sf, breaks = 0:150 * .1, freq  = F); lines(x, df(x, df1=3, df2=2, ncp = 0), col="blue")
#  
#  # lets find maximum likelihood estimates:
#  optim(c(2, 5, 3), function(params) -sum(df(sf, df1 =  params[1],
#                                             df2 =  params[2], ncp = 
#                                               params[3], log = T) ) )
#  
#  hist(sf, breaks = 0:150 * .1, freq  = F); lines(x, df(x,20,22, 0), col="blue")
```






# Savepoint

```{r}
# save.image(file = "/home/felix/PhDother/scAnalysis/sc_methods/hvg/savepoint/start_hvg.rda")
library(ggplot2)
library(pbmcapply) # parallelization
library(quantreg)  # quantile regression with rq.wfit
library(pheatmap)
library(parallelDist)
library(tidyverse)
library(Rtsne)
load(file = "/home/felix/PhDother/scAnalysis/sc_methods/hvg/savepoint/start_hvg.rda")


normC <- log1p(t( t(rawC) / colSums(rawC)))
```






# Curse of dimensionality
Here I will pick more and more HVGs and/or PCs to show the distances within
and between CD4 and CD8 T cells become better and worse.



## Filter Genes [vfew, veven]
I have observed that genes that are only >0 in 1 or 2 cells are captured by
PC1 after regressing library size, simply because regression changes the 0s
into a downward-tilted line (see powerpoint presentation).

To be fair, I'll repeat these findings after filtering genes by their
variance/expression, for this I use our VMR on raw counts:

```{r tcellVMR, results= hide}
vfew <- VMRstats(rawC[, selectFew])
veven<- VMRstats(rawC[, selecteven])

universe <- unique(
  c(rownames(vfew)[vfew$aboveP > 1 & vfew$log_Gene_Mean > -12] ,
    rownames(veven)[veven$aboveP > 1 & veven$log_Gene_Mean > -12] ) 
  )
```





## Normalize data [normFew, normeven]
I will compare euclidean distances with increasing numbers of features: 
  normalized gene expression directly, 
  PCs on normalized genes, 
  PCs on regressed (nUMI, ...) genes.

Normalize by hand (selectFew and selecteven):
  #    careful!  (note from 9th January 2019):
  #    including scale factor (here, the 'median(...)`) messes up linear combinations,
  #    so the following code and everything built upon normFew/normeven is taken
  #    with a pinch of salt!
```{r}
normFew  <- log( t(t(rawC[, selectFew]) * median(colSums(rawC[, selectFew])) /
                  colSums(rawC[, selectFew])  ) + 1 ) 

normeven <- log( t(t(rawC[, selecteven]) * median(colSums(rawC[, selecteven])) /
                  colSums(rawC[, selecteven])  ) + 1 ) 
```


Normalize with seurat (selectFew):
```{r}
library(Seurat)
s <- CreateSeuratObject(rawC[, selectFew])

mito <- rownames(s@data)[grepl("HUMAN_MT-", rownames(s@data))]
s   <- AddMetaData(s,
                   metadata = colSums(s@raw.data[mito, ]) / colSums(s@raw.data),
                   col.name = "percent.mito")
s <- NormalizeData(s)
s <- FindVariableGenes(s)


##    use genes after filtering only
noR <- ScaleData(s, genes.use = universe, vars.to.regress = NULL)
R   <- ScaleData(s, genes.use = universe, vars.to.regress = 
                   c("nUMI", "percent.mito"))


# extract normalized counts:
seurat_norm <- noR@scale.data
seurat_regr <-   R@scale.data
```

## Compute PCA
I've seen that regression introduces artifactual genes into PC1 if you use
all genes, so here I'll only use filtered genes ('universe'):
```{r}
pcFew  <- prcomp((t(normFew[universe, ])), center = TRUE, scale. = FALSE)
hvg1 <- rownames(vfew)[vfew$aboveP > 4]
hvg1 <- hvg1[hvg1 %in% universe]
pcFew_hvg1  <- prcomp((t(normFew[ hvg1, ])), center = TRUE, scale. = FALSE)

pc_seurat_norm <- prcomp(t(noR@scale.data[universe, ]),
                         center = TRUE , scale. = FALSE)

pc_seurat_regr <- prcomp(t(  R@scale.data[universe, ]),
                         center = TRUE , scale. = FALSE)

# for even library sizes:
pceven<- prcomp((t(normeven[universe, ])), center = TRUE, scale. = FALSE)
```

## [function:] correctNeighbors function as readout
```{r}

correctNeighbors <- function(prcompObj = NULL,
                             celltypeInfo = NULL,
                             n_neighbors = 31) {
  # initialize variables:
  sqDs <- matrix(0, nrow = nrow(prcompObj$x), ncol = nrow(prcompObj$x))    
  results <- tibble()
  # loop through all PCs:
  for(p in 1:ncol(prcompObj$x)) {
   if(p %% 10 == 0) print(p)
   sqDs <- sqDs + sqDeviation(prcompObj$x[, p])
   nns  <- NNindices(as.matrix(sqrt(sqDs)), n_neighbors = n_neighbors)
   correct_assignments <- apply(nns, 1, function(ids)
        sum(celltypeInfo[ids[2:ncol(nns)]] == celltypeInfo[ids[1]]))
   tmp <- tibble(  correct_assignments, Celltype = celltypeInfo,
                         PCs = p) %>% # number of PC
     group_by(Celltype, PCs) %>%
     summarise(Mean_correctNeighbors = mean(correct_assignments))  
   results <- bind_rows(results, tmp) 
  } 
  return(results)
}
```

## [function:] sepScores, ct_boxplot
```{r}
sepScores <- function(cellInfo = factor(sort(rep(1:2, 250))),
                      pca, pcs=1:6, ...) {
 # compute separation scores:
  is_A <- cellInfo == unique(cellInfo)[1] 
  sepScores <- sapply(pcs, function(i) {
        pc <- pca$x[, i] 
        pc_sep <- ( mean(pc[is_A]) - mean(pc[!is_A]) )  /  sum( sd(pc[is_A]), sd(pc[!is_A])   )
        pc_sep
        })
  names(sepScores) <- paste0("PC", pcs)
 # outut a plot:
  ys <- max(1.5, max(abs(sepScores)))
  plot(pcs, abs(sepScores), ylim = c(-.1, ys), pch=20, ...);
  abline(h=0, col = adjustcolor("black", alpha.f = .4))
  # end
 return(sepScores)
}

ct_boxplot <- function(cellInfo = factor(sort(rep(1:2, 250))),
                       pca, pcs=1:6, plot_cols = 3, plot_rows=2) {
     par(mfrow = c(plot_rows, plot_cols))
    sapply(pcs, function(i) {
      scores <- pca$x[, i]
      plot(cellInfo, scores, main = paste0("PC", i))
    }); par(mfrow = c(1, 1))
}
```




## Playground NN readout
I change the below codechunk again and again to generate plots - these 
plots are pasted into a powerpoint presentation.


```{r message=FALSE, warning=FALSE, silent=TRUE}
tbl_noR <- correctNeighbors(pc_seurat_norm, ct, 31)
tbl_R <- correctNeighbors(pc_seurat_regr, ct, 31)

tbl_1 <- tbl_R
```
Plot correctly assigned nearest neighbors:
```{r fig.height=3.5, fig.width=10}
cowplot::plot_grid(
     ggplot(tbl_1) + geom_point(aes(PCs, Mean_correctNeighbors, col=Celltype))+
     ylim(0, 30) +  myCols + theme(legend.position = "top"),
     
     data.frame(libsize = colSums(rawC[, selectFew]), Celltype = ct) %>% 
     ggplot(aes(Celltype, libsize, col = Celltype))+geom_violin(draw_quantiles = c(.5)) +
     geom_jitter(size=.3)+myCols + theme(legend.position = "none"),
     
     ncol = 2, 
     rel_widths = c(3,1)
)
```




## Effect of regression
I want to look Gene-Libsize scatter plots, and start with genes that are contributing
most to PCs and thus will influence all further analysis.
```{r}
s_R <- RunPCA(R, pc.genes = universe )

Gs <- 
  apply(s_R@dr$pca@gene.loadings, 2, function(PC) {
    order(abs(PC), decreasing = TRUE)[1:5] 
  })


PC = 2
topgenes <- rownames(s_R@dr$pca@gene.loadings)[Gs[, PC]]

 data.frame(libsize = colSums(R@raw.data),  t( R@raw.data[topgenes, ] )) %>%
   gather(key = "Gene", value ="Expression", -libsize) %>% 
   ggplot() + geom_point(aes(libsize, Expression), size=.4) + facet_wrap( ~ Gene, scale = "free_y" )
```

## Which genes to regress?
Lowly expressed genes will get heavily biased by regressing out nUMI, because
all zeros end up as a straight line with negative slope.
Perhaps we can spot and exclude these genes, and perhaps even genes where
the dependency is real but non-linear, by comparing Pearson and Spearman 
correlations:

```{r}
pcor <- cor(t(s@raw.data), colSums(s@raw.data) )
scor <- cor(t(s@raw.data), colSums(s@raw.data), method = "spearman" )

plot(scor[,1], pcor[,1], pch=20, cex=.4, col = adjustcolor("black", alpha.f = .1),
     main = "Cor(Gene_x, nUMI)\nred: Genes with exactly 1 UMI in entire dataset\n10X data (500 T cells from CBMCs)",
     xlab = "Spearman Correlation (with nUMI)", ylab = "Pearson Correlation (with nUMI")
oneExpressor <- rowSums(s@raw.data) == 1
points(scor[oneExpressor,1], pcor[oneExpressor,1], pch = 20, col="red")



```








# Prior information from bulk data

It's a common case that you might have dozens to hundreds of bulk DEGs and
want to use this as prior information to resolve cells. I play around with this
here, using the somewhat artificial case of pseudobulks from the same cells as 
I want to resolve later.

Useful functions:
```{r}
linMap <- function(x, from, to) (x - min(x)) / max(x - min(x)) * (to - from) + from

comparableGenes <- function(B = c("Dl", "ed", "pros"),
                             rawCountMatrix = prog_raw,
                             notThese = NULL) {
    rawCountMatrix <- rawCountMatrix[, colSums(rawCountMatrix) != 0]
    latentMeans <- base::colMeans(t(rawCountMatrix) / colSums(rawCountMatrix)) 
    orderedMeans <- sort(latentMeans, decreasing = T)   
    B_pos      <- which(names(orderedMeans) %in% B)
    notThese_pos      <- which(names(orderedMeans) %in% notThese)
    notB_pos   <- which(!names(orderedMeans) %in% B)
    random_pos <- B_pos
    counter <- 0
    while(sum(duplicated(random_pos))  |  sum(random_pos %in% c(B_pos, notThese_pos) ) ) {
      counter <- counter+1
      print(counter)
      needsChange <- duplicated(random_pos) | random_pos %in% c(B_pos, notThese_pos)
      step <- c(-1, 1)
      random_pos[needsChange] <- random_pos[needsChange] + sample(step, sum(needsChange), replace = T)
      if(counter > 5000) {stop("Shuffling never converges. Are there enough genes to shuffle to?")}
   }
    random_genes <- names(orderedMeans)[random_pos]
    
    # nice output plot:
   suppressWarnings(qqplot(
       latentMeans[B],
       latentMeans[random_genes],
       log="xy", pch = 20, cex=.8,
       main = "QQ-Plot (log-transformed Axis",
       xlab = "Signature Genes",
       ylab = "Randomly Sampled Genes") )
   legend("bottomright",
            paste0("0s: ", 
                  sum(latentMeans[B] == 0)), bty="n")
   legend("topleft",
            paste0("0s: ", 
                  sum(latentMeans[random_genes] == 0)), bty="n")
   abline(0,1)
    
   return(random_genes)
}
```

Get "pseudobulk" DEGs:
```{r}
a8 <- sample(x = which(isCD8Tcell), size = 125)
b8 <- which(isCD8Tcell)[! which(isCD8Tcell) %in% a8]

a4 <- sample(x = which(isCD4Tcell), size = 1300)
b4 <- which(isCD4Tcell)[! which(isCD4Tcell) %in% a4]

library(DESeq2)

pseudobulk <- DESeqDataSetFromMatrix(
  countData = cbind( CD4_a = rowSums(rawC[, a4]),
                     CD4_b = rowSums(rawC[, b4]),
                     CD8_a = rowSums(rawC[, a8]),
                     CD8_b = rowSums(rawC[, b8])  ),
  colData = data.frame(Celltype = c("CD4", "CD4", "CD8", "CD8"),
                       row.names=c("CD4_a", "CD4_b", "CD8_a", "CD8_b")),
  design = ~ Celltype
)

pseudobulk <- DESeq(pseudobulk)
res <- results(pseudobulk)
res <- res[which(res$padj < .05),]                     

cd8_up <- rownames(res[res$log2FoldChange > 1,])
cd4_up <- rownames(res[res$log2FoldChange < -1,])
```

Separating cells works acceptably well, but it's hard to simulate a stochastic
background (orange dots).

Here is it with completely random genes. Run it multiple times and observe how
it's drastically changing everytime!:
```{r}
plot(sqrt(colSums( rawC[cd8_up,   c(which(isCD4Tcell), which(isCD8Tcell))]^2 )),
     sqrt(colSums(rawC[cd4_up,   c(which(isCD4Tcell), which(isCD8Tcell))]^2) ),
     pch = 20, cex = linMap(colSums(rawC[, c(which(isCD4Tcell), which(isCD8Tcell))]), .05, 1),
     col = c(rep("blue", sum(isCD4Tcell)), rep("red", sum(isCD8Tcell))),
     main = "Orange: Completely random genes",
     xlab = "CD8 >> CD4",
     ylab = "CD4 >> CD8")

points(sqrt(colSums( rawC[sample(rownames(rawC), length(mock_cd8_up)),   c(which(isCD4Tcell), which(isCD8Tcell))]^2 )),
       sqrt(colSums(rawC[sample(rownames(rawC), length(mock_cd4_up)),    c(which(isCD4Tcell), which(isCD8Tcell))]^2) ),
       pch = 20, cex = linMap(colSums(rawC[, c(which(isCD4Tcell), which(isCD8Tcell))]), .05, 1),
       col = "orange")

```


Here we pick genes with same means, expecting naively this would balance out
things. But no, CD4 cells are 10x more abundant, and so genes with similar means
often are those expressed highly by CD4 cells - again biasing us:
```{r}
v_T <- VMRstats(rawC[, c(which(isCD4Tcell), which(isCD8Tcell))])
rownames(v_T)

mock_cd8_up <- comparableGenes(cd8_up, rawC[, c(which(isCD4Tcell), which(isCD8Tcell))],
                               notThese = rownames(v_T[v_T$aboveP<0,]) )
mock_cd4_up <- comparableGenes(cd4_up, rawC[, c(which(isCD4Tcell), which(isCD8Tcell))],
                               notThese = rownames(v_T[v_T$aboveP<0,]) )


plot(sqrt(colSums( rawC[cd8_up,   c(which(isCD4Tcell), which(isCD8Tcell))]^2 )),
     sqrt(colSums(rawC[cd4_up,   c(which(isCD4Tcell), which(isCD8Tcell))]^2) ),
     pch = 20, cex = linMap(colSums(rawC[, c(which(isCD4Tcell), which(isCD8Tcell))]), .05, 1),
     col = c(rep("blue", sum(isCD4Tcell)), rep("red", sum(isCD8Tcell))),
     main = "Orange: non-HVGs, same means",
     xlab = "CD8 >> CD4",
     ylab = "CD4 >> CD8")

points(sqrt(colSums( rawC[mock_cd8_up,   c(which(isCD4Tcell), which(isCD8Tcell))]^2 )),
       sqrt(colSums( rawC[mock_cd4_up,    c(which(isCD4Tcell), which(isCD8Tcell))]^2) ),
       pch = 20, cex = linMap(colSums(rawC[, c(which(isCD4Tcell), which(isCD8Tcell))]), .05, 1),
       col = "orange")



plot(rowMeans(normC[mock_cd4_up, isCD4Tcell]), rowMeans(normC[cd4_up, isCD8Tcell])); abline(0,1)
# Even when filtering out the non-variable genes, we still get genes manifold higher
# in CD4 than in CD8 - explaining why the orange dots' locations are so skewed / overlayed
# with the CD4 cells!
```

I suppose we have to take random genes with similar means, but only from the
uncorrelated genes.

```{r}

rawCountMatrix <- rawC[,   c(which(isCD4Tcell), which(isCD8Tcell))]
genesA <- cd4_up
genesB <- cd8_up


cs <- colSums(rawCountMatrix)
metaGene_A <- colSums(rawCountMatrix[genesA, ]) / cs
metaGene_B <- colSums(rawCountMatrix[genesB, ]) / cs

metaCors_A <- apply(t(rawCountMatrix) / cs, 2, function(gene) cor(metaGene_A, gene, method = "spearman"))
metaCors_B <- apply(t(rawCountMatrix) / cs, 2, function(gene) cor(metaGene_B, gene, method = "spearman"))

metaGene4 <-    colSums(rawC[cd4_up,   c(which(isCD4Tcell), which(isCD8Tcell))])
metaGene4 <- metaGene4 / cs
metaCors4 <- apply(t(rawC[! rownames(rawC) %in% cd4_up,   c(which(isCD4Tcell), which(isCD8Tcell))]) / cs,
      2,
      function(gene) cor(metaGene4, gene, method = "spearman"))

corCutoff <- .002
plot(colMeans(t(rawC[! rownames(rawC) %in% cd4_up,
                     c(which(isCD4Tcell), which(isCD8Tcell))]) / cs),
     metaCors4,
     pch=20,
     cex=.1,
     log="x",
     col = ifelse(abs(metaCors4) < corCutoff, "red", "black")
     )
# even at very stringent cutoff for correlations, we can still choose genes
# from the entire range of means (red) reasonably well. Let's try.

nonCorrelated <- names(colMeans(t(rawC[! rownames(rawC) %in% cd4_up,
                      c(which(isCD4Tcell), which(isCD8Tcell))]) / cs)[which(abs(metaCors4) < corCutoff)])




noncor_mock_4 <-comparableGenes(cd4_up, rawC[, c(which(isCD4Tcell), which(isCD8Tcell))],
                notThese = rownames(rawC)[! rownames(rawC) %in% nonCorrelated] )
noncor_mock_8 <-comparableGenes(cd8_up, rawC[, c(which(isCD4Tcell), which(isCD8Tcell))],
                notThese = rownames(rawC)[! rownames(rawC) %in% nonCorrelated] )



plot(sqrt(colSums( rawC[cd8_up,   c(which(isCD4Tcell), which(isCD8Tcell))]^2 )),
     sqrt(colSums(rawC[cd4_up,   c(which(isCD4Tcell), which(isCD8Tcell))]^2) ),
     pch = 20, cex = linMap(colSums(rawC[, c(which(isCD4Tcell), which(isCD8Tcell))]), .05, 1),
     col = c(rep("blue", sum(isCD4Tcell)), rep("red", sum(isCD8Tcell))),
     xlab = "CD8 >> CD4",
     ylab = "CD4 >> CD8",
     asp = 1)

points(sqrt(colSums( rawC[noncor_mock_8,   c(which(isCD4Tcell), which(isCD8Tcell))]^2 )),
       sqrt(colSums( rawC[noncor_mock_4,    c(which(isCD4Tcell), which(isCD8Tcell))]^2) ),
       pch = 20, cex = linMap(colSums(rawC[, c(which(isCD4Tcell), which(isCD8Tcell))]), .05, 1),
       col = "orange")

```








# Metric for gene signatures

Let's say we know between 10 and 500 genes important for a given cell type or
condition (e.g. from bulk sequencing, literature markers, GO terms, etc.).
What is the best way to summarize them in scRNAseq?
I'll use markers discriminating CD4 and CD8 T cells here.

## With Simon
## Continue here
## continue here
## continue here
```{r}
library(MASS)

normT_ids   <- c(which(isCD4Tcell), which(isCD8Tcell))
normT_class <- c( rep("CD4", sum(isCD4Tcell)), rep("CD8", sum(isCD8Tcell)))

# let's classify them with just a few excellent genes:
fit4 <- lda(formula = Celltype ~ HUMAN_CD8B+ HUMAN_S100B +
                                HUMAN_CD4 + HUMAN_GPR183 + HUMAN_GNG8,
           # Celltype has to be last (dumb implementation design?)
           data=data.frame(t(normC[, normT_ids]), Celltype = normT_class ),
           CV = TRUE)
plot(fit4$posterior) # note that the class posteriors are each other's inverses
data.frame(TrueCT = normT_class, fit4$posterior) %>% ggplot(aes(TrueCT, CD8, col = TrueCT)) + geom_jitter()
# this is already surprisingly good considering there's only 4 genes going into it!


# now all genes.
vmrT <- VMRstats(rawC[, normT_ids])

# Crossvalidation runs into 'all-zeros problem' for lowly expressed genes,
# so we also filter them:
hvg_T <- rownames(vmrT[vmrT$aboveP > 1, ])

tmp <- as.data.frame(apply(normC[hvg_T, normT_ids], 1, function(gene) {
  gene + rnorm(n = length(gene), sd=1e-7)
}) ) # cells are now in rows
tmp$Celltype = normT_class


 77   81   86   88   92   96   98   99  104  109  114  117  122  138  152  153  
 165  170  184  193  204  207  212  215  216  217  222  228  232  236  239  246
 250  251  257  261  263  266  268  269  272  275  283  290  299  302  307  309 
 310  311  313  314  316  318  320  322  324  325  326  329  331  333  335  340  341  347  353  358  361  363  364  365  369  370  371  373  374  375  376  381  385  386  387  389  393  400  401  402  404  408  413  415  417  422  423  425  427  428  430  441  445  447  450  451  457  459  460  468  470  472  474  476  478  479  480  482  483  488  491  493  494  496  499  504  509  514  518  519  520  522  524  526  528  529  532  534  535  536  537  538  539  540  541  542  543  545  547  550  553  554  555  560  561  566  568  571  572  576  578  582  584  585  586  587  590  591  594  596  597  602  603  604  605  606  609  610  611  613  617  619  621  623  624  628  629  632  634  637  639  640  641  644  646  647  653  655
fit <- lda(formula = Celltype ~ .,
           data = tmp[, c(1:76, 2811)]
          )

fit <- lda(formula = Celltype ~ .,
           # Celltype has to be last (dumb implementation design?)
           data=data.frame(t(tmp[1:77, ]), Celltype = normT_class ),
           # CV often runs into all-zero problem - dumb implementation design?!)
           CV = TRUE)
data.frame(TrueCT = normT_class, fit$posterior, predictedClass=fit$class) %>% ggplot(aes(TrueCT, CD8, col = TrueCT)) + geom_jitter()
#  Questions:
#
#   why is exactly 1 cell's posterior estimate NA? For PCA-based below it's hundreds
#







# or perhaps pca:
p <- prcomp(t(normC[rownames(vmrT[vmrT$aboveP > 1,]),
                    normT_ids]))
fitp <- lda(formula = Celltype ~ .,
           # Celltype has to be last (dumb implementation design?)
           data=data.frame(p$x[, 1:1000], Celltype = normT_class ),
           CV = TRUE)
data.frame(TrueCT = normT_class, fitp$posterior, predictedClass=fitp$class) %>% ggplot(aes(TrueCT, CD8, col = TrueCT)) + geom_jitter()
nap <- is.na(fitp$posterior[,1])
table(normT_class[!nap], fitp$class[!nap])
# That looks awesome. Questions:
#  Why does lda assign a class if the posterior value is NA?!
#  see:   table(prediction=fit$class, normT_class)
#  Why are some NA to begin with?! See    table(is.na(fit$posterior[,1]))
#



```




## My own stuff

```{r}
sepScore1 <- apply(normC, 1, function(gene) {
    sepsc <- mean(gene[isCD8Tcell]) - mean(gene[isCD4Tcell]) /
               (sd(gene[isCD8Tcell]) + sd(gene[isCD4Tcell]))
  
})

sepScore2 <- rowMeans(normC[, isCD8Tcell] - rowMeans(normC[, isCD4Tcell]))



# candidate markers from using entire CITEseq dataset (250 CD8 vs 2768 CD4 T cells)
ms_8 <-c( 
  "HUMAN_CD8A", "HUMAN_CD8B", "HUMAN_S100B", "HUMAN_ZFP36L2",
  "HUMAN_JUNB", "HUMAN_KLF6",  "HUMAN_CFL1", "HUMAN_SERPINF1", 
  "HUMAN_PFN1", 
  "HUMAN_DUSP2", "HUMAN_GUK1", "HUMAN_GAS5",    "HUMAN_PLAC8", 
  "HUMAN_C6orf48",    "HUMAN_SRSF3",    "HUMAN_NELL2", "HUMAN_ZNF331", "HUMAN_NKG7", 
  "HUMAN_GYPC", "HUMAN_C1orf63")
ms_4 <- c(
    "HUMAN_TXNIP",   "HUMAN_CORO1B", "HUMAN_GPR183", "HUMAN_ITM2A", 
    "HUMAN_TOMM7",    "HUMAN_SOCS3",    "HUMAN_FXYD5",     "HUMAN_CD69",
    "HUMAN_RNASET2",     "HUMAN_CD52",    "HUMAN_ANXA1",     "HUMAN_IL7R",
    "HUMAN_LDHB",     "HUMAN_FTH1", "HUMAN_MARCKSL1",     "HUMAN_GZMA",
    "HUMAN_CD37", "HUMAN_XBP1", "HUMAN_JUN", "HUMAN_RPL8", "HUMAN_CD4", 
    "HUMAN_BCL11B"
    )

# manually pick best markers from heatmap:
pheatmap(normC[ms_8,selecteven], cluster_rows = F, cluster_cols = F, gaps_col = sum(isCD8Tcell) , color = colPal2, show_colnames = F)


m_8<- c(
  "HUMAN_CD8A", "HUMAN_CD8B", "HUMAN_S100B",  "HUMAN_SERPINF1", "HUMAN_DUSP2", 
  "HUMAN_GUK1", "HUMAN_NELL2")
m_4<- c(
  "HUMAN_TXNIP",   "HUMAN_CORO1B", "HUMAN_GPR183", 
  "HUMAN_SOCS3",    "HUMAN_RNASET2", "HUMAN_IL7R",
  "HUMAN_MARCKSL1",     "HUMAN_GZMA",
   "HUMAN_CD4" )


# Final markers:
pheatmap(normC[c(m_4, m_8),selecteven], cluster_rows = F, cluster_cols = F,
         gaps_col = 250 , gaps_row = length(m_4),scale = "row", color = colPal3, main = "Markers to discriminate CD8 and CD4 T cells (2x 250 cells shown)",
         show_colnames = F)


# Probably linear combinations will fail if you do not scale the values? Not sure...
```
Can PCA resolve these Cells?
```{r}
mp_noS <- data.frame(PC1=prcomp(t(normC[c(m_4, m_8), normT_ids]))$x[, 1],
             ct=normT_class) %>%
  ggplot(aes(ct, PC1, col=ct))+geom_jitter() + ggtitle("PCA on log1p(umi/totalUMI)\nNo Scaling")

mp_S <- data.frame(PC1=prcomp(t(normC[c(m_4, m_8), normT_ids]), scale. = T)$x[, 1],
             ct=normT_class) %>%
  ggplot(aes(ct, PC1, col=ct))+geom_jitter() + ggtitle("PCA on log1p(umi/totalUMI)\nScaling")


m <- CreateSeuratObject(rawC[, normT_ids])
m <- NormalizeData(m)
m <- FindVariableGenes(m)
m <- ScaleData(m, genes.use = c(m_4, m_8), vars.to.regress = "nUMI")
m <- RunPCA(m, pc.genes = c(m_4, m_8), pcs.compute = 7, do.print = F)
seu <- data.frame(PC1=m@dr$pca@cell.embeddings[,1], ct=normT_class) %>%
  ggplot(aes(ct, PC1, col=ct))+geom_jitter() + ggtitle("Regress nUMI with Seurat")

cowplot::plot_grid(mp_noS, mp_S, seu, ncol = 3)
```

I observe that PCA on a few selected genes can be highly effective, but only
with scaling.


Can we sum up gene counts and get nice results?

```{r}
rawC[m_4, normT_ids]
#    ...
#    ...
#    ...
#    ...
```


Could the ME algorithm divide each gene into two expression groups, which
we could then use somehow to compute cell-cell distances?

```{r}

data.frame(index=normT_ids, ct=normT_class, t(normC[m_4, normT_ids]) / colSums(rawC[, normT_ids])) %>%
  gather(key = "Gene", "Expression", -index, -ct) %>% 
 ggplot() + geom_histogram(aes(Expression, fill = ct), position = "dodge") +facet_wrap(~Gene)
ggplot(aes(ct, Expression, col = ct)) + geom_violin() + facet_wrap(~Gene, scales = "free_y") + geom_jitter(size=.4)

# with equal groups (even equal library sizes):
data.frame(index=selecteven, ct=ct, t(normC[m_4, selecteven]) / colSums(rawC[, selecteven])) %>%
  gather(key = "Gene", "Expression", -index, -ct) %>% 
ggplot(aes(ct, Expression, col = ct)) + geom_violin() + facet_wrap(~Gene, scales = "free_y") + geom_jitter(size=.4)

data.frame(index=selecteven, ct=ct, t(normC[m_8, selecteven]) / colSums(rawC[, selecteven])) %>%
    gather(key = "Gene", "Expression", -index, -ct) %>% 
    ggplot() + geom_histogram(aes(Expression, fill = ct), position = "dodge") +facet_wrap(~Gene)

```







## unfinished section, not sure if important
PCA on all genes:
```{r}
# even libsizes:
  pc_even  <- prcomp( t(normC[, selecteven]))
  # get this function from script 'poisson_simulations.R':
  ct_boxplot(cellInfo = factor(ct, levels = unique(ct)),
             pca =  pc_even)
  plot(pc_even$x[,2:3], col = factor(ct, levels=unique(ct)))
  #    almost perfect separation, very nice.
  plot(colSums(rawC[, selecteven]), 
       pc_even$x[, 2], pch=20, cex=.4, col = factor(ct, levels = c("CD4", "CD8")))
  
  
  

# realistic (uneven) libsizes:
  pc_few <- prcomp( t(normC[, selectFew]))
  # get this function from script 'poisson_simulations.R':
  ct_boxplot(cellInfo = factor(ct, levels = unique(ct)),
             pca =  pc_few)
  cs_few <- colSums(rawC[, selectFew])
  plot(colSums(rawC[, selectFew]), 
       pc_few$x[, 1], pch=20, cex=.4, col = factor(ct, levels = c("CD4", "CD8")))
  
  
  
# on all cells:
  pc_all <- irlba::irlba(t(normC), nv =25)
  
  ct_citeseq <- ifelse(isCD4Tcell, "CD4", "other") # levels: CD4 & other
  ct_citeseq <- ifelse(isCD8Tcell, "CD8", ct_citeseq) # levels: CD4 & other
  ct_citeseq <- factor(ct_citeseq, levels = c("CD4", "CD8", "other"))

  par(mfrow = c(2,3))
  sapply(1:6, function(PC) {
    scores <- pc_all$u[, PC]
    plot(ct_citeseq, scores, main = PC)
  })
  
  
```












# Playzone




## [9th January]: T statistic as readout

Compute distances with Seurat:
```{r}
library(Seurat)
s <- CreateSeuratObject(rawC[, c(which(isCD4Tcell), which(isCD8Tcell))])
s <- NormalizeData(s, display.progress = F)
s <- FindVariableGenes(s, do.plot = F, display.progress = F)
s <- ScaleData(s, genes.use = s@var.genes, vars.to.regress = "nUMI", display.progress = F)
s <- RunPCA(s, pc.genes = s@var.genes, do.print = F)
ds <- as.matrix(dist( s@dr$pca@cell.embeddings[, 1:10] ))
```

Alternatively, compute distances by hand:
```{r}
normRandom <- log1p( t(rawC[, selectFew]) / colSums(rawC[, selectFew]) )
vFew <- VMRstats(rawC[, selectFew])
p_few <-  prcomp(normRandom[, rownames(vFew[vFew$aboveP > 2, ])], scale. = FALSE)
ds <- as.matrix(dist( p_few$x[, 1:10]  ))
```

Use t-test on distances to see how close/far each cell is to CD4s/CD8s:
```{r}
res <- t(sapply(1:nrow(ds), function(i){
  x <- ds[i, 1:250]
  y <- ds[i, 251:500]
  if(i <= 250) {
                x <- x[-i] }else{
                y <- y[-(i-250)] }
  res <- t.test(x = x, y=y, var.equal = F) 
  return(c(tstat=res$statistic, p=res$p.value))
})); colnames(res) <- c("tstat", "p")


plotDF <- data.frame(#s@dr$tsne@cell.embeddings,
           ct=ct,
           libsize = colSums(rawC[, selectFew]),
           res,
           FDR = p.adjust(res[, "p"], "BH"))

```


```{r}
ggplot(plotDF, aes(ct, tstat, col = ct)) + geom_jitter(aes(alpha = FDR < .05))
```



## [old] Supervised UMAP (with labels)
Only exists in python implementation so far, but here I play around with Simon
on 9th january.
You can delete this section soon.

```{r}
library(umap)

# To create a custom configuration, create a copy of the defaults and then update any of the fields. For example, let’s change the seed for random number generation.
custom.config = umap.defaults
custom.config$random_state = 123
# Another way to customize the algorithm is to specify the non-default parameters explicitly. To achieve equivalent results to the above, we can thus use
iris.umap.3 = umap(iris.data, random_state=123)


# to use python package umap-learn rather than R-implementation:
iris.umap.4 = umap(iris.data, method="umap-learn")


help(umap.defaults)
```



```{r}
rawT <- rawC[, c(which(isCD4Tcell), which(isCD8Tcell))]
normT <- t(log1p( t(rawT) / colSums(rawT) ))


# "gold standard cells":
gold4 <- which(rawT["HUMAN_CD4", ] > 0 & rawT["HUMAN_CD8B", ] == 0)
gold8 <- which(rawT["HUMAN_CD4", ] == 0 & rawT["HUMAN_CD8B", ] > 0)
plot(jitter(t(rawT[c("HUMAN_CD4", "HUMAN_CD8B"), ]), factor = 1), pch = 20, cex=.4,
     col = 1 + 1 * (1:ncol(rawT) %in% gold4) + 2 * (1:ncol(rawT) %in% gold8))

# umap on goldstandard cells:
u <- umap( t(normT[, c(gold4, gold8)]))
plot(u$layout, col = c(rep(2, length(gold4)), rep(3, length(gold8))))

u2 <- predict(u, t(normT))
plot(u2, col = c(rep(2, sum(isCD4Tcell)), rep(3, sum(isCD8Tcell))), asp = 1,
     pch = 20, cex=.4)


```









# ML on PCs


## ...Amongst T cells: CD4 and CD8
```{r}
library(umap)
library(glmnet)
rawT  <- rawC[, c(which(isCD4Tcell), which(isCD8Tcell))]
vT    <- VMRstats(rawT)
normT <- log1p( t(rawT) / colSums(rawT) )
pT    <- prcomp(normT[, rownames(vT[vT$aboveP > 2,])], scale. = TRUE)
uT    <- umap(pT$x[, 1:10])
data.frame(uT$layout, ct=c( rep("CD4", sum(isCD4Tcell)), rep("CD8", sum(isCD8Tcell)))) %>%
  ggplot(aes(X1, X2, col=ct)) + geom_point()



# "gold standard cells":
gold4 <- which(rawT["HUMAN_CD4", ] > 0 & rawT["HUMAN_CD8B", ] == 0)
gold8 <- which(rawT["HUMAN_CD4", ] == 0 & rawT["HUMAN_CD8B", ] > 0)
plot(jitter(t(rawT[c("HUMAN_CD4", "HUMAN_CD8B"), ]), factor = 1), pch = 20, cex=.4,
     col = 1 + 1 * (1:ncol(rawT) %in% gold4) + 2 * (1:ncol(rawT) %in% gold8))
# train on goldstandard cells:
goldTrain <-cv.glmnet(x = pT$x[c(gold4, gold8), ],
          y = factor(c(
            rep("CD4", length(gold4)),
            rep("CD8", length(gold8))),
            levels = c("CD4", "CD8")    ),
          family = "binomial",
          alpha = 1
          )

# check out training results
plot(coef(goldTrain, s = "lambda.1se"), xlab = "PC", ylab="Regression coefficient")


# predict all
goldTest <- predict(goldTrain, newx = pT$x[, ], s = "lambda.1se", type = "response")



plot(goldTest, cex=.4, pch=20,
     col = 1 + (1:nrow(goldTest) %in% gold4 + 2 *(1:nrow(goldTest) %in% gold8)));abline(v=sum(isCD4Tcell))

data.frame(uT$layout,
           ct=c( rep("CD4", sum(isCD4Tcell)), rep("CD8", sum(isCD8Tcell))),
           response = goldTest[,1]) %>%
  ggplot(aes(X1, X2, col= response)) + geom_point() + scale_color_gradient2(midpoint = .5)

```


## ...Entire CITEseq data

```{r}
library(tidyverse)
library(irlba)
library(glmnet)
library(umap)

v <- VMRstats(rawC)

normC <- log1p(t( t(rawC)  / colSums(rawC)))
i <- irlba::prcomp_irlba(
  scale(t(normC[rownames(v[v$aboveP > 2,]),])), n = 100)

# UMAP:
u <- umap(i$x[, 1:30])
plot(u$layout, col = 1 + 1*(isTcell)+2*(isBcell) + 2*(isCD8Tcell), pch=20, cex=.4, asp=1)

plot(jitter(rawC["HUMAN_CD3E", ]), jitter(rawC["HUMAN_CD79B",]), pch=20, cex=.4)
goldT <- which(rawC["HUMAN_CD3E", ] > 0)
goldB <- which(rawC["HUMAN_CD79B", ] > 0)
goldM <- c(which(rawC["HUMAN_CD14", ] > 0),
           which(rawC["HUMAN_FCGR3A", ] > 0  & rawC["HUMAN_NCAM1", ] == 0 ))

idxA<- goldM
idxB<- setdiff(1:ncol(rawC), idxA)
# train on goldstandard cells:
goldTrain <-cv.glmnet(x = i$x[c(idxA, idxB), ],
          y = factor(c(
            rep("A", length(idxA)),
            rep("B", length(idxB))),
            levels = c("A", "B")    ),
          family = "binomial",
          alpha = 1
          )
# predict all
goldTest <- predict(goldTrain, newx = i$x, s = "lambda.1se", type = "response")

ggplot(data.frame(u$layout,
                  response = goldTest[,1],
                  Gene = normC["HUMAN_NCAM1",]),)+
  geom_point(aes(X1, X2, col = response), size =1)+
  coord_fixed()+
  scale_color_gradient2(midpoint = .5)   
```

```{r}
legend <- get_legend(p_T+ theme(legend.direction = "vertical"))
plot_grid(p_T + guides(color=F)+xlab(NULL)+ylab(NULL),
          p_B + guides(color=F)+xlab(NULL)+ylab(NULL),
          p_M + guides(color=F)+xlab(NULL)+ylab(NULL),
          p_M2 + guides(color=F)+xlab(NULL)+ylab(NULL),
          legend,
          ncol=2,
          rel_heights = c(4,4,1)
          )
```






























# ---------------------------------------
# Old parts of script, delete soon
# ---------------------------------------

## B vs T cells: all ok

```{r}
# cell types in alphabetical order (otherwise doesn't match ordered selectBT):
ctBT <- c(  rep("B", sum(isBcell)),  rep("T", 500) )
selectBT <- c(which(isBcell), sample(which(Ts), 500) )
selectBT <- selectBT[ order(ctBT, colSums(rawC[, selectBT]))]


normBT <- log( t(t(rawC[, selectBT]) * median(colSums(rawC[, selectBT])) /
                  colSums(rawC[, selectBT])  ) + 1 )

# preselect genes very coarsely for speedup in PCA:
vmrBT <- VMRstats(rawC[, selectBT])
coarse_pickBT <- rownames( vmrBT[vmrBT$log_Gene_VMR > vmrBT$log_PoissonVMR_median,] )

pcBT<- prcomp((t(normBT[ coarse_pickBT, ])), center = TRUE, scale. = FALSE)
```


```{r}
 sqDs <- matrix(0, nrow = nrow(pcBT$x), ncol = ncol(pcBT$x))    
 BT_DF <- tibble()
for(p in 1:ncol(pcBT$x)) {
 if(p %% 10 == 0) print(p)
 sqDs <<- sqDs + sqDeviation(pcBT$x[, p])
 nns  <- NNindices(as.matrix(sqrt(sqDs)), n_neighbors = 31)
 # within NNs we can now ask for interesting statistics:
 correct_assignments <- apply(nns, 1, function(ids)
      sum(ctBT[ids[2:ncol(nns)]] == ctBT[ids[1]]))
 tmp <- tibble(  correct_assignments, Celltype = ctBT,
                       PCs = p) %>% # number of PC
   group_by(Celltype, PCs) %>%
   summarise(Mean_correctNeighbors = mean(correct_assignments))  
 BT_DF <<- bind_rows(BT_DF, tmp) 
}

p_BT<- ggplot(BT_DF) + geom_point(aes(PCs, Mean_correctNeighbors, colour = Celltype))
p_BT 

BTnn10 <- NNindices( as.matrix(dist(pcBT$x[, 1:10])), n_neighbors = 31)
as.data.frame(stratNs(BTnn10, ctBT)) %>%
  gather("Ntype", "Neighborhoods", -ID) %>%
  ggplot()+geom_point(aes(ID, Neighborhoods, col = Ntype))

BTnn400 <- NNindices( as.matrix(dist(pcBT$x[, 1:400])), n_neighbors = 31)
as.data.frame(stratNs(BTnn400, ctBT)) %>%
  gather("Ntype", "Neighborhoods", -ID) %>%
  ggplot()+geom_point(aes(ID, Neighborhoods, col = Ntype))


plot( as.numeric(table(BTnn10)), pch = 20, col = 3 + (ctBT=="T"),
      xlab = "B cells (green), T cells (blue), ordered by library size",
      ylab = "Dots: N30 neighborhoods. Line: libsize (A.U.)", 
      main = "265 B cells and 500 T cells\nDistances on first 10 PCs")
lines( colSums(rawC[, selectBT]) / 100)
```













## CD4 vs CD8 T cells - no regressing

For an increasing number of genes, the below chunk computes euclidean distance
between cells. Depending on which line is commented in or out, it computes it
either directly on normalized expression or on the first 30 PCs. 
Overall we see that varying number of input genes into PCA does not change much:
```{r}
sel <- c(which(isCD4Tcell), which(isCD8Tcell))
identities <- c( rep("CD4", sum(isCD4Tcell)), rep("CD8", sum(isCD8Tcell)) )

vmr <- VMRstats(rawC[, sel] )
norm2 <- log( t(t(rawC[, sel]) * median(colSums(rawC[, sel])) /
                  colSums(rawC[, sel])  ) + 1 )



for(ng in 2^(5:14)) {
  print(ng)
  fast <- ifelse(ng > 500, TRUE, FALSE)
  #########  eucl. distance on PCs:
  # d <- pcDist(norm2[rownames(head(vmr, n = ng)), ], do.fast = fast)
  #########  eucl. distance directly on gene expression (very poor performance):
  d <- parDist(t(norm2[rownames(head(vmr, n = ng)), ]) )
  
  nns <- NNindices(as.matrix(d), n_neighbors = 11 )
  correct_assignments <- apply(nns, 1, function(ids)
    sum(identities[ids[2:11]] == identities[ids[1]]))
  cdf <- (cbind.data.frame(correct_assignments, identities))
  cdf$correct_assignments <- factor(cdf$correct_assignments, levels = 0:10)
  
  cd4 <-  data.frame(table(cdf[cdf$identities == "CD4",
                        "correct_assignments"]), Celltype = "CD4")
  cd8 <-  data.frame(table(cdf[cdf$identities == "CD8",
                        "correct_assignments"]), Celltype = "CD8")
  cdf <- data.frame(rbind(cd4, cd8),
             prop = c( cd4$Freq / sum(identities == "CD4"),
                       cd8$Freq / sum(identities == "CD8") ))
  
    
  # output nice plots:
   png(paste0(
        "/home/felix/Dropbox/PhD/Meetings-Talks-etc/labM_Anders/7_hvg_vmr_CD4CD8/",
        "lognormalization_euclidean/" ,      
        "NNs_PCs1to30_hvgs1to", ng, ".png"), width = 1000, height = 600, units = "px")
  # par(mfrow = c(1, 2)) 
  # plot(vmr$log_Gene_Mean, vmr$log_Gene_VMR, pch = 20, cex=.1, col = "#00000040",
  #      ylab = "log_VMR ( log(var/mu)", xlab = "Average expression ( log(mu) )",
  #      main = ng) 
  # points(vmr$log_Gene_Mean[1:ng], vmr$log_Gene_VMR[1:ng], pch=20, col = "red")
   
  print(ggplot(cdf) + geom_bar(aes(Var1, prop, fill = Celltype),stat = "identity", position = "dodge") + xlab("NNs of same cell type")+ ylab("Proportion") +
    ggtitle(ng))
   # deprecated plot:
   # hist(ns, xlab = "CD4 T cells amongst 100 NNs", main = paste0("CD4 and CD8 T cells\n", ng),
   #      breaks = -.5+0:101, ylim = c(0, 800) )
   dev.off()
 
}
```

## CoD on PCs (3000 cells)
Curse of dimensionality (CoD): as we increase the PCs, does euclidean distance
deteriorate?

```{r}
sel <- c(which(isCD4Tcell), which(isCD8Tcell))
identities <- c( rep("CD4", sum(isCD4Tcell)), rep("CD8", sum(isCD8Tcell)) )

vmr <- VMRstats(rawC[, sel] )
norm2 <- log( t(t(rawC[, sel]) * median(colSums(rawC[, sel])) /
                  colSums(rawC[, sel])  ) + 1 )


# compute PCA on (almost) all genes (minus exremely noisy ones, for speed):
  coarse_pick <- rownames( vmr[vmr$log_Gene_VMR > vmr$log_PoissonVMR_median,] )
  pc  <- prcomp((t(norm2[ coarse_pick, ])), center = TRUE, scale. = FALSE)
  # I used to scale and center here but scaling is debatable.
  # Note that irlba's implementation in Seurat does both.
```


```{r}
sqDs <- matrix(0, nrow = nrow(pc$x), ncol = ncol(pc$x))    
T_DF <- tibble()
for(p in 1:1000) {
 sqDs <<- sqDs + sqDeviation(pc$x[, p])

 if(p %% 50 ==0 | p < 50) { 
   print(p)
  nns  <- NNindices(as.matrix(sqrt(sqDs)), n_neighbors = 31)
  correct_assignments <- apply(nns, 1, function(ids)
      sum(identities[ids[2:ncol(nns)]] == identities[ids[1]]))
  tmp <- tibble(  correct_assignments, Celltype = identities,
                       PCs = p) %>% # number of PC
   group_by(Celltype, PCs) %>%
   summarise(Mean_correctNeighbors = mean(correct_assignments))  
 T_DF <<- bind_rows(T_DF, tmp) 
 }
}

 
p_T <- ggplot(T_DF) + geom_point(aes(PCs, Mean_correctNeighbors, colour = Celltype)) +
  ggtitle("2768 CD4 T cells, 250 CD8 T cells")
```
There are roughly 10x more CD4 
than CD8 T cells. So without any transcriptomic differences, we'd expect the
red line around 29 and the blue line around 1. So I suppose this is not bad.

```{r}
Tnn8<- NNindices( as.matrix(dist(pc$x[, 1:8])), n_neighbors = 31)
as.data.frame(stratNs(Tnn8, identities)) %>%
  gather("Ntype", "Neighborhoods", -ID) %>%
  ggplot()+geom_point(aes(ID, Neighborhoods, col = Ntype))



Tnn250<- NNindices( as.matrix(dist(pc$x[, 1:250])), n_neighbors = 31)
as.data.frame(stratNs(Tnn250, identities)) %>%
  gather("Ntype", "Neighborhoods", -ID) %>%
  ggplot()+geom_point(aes(ID, Neighborhoods, col = Ntype))
```



## CoD on PCs (250 + 250 cells)


### More PCs, popular neighbors and nUMI
We can see that at one point, the more PCs we include in the distance computation,
the less resolution we have to separate CD4 and CD8 T cells.
```{r}
sqDs <- matrix(0, nrow = nrow(pcFew$x), ncol = ncol(pcFew$x))    
few_DF <- tibble()
for(p in 1:500) {
 if(p %% 10 == 0) print(p)
 sqDs <<- sqDs + sqDeviation(pcFew$x[, p])
 nns  <- NNindices(as.matrix(sqrt(sqDs)), n_neighbors = 31)
 correct_assignments <- apply(nns, 1, function(ids)
      sum(ctFew[ids[2:ncol(nns)]] == ctFew[ids[1]]))
 tmp <- tibble(  correct_assignments, Celltype = ctFew,
                       PCs = p) %>% # number of PC
   group_by(Celltype, PCs) %>%
   summarise(Mean_correctNeighbors = mean(correct_assignments))  
 few_DF<<- bind_rows(few_DF, tmp) 
}

p_few <- ggplot(few_DF) + geom_point(aes(PCs, Mean_correctNeighbors, colour = Celltype))+
    ggtitle("(Almost) all genes -> PCA on 250 CD4 and 250 CD8 T cells\n-> eucl. distance on n PCs") + ylim(c(0,30))
p_few + geom_vline(xintercept = 9)  


tsneFew_9 <- Rtsne(pcFew$x[, 1:9])
plot(tsneFew_9$Y, pch =20, col = 2+ .5 * incFew, main = "250/250 CD4/CD8 T cells\nPCs 1-9")

tsneFew_25 <- Rtsne(pcFew$x[, 1:25])
plot(tsneFew_25$Y, pch =20, col = 2+ .5 * incFew, main = "250/250 CD4/CD8 T cells\nPCs 1-25")

tsneFew_200 <- Rtsne(pcFew$x[, 1:200])
plot(tsneFew_200$Y, pch =20, col = 2+ .5 * incFew, main = "250/250 CD4/CD8 T cells\nPCs 1-200")
```



```{r}
Fewnn4 <- NNindices( as.matrix(dist(pcFew$x[, 1:4])), n_neighbors = 31)
as.data.frame(stratNs(Fewnn4, ctFew)) %>%
  gather("Ntype", "Neighborhoods", -ID) %>%
  ggplot()+geom_point(aes(ID, Neighborhoods, col = Ntype))


Fewnn100 <- NNindices( as.matrix(dist(pcFew$x[, 1:100])), n_neighbors = 31)
as.data.frame(stratNs(Fewnn100, ctFew)) %>%
  gather("Ntype", "Neighborhoods", -ID) %>%
  ggplot()+geom_point(aes(ID, Neighborhoods, col = Ntype))

data.frame(celltype = ctFew, libsize = colSums(rawC[, selectFew])) %>% ggplot(aes(celltype, libsize)) + geom_violin() + geom_jitter()
```

We see that deeply sequenced cells (with high nUMI) are popular neighbors.
In fact, > 60 % of cells are no one's nearest neighbors, whereas the one or two
dozen cells with highest sequencing coverage are neighbors of 80 or 90 % of the cells.















### Regress out nUMI

```{r}
seur <- CreateSeuratObject(rawC[, selectFew],
         meta.data = data.frame(Celltype = ctFew,
                                row.names = colnames(rawC[, selectFew])))
seur@data <- normFew
# seur@var.genes <- coarse_pickFew
seur <- ScaleData(object = seur, vars.to.regress = c("nUMI"),
                  genes.use = coarse_pickFew, model.use = "negbinom")
pcReg_nb  <- prcomp((t( seur@scale.data) ), center = TRUE, scale. = FALSE)

seur <- ScaleData(object = seur, vars.to.regress = c("nUMI"),
                  genes.use = coarse_pickFew, model.use = "linear")
pcReg_l <- prcomp((t( seur@scale.data) ), center = TRUE, scale. = FALSE)

# tReg <- Rtsne(pcReg$x[, 1:150])

 sqDs <- matrix(0, nrow = 500, ncol = 500)    
 nbinom_DF <- tibble()
for(p in 1:500) {
 if(p %% 10 == 0) print(p)
 sqDs <<- sqDs + sqDeviation(pcReg_nb$x[, p])
 nns  <- NNindices(as.matrix(sqrt(sqDs)), n_neighbors = 31)
 correct_assignments <- apply(nns, 1, function(ids)
      sum(ctFew[ids[2:ncol(nns)]] == ctFew[ids[1]]))
 tmp <- tibble(  correct_assignments, Celltype = ctFew,
                       PCs = p) %>% # number of PC
   group_by(Celltype, PCs) %>%
   summarise(Mean_correctNeighbors = mean(correct_assignments))  
 nbinom_DF <<- bind_rows(nbinom_DF, tmp) 
}

 p_nbinom <- ggplot(nbinom_DF) + geom_point(aes(PCs, Mean_correctNeighbors, colour = Celltype))+ ylim(c(0,30))

 
 
 
sqDs <- matrix(0, nrow = 500, ncol = 500)    
linear_DF <- tibble()
for(p in 1:500) {
 if(p %% 10 == 0) print(p)
 sqDs <<- sqDs + sqDeviation(pcReg_l$x[, p])
 nns  <- NNindices(as.matrix(sqrt(sqDs)), n_neighbors = 31)
 correct_assignments <- apply(nns, 1, function(ids)
      sum(ctFew[ids[2:ncol(nns)]] == ctFew[ids[1]]))
 tmp <- tibble(  correct_assignments, Celltype = ctFew,
                       PCs = p) %>% # number of PC
   group_by(Celltype, PCs) %>%
   summarise(Mean_correctNeighbors = mean(correct_assignments))  
 linear_DF <<- bind_rows(linear_DF, tmp) 
}

 p_linear <- ggplot(linear_DF) + geom_point(aes(PCs, Mean_correctNeighbors, colour = Celltype)) + ylim(0,30)

```


```{r}
d <- dist(pcReg$x[, 1:20])
nns <- NNindices(d, n_neighbors = 31)

plot(1:500, table(nns), main = "How often is each cell amongst 30 NNs of all other cells?\n(1:250 are CD4, 251:500 are CD8")
```








### More Celltypes, less resolution

PCA and then cell-cell distances, this time including all cells from the dataset



```{r}
# Takes a ~ 5 GB of memory:
normCells <- log( t(t(rawC) * median(colSums(rawC)) /
                  colSums(rawC)  ) + 1 )


# compute PCA on (almost) all genes (minus exremely noisy ones, for speed):
vmrCells <- VMRstats( rawC )
coarse_pickCells<- rownames( vmrCells[vmrCells$log_Gene_VMR > vmrCells$log_PoissonVMR_median,] )

# takes ~ 10 GB of memory, runs > 25 min
# pcCells<- prcomp((t(normCells[ coarse_pickCells, ])), center = TRUE, scale. = TRUE)

```
Distances for these cells:
```{r}

sqDs <- matrix(0, nrow = 500, ncol = 500)    
 cells_DF <- tibble()
for(p in 1876:8005) {
 if(p %% 10 == 0) print(p)
 sqDs <<- sqDs + sqDeviation(pcCells$x[selectFew, p])

 #if(p %% 50 ==0 | p < 50) { 
  nns  <- NNindices(as.matrix(sqrt(sqDs)), n_neighbors = 31)
  correct_assignments <- apply(nns, 1, function(ids)
      sum(ctFew[ids[2:ncol(nns)]] == ctFew[ids[1]]))
  tmp <- tibble(  correct_assignments, Celltype = ctFew,
                       PCs = p) %>% # number of PC
   group_by(Celltype, PCs) %>%
   summarise(Mean_correctNeighbors = mean(correct_assignments))  
 cells_DF <<- bind_rows(cells_DF, tmp) 
 #}
}

ggplot(cells_DF) + geom_point(aes(PCs, Mean_correctNeighbors, colour = Celltype))+
  ggtitle("(Almost) all genes -> PCA on all 8005 cells\n-> eucl. distance between 250 CD4 and 250 CD8 T cells on n PCs") + ylim(c(0,30)) + 
  xlim(c(0,500))
```


### Celltype proportion

First we pick 250 other CD4 T cells and see how much the result differs from above,
to control for heterogeneity within the CD4 cells:
```{r}
furtherCD4 <- sample(setdiff(which(isCD4Tcell), selectFew), 250)

selectFurther <- c(furtherCD4, which(isCD8Tcell))




# find exremely noisy genes, those we exclude in PCA for speed):
vmrFurther <- VMRstats(rawC[, selectFurther] )
normFurther <- log( t(t(rawC[, selectFurther]) * median(colSums(rawC[, selectFurther])) /
                  colSums(rawC[, selectFurther])  ) + 1 )
coarse_pickFurther <- rownames(
    vmrFurther[ vmrFurther$log_Gene_VMR > vmrFurther$log_PoissonVMR_median, ] )
  
  
pcFurther<- prcomp((t(normFurther[ coarse_pickFurther, ])), center = TRUE, scale. = FALSE)
 
```

let's show the 250 other CD4 picked here give the same results as above:
```{r}
sqDs <- matrix(0, nrow = 500, ncol = 500)    
further_DF <- tibble()
for(p in 1:500) {
 if(p %% 10 == 0) print(p)
 sqDs <<- sqDs + sqDeviation(pcFurther$x[, p])

 #if(p %% 50 ==0 | p < 50) { 
  nns  <- NNindices(as.matrix(sqrt(sqDs)), n_neighbors = 31)
  correct_assignments <- apply(nns, 1, function(ids)
      sum(ctFew[ids[2:ncol(nns)]] == ctFew[ids[1]])) # ctFew still valid for further CD4
  tmp <- tibble(  correct_assignments, Celltype = ctFew,
                       PCs = p) %>% # number of PC
   group_by(Celltype, PCs) %>%
   summarise(Mean_correctNeighbors = mean(correct_assignments))  
 further_DF <<- bind_rows(further_DF, tmp) 
 #}
}
p_further <- ggplot(further_DF) + geom_point(aes(PCs, Mean_correctNeighbors, colour = Celltype))+
    ggtitle("(Almost) all genes -> PCA on 250 __other__ CD4 and 250 CD8 T cells\n-> eucl. distance on n PCs") + ylim(c(0,30))
cowplot::plot_grid(p_few, p_further, nrow = 2)
```

This shows they're highly similar. We can now investigate what happens when we do
500 CD4 and 250 CD8 T cells:

```{r}
selectProp <- c(furtherCD4, selectFew)
# selectProp now has indices of 500 CD4 cells followed by those of 250 CD8 cells.
# However, I only take __250/250__ here as well, as I want to read out the same
# cells as above:
ctProp     <- c( rep("CD4", 250), rep("CD8", sum(isCD8Tcell)) ) 

# find exremely noisy genes, those we exclude in PCA for speed):
vmrProp  <- VMRstats(rawC[, selectProp] )
normProp <- log( t(t(rawC[, selectProp]) * median(colSums(rawC[, selectProp])) /
                  colSums(rawC[, selectProp])  ) + 1 )
coarse_pickProp <- rownames(
    vmrProp[ vmrProp$log_Gene_VMR > vmrProp$log_PoissonVMR_median, ] )
  
  
pcProp <- prcomp((t(normProp[ coarse_pickProp, ])), center = TRUE, scale. = FALSE)
```

```{r}
sqDs <- matrix(0, nrow = 500, ncol = 500)    
prop_DF <- tibble()
for(p in 1:750) {
 if(p %% 10 == 0) print(p)
 sqDs <<- sqDs + sqDeviation(pcProp$x[251:750, p])
 nns  <- NNindices(as.matrix(sqrt(sqDs)), n_neighbors = 31)
  correct_assignments <- apply(nns, 1, function(ids)
      sum(ctProp[ids[2:ncol(nns)]] == ctProp[ids[1]]))
  tmp <- tibble(  correct_assignments, Celltype = ctProp,
                       PCs = p) %>% # number of PC
   group_by(Celltype, PCs) %>%
   summarise(Mean_correctNeighbors = mean(correct_assignments))  
 prop_DF <<- bind_rows(prop_DF, tmp) 
}


p_prop <- ggplot(prop_DF) + geom_point(aes(PCs, Mean_correctNeighbors, colour = Celltype))+
    ggtitle("(Almost) all genes -> PCA on 500 CD4 and 250 CD8 T cells\n-> eucl. distance between same 250/250 cells as above on n PCs") + ylim(c(0,30))
cowplot::plot_grid(p_few, p_prop, nrow = 2)
```






## Kolmogorow-Smirnow statistics

### Idea
Let's walk up the density histograms from the left side and in a cumsum, we 
count upward when we encounter a CD8 cell and downward when we encounter a
CD4 cell.
Illustration:
```{r}
# pc is object computed in CoD on PCs
d <- dist(pcFew$x[, 1:30])


id_g = which(rownames(pcFew$x) == "GGAACTTAGGTGTGGT")
ggplot( data.frame(Celltype = ctFew[- id_g], Distance = as.matrix(d)[ id_g, -id_g]) ) + geom_histogram( aes(Distance, y = ..density.., fill = Celltype) , binwidth = 1, alpha = .4, position = "identity") + ggtitle("Good CD8 cell")

id_b = which(rownames(pcFew$x) == "CCGTTCACAGGCGATA")
ggplot( data.frame(Celltype = ctFew[- id_b], Distance = as.matrix(d)[ id_b, -id_b]) ) + geom_histogram( aes(Distance, y = ..density.., fill = Celltype) , binwidth = 1, alpha = .4, position = "identity") + ggtitle("Bad CD8 cell")
```


```{r}
n_PCs <- 30
d <- dist(pcFew$x[, 1:n_PCs])


id <- id_g
do <- order(as.matrix(d)[id, ])
x <- cumsum(incFew[do])
plot( cumsum(incFew[do]), xlab = "NN          <---->         furthest cells" , main = "CD8 enrichment for bad CD8 cell", ylim = c(0, 120))

```




```{r}
n_PCs <- 30
d <- dist(pcFew$x[, 1:n_PCs])

# we can use statistical test as well, but not sure that's sound:
ks.test(x = which(ctFew[do] == "CD8"), 
             y = which(ctFew[do] == "CD4") )

# for a given distance matrix d, compute nearest neighbor enrichments
    enrichments <- t(sapply(1:500, function(id) {
      
     do <- order(as.matrix(d)[id, ])
     x <- cumsum(incFew[do])
     return( c(ES = x[which.max(abs(x))],
               NN_at_max = which.max(abs(x))) )
     
    }))
# for a given distance matrix d, compute Kolmogorov-Smirnov test

ksresults <- t( sapply(1:500, function(id) {
    do <- order(as.matrix(d)[id, ])
    ks <- ks.test(x = which(ctFew[do] == "CD8"), 
                  y = which(ctFew[do] == "CD4") )
    c(D_statistic = unname(ks$statistic), pval = ks$p.value)
}) )

    
df <- data.frame(PCs = n_PCs, Celltype = ctFew, increment = incFew,
               enrichments, ksresults)


df %>% ggplot() + geom_jitter(aes(Celltype, ES, col = Celltype)) + ggtitle("CD8 enrichment\nfor 250:250 CD4:CD8 T cells, 1:30 PCs")

df %>% ggplot() + geom_point(aes(ES, NN_at_max, col = Celltype)) + ggtitle("Late peak for false-positive CD4s")
```










### enrichments Functions

```{r}


enrichments <- function(d, increms = c(rep(-1, 250), rep(1, 250))) {
# for a given distance matrix d of class 'dist' or 'matrix', compute nearest neighbor enrichments
     if(inherits(d, "dist")) d <- as.matrix(d)
     enr <- t(sapply(1:ncol(d), function(id) {
                do <- order(d[id, ])
                x <- cumsum(increms[do])
                return( c(ES = x[which.max(abs(x))],
                          NN_at_ES= which.max(abs(x))) )
              })
           )
     return( data.frame(enr, increments = increms) )
}

```


### 250:250

```{r}

 sqDs <- matrix(0, nrow = 500, ncol = 500)    
 res_250.250 <- tibble()
for(n_PCs in 1:500) {
 if(n_PCs %% 10 == 0) print(n_PCs)
  # add squared deviations of the current PC:
  sqDs <<- sqDs + sqDeviation(pcFew$x[, n_PCs])
  # compute enrichments:
  d <- sqrt(sqDs)
  foo <- data.frame(enrichments(d, incFew),
                    Celltype = ctFew)
  # summarize enrichment readouts for current number of PCs:
  foo <- foo %>%
    mutate(ES_sign = ifelse(ES>0, "positive", "negative")) %>% group_by(Celltype, ES_sign) %>% 
    summarise(Mean = mean(ES),
              sd=sd(ES),
              Number = n()) %>%
    mutate(PCs = n_PCs,
           correlation_ES_celltype = cor(foo$increments, foo$ES)) %>%
    select(PCs, correlation_ES_celltype, everything())
  
  res_250.250 <<- bind_rows(res_250.250, foo) 
}

 
 
 
p_250.250 <- ggplot(res_250.250) + geom_point(aes(PCs, correlation_ES_celltype))
p_250.250
p_250.250  + geom_vline(xintercept = 35) + 
             geom_vline(xintercept = 12) +
             geom_vline(xintercept = 65)


cowplot::plot_grid( 
   ggplot( data.frame( enrichments(dist(pcFew$x[, 1:12])), 
                       Celltype = ctFew) ) +
           geom_jitter(aes(Celltype, ES, col = Celltype)) + theme(legend.position="none") +
           ggtitle("PCs 1-12"),
   
   ggplot( data.frame( enrichments(dist(pcFew$x[, 1:35])), 
                       Celltype = ctFew) ) +
           geom_jitter(aes(Celltype, ES, col = Celltype)) + theme(legend.position="none") +
           ggtitle("PCs 1-35"),
   
   
   ggplot( data.frame( enrichments(dist(pcFew$x[, 1:65])), 
                       Celltype = ctFew) ) +
           geom_jitter(aes(Celltype, ES, col = Celltype)) + theme(legend.position="none") +
           ggtitle("PCs 1-65"),
   ncol = 3)
```
















# Unfinished





## CoD on Cells
Curse of dimensionality (CoD): let's take all cells, this should also make
it harder to tell CD4 and CD8 T cells apart.

```{r}
# vmr <- VMRstats(rawC )
norm3 <- log( t(t(rawC) * median(colSums(rawC)) /
                  colSums(rawC)  ) + 1 )
library(irlba)
pc <- irlba::irlba(t(scale(t(norm3[rownames(vmr), ]))),
                         nv = 40)
embeddings <- pc$v %*% diag(pc$d)


d  <- dist(embeddings[, 1:28])
tmp <- Rtsne::Rtsne(d, is_distance = TRUE)  
plot(tmp$Y, pch = 20, col = rgb(.5+.5*isCD4Tcell, .5+.5*isCD8Tcell, 0),
     asp=1)

for(np in c(1:39)) {
  print(np)
  
  d <- dist(embeddings[, 1:np])
  
  nn4 <- NNindices(as.matrix(d)[isCD4Tcell, ] )
  nn4 <- apply(nn4, 1, function(drow) sum(isCD4Tcell[drow]))
  
  nn8 <- NNindices(as.matrix(d)[isCD8Tcell, ] )
  nn8 <- apply(nn8, 1, function(drow) sum(isCD8Tcell[drow]))
 
   
  df <- rbind(data.frame(Celltype = "CD4", NNsameType = unname(nn4) ),
               data.frame(Celltype = "CD8", NNsameType = unname(nn8) ) )
  df$NNsameType =  factor(df$NNsameType, levels = 0:10)
  
  
  
  cd4 <-  data.frame(table(df[df$Celltype== "CD4",
                        "NNsameType"]), Celltype = "CD4")
  cd8 <-  data.frame(table(df[df$Celltype== "CD8",
                        "NNsameType"]), Celltype = "CD8")
  cdf <- data.frame(rbind(cd4, cd8),
             prop = c( cd4$Freq / sum(isCD4Tcell),
                       cd8$Freq / sum(isCD8Tcell) ))
  
    
  # output nice plots:
   png(paste0(
        "/home/felix/Dropbox/PhD/Meetings-Talks-etc/labM_Anders/7_hvg_vmr_CD4CD8/",
        "lognormalization_allGenes_allCells_varyPC/" ,      
        "NNs_lognormAllGenes_PCs1to", np, ".png"), width = 1000, height = 600, units = "px")
  # par(mfrow = c(1, 2)) 
  # plot(vmr$log_Gene_Mean, vmr$log_Gene_VMR, pch = 20, cex=.1, col = "#00000040",
  #      ylab = "log_VMR ( log(var/mu)", xlab = "Average expression ( log(mu) )",
  #      main = ng) 
  # points(vmr$log_Gene_Mean[1:ng], vmr$log_Gene_VMR[1:ng], pch=20, col = "red")
   
  print(ggplot(cdf) + geom_bar(aes(Var1, prop, fill = Celltype),stat = "identity", position = "dodge") + xlab("NNs of same cell type")+ ylab("Proportion") +
    ggtitle(np))
   # deprecated plot:
   # hist(ns, xlab = "CD4 T cells amongst 100 NNs", main = paste0("CD4 and CD8 T cells\n", ng),
   #      breaks = -.5+0:101, ylim = c(0, 800) )
   dev.off()
 
}
```




















## CD4 vs CD8 T cells - regress nUMIs

```{r}
library(Seurat)
s <- MakeSparse(CreateSeuratObject(rawC[, c(which(isCD4Tcell), which(isCD8Tcell))]))
s <- NormalizeData(s)
s <- FindVariableGenes(s, display.progress = F)

s <- ScaleData(object = s, vars.to.regress = c("nUMI"),
               genes.use = rownames(vmr[1:4100,]), use.umi = T,
               model.use = "negbinom", do.par = T, num.cores = 4, display.progress = T)
```


# Savepoint

```{r}
# save.image(file = "/home/felix/PhDother/scAnalysis/sc_methods/hvg/savepoint/messySeurat_s.rda")
library(ggplot2)
library(pbmcapply) # parallelization
library(quantreg)  # quantile regression with rq.wfit
library(parallelDist)
library(tidyverse)
library(Rtsne)
load(file = "/home/felix/PhDother/scAnalysis/sc_methods/hvg/savepoint/messySeurat_s.rda")
```







```{r}

for(ng in 2^(6:14)) {
  print(ng)
 seurat <- RunPCA(s, pc.genes = rownames(head(vmr, n = ng)), pcs.compute = 30,
            do.print = F)
   d    <- parallelDist::parDist(seurat@dr$pca@cell.embeddings[, 1:30],
                                  threads = 4)
 
  nns <- NNindices(as.matrix(d), n_neighbors = 11 )
  correct_assignments <- apply(nns, 1, function(ids)
    sum(identities[ids[2:11]] == identities[ids[1]]))
  cdf <- (cbind.data.frame(correct_assignments, identities))
  # below, table will note empty levels only if it's a factor:
  cdf$correct_assignments <- factor(cdf$correct_assignments, levels = 0:10)
  
  cd4 <-  data.frame(table(cdf[cdf$identities == "CD4",
                        "correct_assignments"]), Celltype = "CD4")
  cd8 <-  data.frame(table(cdf[cdf$identities == "CD8",
                        "correct_assignments"]), Celltype = "CD8")
  cdf <- data.frame(rbind(cd4, cd8),
             prop = c( cd4$Freq / sum(identities == "CD4"),
                       cd8$Freq / sum(identities == "CD8") ))
  
    
  # output nice plots:
  fn <- paste0(
        "/home/felix/Dropbox/PhD/Meetings-Talks-etc/labM_Anders/7_hvg_vmr_CD4CD8/",
        "withRegressingUMIs/",
        "NNs_PCs1to30_hvgs1to", ng, ".png")
  # png(fn, width = 1000, height = 800, units = "px")
  # par(mfrow = c(1, 2)) 
  # plot(vmr$log_Gene_Mean, vmr$log_Gene_VMR, pch = 20, cex=.1, col = "#00000040",
  #      ylab = "log_VMR ( log(var/mu)", xlab = "Average expression ( log(mu) )",
  #      main = ng) 
  # points(vmr$log_Gene_Mean[1:ng], vmr$log_Gene_VMR[1:ng], pch=20, col = "red")
   
  ggplot2::ggsave(fn, ggplot(cdf) + geom_bar(aes(Var1, prop, fill = Celltype),stat = "identity", position = "dodge") + xlab("NNs of same cell type")+ ylab("Proportion")
  )
   # deprecated plot:
   # hist(ns, xlab = "CD4 T cells amongst 100 NNs", main = paste0("CD4 and CD8 T cells\n", ng),
   #      breaks = -.5+0:101, ylim = c(0, 800) )
   #dev.off()
 
}
```































# Play zone





## Covariance of Poisson estimators
```{r}
# My VMR-over-mean plot implicitly estimates the true variance and mean of each
# gene, and the vmr as well. Sveta has come up with formulas for the variances
# of the first two estimators (mean_hat and var_hat), and from these I can compute
# the variance of the vmr estimator. This follows the formula
#
#      var(log_vmr) = var(log_var) + var(-log_mean) + 2 * cov(log_var, -log_mean)
#
# It is interesting for me to see whether the covariance of log_var and log_mean
# is 0 as hoped, or if it's above.

meanvar_correl <- do.call(rbind, lapply(exp(-5:10), function(x) {

cors <- t(replicate(5, {
  mv <- t(replicate(100, expr = {
  p <- rpois(8000, x)
  c(mean(p), var(p)) } ))
  c( lambda = x, varmean_cov = cov(log(mv[,1]), log(mv[,2]) ), var_of_var = var(log(mv[, 2])),
     var_of_mean = var(log(mv[, 1])))
}))

}) )

plot(meanvar_correl, log = "x")
```

Let's try this again with realistic values, stemming from the CITEseq dataset:
```{r}
# size factors:
s_j <- colSums(rawC[, Ts])
# means (after colsum normalization):
mu_i <- rowMeans( t( t(rawC[, Ts]) / s_j))

m <- 0.008

  p <- rpois(length(s_j), lambda = m * s_j)
  p <- p / s_j
  c(mean = mean(p), var = var(p))

```









## Sveta's variance of Poisson-variance
she derived the formula for variance of variance when we include size factors in
our poisson models. Ultimately this will be helpful in picking HVGs as we do not
have to rely on poisson simulations anymore, so here I simulate data to see whether
the formula is correct.


I simulate poisson counts:
```{r}
cs <- colSums(rawC[, Ts]) 
# estimate each gene's true mean and var - for this we normalize with library size:
estMeans <- apply(t( t(rawC[ rowMeans(rawC[, Ts]) > 0.00530, Ts]) / cs), 1, mean)
estMeans <- sample(estMeans, 1000)

# simulate poisson raw counts relevant to our data (i.e. using allM):
    nc <- ncol(rawC[, Ts])
   
    
    poisson_replicates <- replicate(100, { 
    singlePoisson <- do.call(rbind,
                           lapply(estMeans, function(mu) rpois(nc, lambda = mu * cs))
                           )
    
    apply( t(singlePoisson) / cs, 2, var)} )
    
    
    
# Sveta's variance estimation:
    var_theo <- function(mu, colsums = colSums(rawC[, Ts])) {
     N   <- length(colsums)
     psi <- sum(1/colsums) / N
     
     vt <- 2/(N-1)/(N-1) * mu^2 * psi^2 +
           (2*N-4)/(N*(N-1)^2) * mu * mu * sum(1/(colsums^2)) +
           mu / N / N * sum(1/(colsums^3))
    }


plot(apply(poisson_replicates, 1, var), var_theo(estMeans, cs), log = "xy")

all.equal(apply(poisson_replicates, 1, var), var_theo(estMeans, cs))


```

```{r}
poisson_sds <- sqrt( var_theo(exp(v$log_Gene_Mean), colSums(rawC[, Ts])) )

plot(v$log_Gene_Mean, v$log_Gene_VMR, pch = 20, cex=.1, col = "#00000090")
lines(v$log_Gene_Mean, v$log_PoissonVMR_median,
      lwd = 1.5, col = "orange")
lines(v$log_Gene_Mean, )
```


































# weighted distance

```{r}
distanceDF <- function(dist, idx1, idx2, name1, name2) {
  rbind(
    # both groups amongst themselves (only keep upper half of matrix)
    data.frame(Group = paste(name1, name1, sep = "_"),
               Distance = getUpper( as.matrix(dist)[idx1, idx1])),
    data.frame(Group = paste(name2, name2, sep = "_"),
               Distance = getUpper( as.matrix(dist)[idx2, idx2])),
    # between the two groups (keep all values)
    data.frame(Group = paste(name1, name2, sep = "_"),
               Distance = as.numeric( as.matrix(dist)[idx1, idx2]))
  )
}


histo_distance_group <- function(distanceDF){
  ggplot(distanceDF,
       aes(x = Distance, stat(density), fill = Group)) +
       geom_histogram(position = "identity", alpha = .6, binwidth = 1) }



```



Simon and I play around with this but find it's not very powerful
yet.
```{r}

mat <- rawC[, Ts]
cs  <- colSums(mat)
mat <- mat[rownames(mat) %in% names(hvgs_T$hvg_2IQR) ,]

w <- apply(mat, 1, function(k) 1 / mean(1/ (k+.5)))

plot(w, col =
(1 + rownames(mat) %in% names(hvgs_T$hvg_2IQR) ))

d <- parDist(t(sqrt(w) * log( t(t(mat + .5) / cs)) ),
               threads = 4 )

ggplot(distanceDF_Tcells(d),
       aes(x = Distance, stat(density), fill = Group)) +
       geom_histogram(position = "identity", alpha = .6, binwidth = 1)

```








# Seurat's means and disps

FindVariableGenes by default uses ExpMean and LogVMR, which effectively do this
with the lognormalized values x:

```{r}
x  <- s@data["poisson_1305", ]
expMean <-  log(
              x = mean(x = exp(x = x) - 1) + 1 )

disp    <-  log(
              x = var(x = exp(x = x) - 1) / mean(x = exp(x = x) - 1) )



xf <- pCounts["poisson_1305", ]  / cs 

```

I find this worrying.



# End of Script

```{r}
devtools::session_info()
```


