---
title: "Find Highly Variable Genes"
output:
  html_document:
    df_print: paged
    toc: yes
    number_sections: true
    toc_float: true
    self_contained: true
    mathjax: default
    code_download: true
---




# Load Packages

```{r}
library(ggplot2)
library(pbmcapply) # parallelization
library(quantreg)  # quantile regression with rq.wfit
library(parallelDist)
```



# Load data
On the CITEseq data, we play around with HVGs (highly variable genes), i.e.
superpoissonian genes.

```{r loadCITE}
citeDIR <- "~/sds/sd17l002/p/scRNAseq_datasets/CITEseq_NatMethods_2017/data/"
rna_file <- paste0(citeDIR, "GSE100866_CBMC_8K_13AB_10X-RNA_umi.csv.gz")
adt_file <- paste0(citeDIR, "GSE100866_CBMC_8K_13AB_10X-ADT_umi.csv.gz")

rawC <- as.matrix(read.csv(gzfile(rna_file), row.names = 1))
prot <- as.matrix(read.csv(gzfile(adt_file), row.names = 1))

# exclude mouse cells:
is_mouse <- colSums(rawC[grepl("MOUSE", rownames(rawC)),]) / colSums(rawC) > .1
rawC <- rawC[grepl("HUMAN", rownames(rawC)), ! is_mouse]
prot <- prot[, ! is_mouse]
```




# RNA data: normalization

```{r}
normC <- apply( rawC, 2, function(col) {
  (sqrt(col+3/8)-sqrt(3/8)) / sqrt(sum(col))
})

```




# Protein data: normalization and Cell identities
```{r}
# simplifying Seurat's code to compute Centered Log Ratio (CLR):
norm_prot <- apply(prot, 1, function(x) {
  log1p( (x) /
                          (exp(sum(log1p((x)[x > 0]), na.rm = TRUE)/length(x + 1))) ) })


 # exclude NK and monos:
  plot(norm_prot[, "CD11c"], norm_prot[, "CD56"], pch =20, cex=.1, 
       main = "CITEseqs protein data\nto exclude NKs (CD56) and myeloid cells (CD11c)")
  lin_neg <- norm_prot[, "CD56"] < 1.1   & norm_prot[, "CD11c"] < 1 
 
# exclude erythrocytes. No good protein marker available, we use RNA for it:
  not_ery <- normC["HUMAN_HBB", ]  < .05  &
             normC["HUMAN_HBG2", ] < .05 &
             normC["HUMAN_HBA1", ] < .04 
# same for platelets / megakaryocytes:
  not_platelet <- normC["HUMAN_GP9", ] < .005 &
                  normC["HUMAN_PF4", ] < .005 &
                  normC["HUMAN_PPBP", ] < .005 
  
  
# B and T cells:
  has_B_markers <- norm_prot[, "CD19"] > 2  &  norm_prot[, "CD3"] < .5
  has_T_markers <- norm_prot[, "CD19"] < 1.5  &  norm_prot[, "CD3"] > 1
  plot(norm_prot[, "CD3"], norm_prot[, "CD19"], pch =20, col =
         rgb(has_B_markers, has_T_markers, .7 *lin_neg),
       main = "B and T cells in red and green\nCD56-CD11c- in blue\nPick cells with mixture colors")

  
    
  isBcell <- has_B_markers & lin_neg & not_ery & not_platelet
  isTcell <- has_T_markers & lin_neg & not_ery & not_platelet

  
  
  has_CD4 <- norm_prot[, "CD4"] > 1  &  norm_prot[, "CD8"] < 1
  has_CD8 <- norm_prot[, "CD4"] < .5  &  norm_prot[, "CD8"] > 3
  isCD4Tcell <- isTcell & has_CD4
  isCD8Tcell <- isTcell & has_CD8
   
  plot(norm_prot[, "CD4"], norm_prot[, "CD8"], pch=20, cex=.4,
       col = rgb(0, isCD4Tcell, isCD8Tcell), 
       main = "CD4 and CD8 T cells\nselected amongst T cells picked above")  
  
  
Ts <- isCD4Tcell | isCD8Tcell
```




# VMRstats function
```{r}
## bug report for below function(s):
  #
  #  This function is terrible coding practice; rewrite it completely!
  #
  #  rowMeans(rawC) and allM is redundant computation, that's uncritical but not elegant
  #
  #  everytime you run it, different genes get selected due to stochastic poisson.
  #  We'll probably find an analytic solution eventually anyways that should fix it.

                                                  ##

VMRstats <- function(mat = rawC, nbin_qreg = 75) {
  cs <- colSums(mat) 
  
  # Simon wants to filter out low-abundance genes. A very conservative cutoff is
  # the mean resulting from 1 small cell (nUMI = 500) having an UMI of 1, the rest 0s:
  threshold <- mean(c(1/500,  rep(0, ncol(mat)-1)))
    print("Simulate Poisson Counts...")
  # estimate each gene's true mean and var - for this we normalize with library size:
    normC <- t( t(mat) / cs)
    normC <- normC[rowMeans(normC) > threshold,]
    allM <- apply(normC, 1, mean)
    allV <- apply(normC, 1, var)
  
  
  # simulate poisson raw counts relevant to our data (i.e. using allM):
    nc <- ncol(mat)
    pCounts <- do.call(rbind,
                       pbmclapply(allM, function(mu) {rpois(nc, lambda = mu * cs)},
                                  mc.cores=4) )
    # superfluous, here because I used to punch these into Seurat:
    rownames(pCounts) <- paste0("poisson_", 1:nrow(pCounts))
    colnames(pCounts) <- paste0("Cell_", 1:ncol(pCounts))
  
    print("Normalize Poisson Counts...")
  # as with real counts:
    normP <- t( t(pCounts) / cs)
    allMP <- apply(normP, 1, mean)
    allVP <- apply(normP, 1, var)
    # x,y for ease of typing. threshold also prevents -Inf values, defensive progr.:
    x <- log(allMP[allMP > threshold])
    y <- log(allVP[allMP > threshold] / allMP[allMP > threshold])
  
  print("Fit VMR-mean relationship...")
  # going over all x takes long, so we fit the VMR-mean relationship in bins:
  xrange <- range(x) # log(c(max(1e-9, min(allM), min(allMP)), max(max(allM), max(allMP))))
  bin_x <- seq(xrange[1], xrange[2], length.out = nbin_qreg)
  # random poisson by chance might not cover the extreme values:
  bin_x <- c(log(min(allM)), bin_x, log(max(allM)))
  
  # Simon's local quantile fit (faster using matrix notation, `mm`)
  mm <- cbind( Intercept = 1, X = x, X2 = x^2 )
  yfit <- do.call(rbind, pbmclapply( bin_x, function(xp)
    {
        fit        <- rq.wfit( mm, y, .75, dnorm( x, mean=xp, sd=2 ) )
        fit_median <- rq.wfit( mm, y, .5, dnorm( x, mean=xp, sd=2 ) )
        c(bin_x = xp,
          q75= fit$coefficients %*% c( 1, xp, xp^2 ),
          median = fit_median$coefficients %*% c( 1, xp, xp^2 ))
    } )  )
  
  # interpolate Poisson median and quantile for each gene: 
  all_y<- do.call(data.frame,
                 c( 
                   poisson_medians <- approx(x = yfit[, "bin_x"],
                                             y = yfit[, "median"],
                                             xout = log(allM)),
                   poisson_q75     <- approx(x = yfit[, "bin_x"],
                                             y = yfit[, "q75"],
                                             xout = log(allM)) ) )
  # summarize final result:
  df <- data.frame(
      log_Gene_Mean         = all_y[, 1],
      log_Gene_VMR          = log(allV / allM),
      log_PoissonVMR_median = all_y[, 2],
      log_PoissonVMR_q75    = all_y[, 4],
      IQR_of_logged_VMR         = 2 * (all_y[, 4] - all_y[, 2]),
      row.names = rownames(all_y)
    )
  df$aboveP  <- (df$log_Gene_VMR - df$log_PoissonVMR_median) /
    df$IQR_of_logged_VMR
  df <- df[order(df$aboveP, decreasing = TRUE), ]
  
  return(df)
} 


getHVG <- function(vmrstats = VMRstats(rawCounts), IQRs = 10) {
  # returns genes that are more than n IQRs away (default: n = 10).
  hvg  <- vmrstats$log_Gene_VMR >
         vmrstats$log_PoissonVMR_median + IQRs * vmrstats$IQR_of_logged_VMR
  return(vmrstats[hvg, ])
}
```






# Plausible gene means informed by the CITEseq dataset 
Here are raw counts typically observed for an average cell in a 10X scRNAseq dataset.
These can be used directly as Poisson rates  to simulate data, or they can be
adjusted with sizefactors around 1 (1 being the cell with average sequencing depth).
```{r}
plausible_means <- c( 
  runif(575, min = .5, max = 1),
  runif(267, min = 1,  max = 2),
  runif(190, min = 2,  max = 10),
  runif(70, min = 10, max = 40)  )
# compare to citeseq by loading rawC as in hvg.Rmd, followed by:
# cs <- colSums(rawC)
# normC <- t( t(rawC) *mean(cs)/ cs)
# table(round(rowMeans(normC)))
# rms <- rowMeans(normC)
# plot(table(round(plausible_means, digits = 1 )), main = "Simulated means", ylab = "Number of Genes")
# plot(table(round(rms[rms < 40 & rms > .5], digits = 1)), main = "CITEseq means (> .5 UMIs)", ylab="Number of Genes")
```












# Curse of dimensionality
Here I will pick more and more HVGs to show the distances within
and between CD4 and CD8 T cells become better and worse.

## Useful functions
```{r}

anscNorm <- function(rawCounts) {
      apply( rawCounts, 2, function(col) {
                     (sqrt(col+3/8)-sqrt(3/8)) / sqrt(sum(col)) } )
} 

library(irlba)
pcDist <- function(counts, npc = 30, do.fast = TRUE) {
  # computes cell-cell distances on the first 30 PCs
  # input should be normalized
  # each gene is scaled
  # the PCs are not scaled to give less and less weight to higher PCs
  if(do.fast){
    print("computing irlba")
     pc <- irlba::irlba(t(scale(t(counts))), nv = npc)
     # irlba gives normalized PCs, so I re-weight by the PC's overall variance:
     embeddings <- pc$v %*% diag(pc$d)
     d  <- dist(embeddings)
  } else {
     pc  <- prcomp((t(counts)), center = TRUE, scale. = TRUE)
     d   <- dist(pc$x[, 1:npc])
  }
  return(d) }


getUpper <- function(mat) mat[upper.tri(mat, diag = FALSE)]
  


# poor-style custom function, delete soon:  
distanceDF_Tcells <- function(dist) {
  rbind(
    data.frame(Group ="CD4_CD4",  
         Distance = getUpper( as.matrix(dist)[isCD4Tcell[Ts], isCD4Tcell[Ts]] ),
         stringsAsFactors = F),
    data.frame(Group = "CD4_CD8", 
         Distance = getUpper( as.matrix(dist)[isCD4Tcell[Ts], isCD8Tcell[Ts]] ),
         stringsAsFactors = F),
    data.frame(Group = "CD8_CD8", 
         Distance = getUpper( as.matrix(dist)[isCD8Tcell[Ts], isCD8Tcell[Ts]] ),
         stringsAsFactors = F)
   )
}

distanceDF <- function(dist, idx1, idx2, name1, name2) {
  rbind(
    # both groups amongst themselves (only keep upper half of matrix)
    data.frame(Group = paste(name1, name1, sep = "_"),
               Distance = getUpper( as.matrix(dist)[idx1, idx1])),
    data.frame(Group = paste(name2, name2, sep = "_"),
               Distance = getUpper( as.matrix(dist)[idx2, idx2])),
    # between the two groups (keep all values)
    data.frame(Group = paste(name1, name2, sep = "_"),
               Distance = as.numeric( as.matrix(dist)[idx1, idx2]))
  )
}


histo_distance_group <- function(distanceDF){
  ggplot(distanceDF,
       aes(x = Distance, stat(density), fill = Group)) +
       geom_histogram(position = "identity", alpha = .6, binwidth = 1) }




 
 sqDeviation <- function(PCvector) {
  # For a given numerical vector, returns a matrix
  # with squared deviations of amongst all its elements.
  # Taking this matrix's square root is the 1D Euclidean Distance.
  sapply(PCvector, function(entry) (entry - PCvector) * (entry - PCvector))
 }
 
# get n nearest neighbors' indices. 
 NNindices <- function(dists = as.matrix(dist(matrix(1:9,3))), n_neighbors = 10) {
  NN = t(apply(dists, 1,   
           function(ds) head(order(ds), n = n_neighbors)
             ) )
  return(NN)
 }

 
  
stratNs <- function(NN, ct) {
# Stratify neighborhoods: for each cell (row) in NN, count neighborhoods it 
# belongs to.
  # NN: is a matrix with one row per cell (neighborhood), filled with the
  #     indices of that cell's
  #     nearest neighbors (e.g. top 10, top 30, ...).
  # ct: is a character vector with the celltype of each cell, in the same order
  #     as the rows in NN.
  # value: 
  cbind(ID = 1:nrow(NN), 
    sapply(unique(ct), function(type)
      { as.numeric( table( factor( NN[ ct == type,  ], levels = 1:nrow(NN) ) ) ) 
    })
  )
}


```

## B vs T cells: all ok

```{r}
# cell types in alphabetical order (otherwise doesn't match ordered selectBT):
ctBT <- c(  rep("B", sum(isBcell)),  rep("T", 500) )
selectBT <- c(which(isBcell), sample(which(Ts), 500) )
selectBT <- selectBT[ order(ctBT, colSums(rawC[, selectBT]))]


normBT <- log( t(t(rawC[, selectBT]) * median(colSums(rawC[, selectBT])) /
                  colSums(rawC[, selectBT])  ) + 1 )

# preselect genes very coarsely for speedup in PCA:
vmrBT <- VMRstats(rawC[, selectBT])
coarse_pickBT <- rownames( vmrBT[vmrBT$log_Gene_VMR > vmrBT$log_PoissonVMR_median,] )

pcBT<- prcomp((t(normBT[ coarse_pickBT, ])), center = TRUE, scale. = FALSE)
```


```{r}
 sqDs <- matrix(0, nrow = nrow(pcBT$x), ncol = ncol(pcBT$x))    
 BT_DF <- tibble()
for(p in 1:ncol(pcBT$x)) {
 if(p %% 10 == 0) print(p)
 sqDs <<- sqDs + sqDeviation(pcBT$x[, p])
 nns  <- NNindices(as.matrix(sqrt(sqDs)), n_neighbors = 31)
 # within NNs we can now ask for interesting statistics:
 correct_assignments <- apply(nns, 1, function(ids)
      sum(ctBT[ids[2:ncol(nns)]] == ctBT[ids[1]]))
 tmp <- tibble(  correct_assignments, Celltype = ctBT,
                       PCs = p) %>% # number of PC
   group_by(Celltype, PCs) %>%
   summarise(Mean_correctNeighbors = mean(correct_assignments))  
 BT_DF <<- bind_rows(BT_DF, tmp) 
}

p_BT<- ggplot(BT_DF) + geom_point(aes(PCs, Mean_correctNeighbors, colour = Celltype))
p_BT 

BTnn10 <- NNindices( as.matrix(dist(pcBT$x[, 1:10])), n_neighbors = 31)
as.data.frame(stratNs(BTnn10, ctBT)) %>%
  gather("Ntype", "Neighborhoods", -ID) %>%
  ggplot()+geom_point(aes(ID, Neighborhoods, col = Ntype))

BTnn400 <- NNindices( as.matrix(dist(pcBT$x[, 1:400])), n_neighbors = 31)
as.data.frame(stratNs(BTnn400, ctBT)) %>%
  gather("Ntype", "Neighborhoods", -ID) %>%
  ggplot()+geom_point(aes(ID, Neighborhoods, col = Ntype))


plot( as.numeric(table(BTnn10)), pch = 20, col = 3 + (ctBT=="T"),
      xlab = "B cells (green), T cells (blue), ordered by library size",
      ylab = "Dots: N30 neighborhoods. Line: libsize (A.U.)", 
      main = "265 B cells and 500 T cells\nDistances on first 10 PCs")
lines( colSums(rawC[, selectBT]) / 100)
```













## CD4 vs CD8 T cells - no regressing

For an increasing number of genes, the below chunk computes euclidean distance
between cells. Depending on which line is commented in or out, it computes it
either directly on normalized expression or on the first 30 PCs. 
Overall we see that varying number of input genes into PCA does not change much:
```{r}
sel <- c(which(isCD4Tcell), which(isCD8Tcell))
identities <- c( rep("CD4", sum(isCD4Tcell)), rep("CD8", sum(isCD8Tcell)) )

vmr <- VMRstats(rawC[, sel] )
norm2 <- log( t(t(rawC[, sel]) * median(colSums(rawC[, sel])) /
                  colSums(rawC[, sel])  ) + 1 )



for(ng in 2^(5:14)) {
  print(ng)
  fast <- ifelse(ng > 500, TRUE, FALSE)
  #########  eucl. distance on PCs:
  # d <- pcDist(norm2[rownames(head(vmr, n = ng)), ], do.fast = fast)
  #########  eucl. distance directly on gene expression (very poor performance):
  d <- parDist(t(norm2[rownames(head(vmr, n = ng)), ]) )
  
  nns <- NNindices(as.matrix(d), n_neighbors = 11 )
  correct_assignments <- apply(nns, 1, function(ids)
    sum(identities[ids[2:11]] == identities[ids[1]]))
  cdf <- (cbind.data.frame(correct_assignments, identities))
  cdf$correct_assignments <- factor(cdf$correct_assignments, levels = 0:10)
  
  cd4 <-  data.frame(table(cdf[cdf$identities == "CD4",
                        "correct_assignments"]), Celltype = "CD4")
  cd8 <-  data.frame(table(cdf[cdf$identities == "CD8",
                        "correct_assignments"]), Celltype = "CD8")
  cdf <- data.frame(rbind(cd4, cd8),
             prop = c( cd4$Freq / sum(identities == "CD4"),
                       cd8$Freq / sum(identities == "CD8") ))
  
    
  # output nice plots:
   png(paste0(
        "/home/felix/Dropbox/PhD/Meetings-Talks-etc/labM_Anders/7_hvg_vmr_CD4CD8/",
        "lognormalization_euclidean/" ,      
        "NNs_PCs1to30_hvgs1to", ng, ".png"), width = 1000, height = 600, units = "px")
  # par(mfrow = c(1, 2)) 
  # plot(vmr$log_Gene_Mean, vmr$log_Gene_VMR, pch = 20, cex=.1, col = "#00000040",
  #      ylab = "log_VMR ( log(var/mu)", xlab = "Average expression ( log(mu) )",
  #      main = ng) 
  # points(vmr$log_Gene_Mean[1:ng], vmr$log_Gene_VMR[1:ng], pch=20, col = "red")
   
  print(ggplot(cdf) + geom_bar(aes(Var1, prop, fill = Celltype),stat = "identity", position = "dodge") + xlab("NNs of same cell type")+ ylab("Proportion") +
    ggtitle(ng))
   # deprecated plot:
   # hist(ns, xlab = "CD4 T cells amongst 100 NNs", main = paste0("CD4 and CD8 T cells\n", ng),
   #      breaks = -.5+0:101, ylim = c(0, 800) )
   dev.off()
 
}
```

## CoD on PCs (3000 cells)
Curse of dimensionality (CoD): as we increase the PCs, does euclidean distance
deteriorate?

```{r}
sel <- c(which(isCD4Tcell), which(isCD8Tcell))
identities <- c( rep("CD4", sum(isCD4Tcell)), rep("CD8", sum(isCD8Tcell)) )

vmr <- VMRstats(rawC[, sel] )
norm2 <- log( t(t(rawC[, sel]) * median(colSums(rawC[, sel])) /
                  colSums(rawC[, sel])  ) + 1 )


# compute PCA on (almost) all genes (minus exremely noisy ones, for speed):
  coarse_pick <- rownames( vmr[vmr$log_Gene_VMR > vmr$log_PoissonVMR_median,] )
  pc  <- prcomp((t(norm2[ coarse_pick, ])), center = TRUE, scale. = FALSE)
  # I used to scale and center here but scaling is debatable.
  # Note that irlba's implementation in Seurat does both.
```


```{r}
sqDs <- matrix(0, nrow = nrow(pc$x), ncol = ncol(pc$x))    
T_DF <- tibble()
for(p in 1:1000) {
 sqDs <<- sqDs + sqDeviation(pc$x[, p])

 if(p %% 50 ==0 | p < 50) { 
   print(p)
  nns  <- NNindices(as.matrix(sqrt(sqDs)), n_neighbors = 31)
  correct_assignments <- apply(nns, 1, function(ids)
      sum(identities[ids[2:ncol(nns)]] == identities[ids[1]]))
  tmp <- tibble(  correct_assignments, Celltype = identities,
                       PCs = p) %>% # number of PC
   group_by(Celltype, PCs) %>%
   summarise(Mean_correctNeighbors = mean(correct_assignments))  
 T_DF <<- bind_rows(T_DF, tmp) 
 }
}

 
p_T <- ggplot(T_DF) + geom_point(aes(PCs, Mean_correctNeighbors, colour = Celltype)) +
  ggtitle("2768 CD4 T cells, 250 CD8 T cells")
```
There are roughly 10x more CD4 
than CD8 T cells. So without any transcriptomic differences, we'd expect the
red line around 29 and the blue line around 1. So I suppose this is not bad.

```{r}
Tnn8<- NNindices( as.matrix(dist(pc$x[, 1:8])), n_neighbors = 31)
as.data.frame(stratNs(Tnn8, identities)) %>%
  gather("Ntype", "Neighborhoods", -ID) %>%
  ggplot()+geom_point(aes(ID, Neighborhoods, col = Ntype))



Tnn250<- NNindices( as.matrix(dist(pc$x[, 1:250])), n_neighbors = 31)
as.data.frame(stratNs(Tnn250, identities)) %>%
  gather("Ntype", "Neighborhoods", -ID) %>%
  ggplot()+geom_point(aes(ID, Neighborhoods, col = Ntype))
```



## CoD on PCs (250 + 250 cells)

Let's only use a few cells, so that the groups are even (250 CD4, 250 CD8 T cells):
```{r}
ctFew     <- c( rep("CD4", 250), rep("CD8", sum(isCD8Tcell)) )
selectFew <- c(sample(which(isCD4Tcell), 250), which(isCD8Tcell))
# order by celltype, then by libsize:
selectFew <- selectFew[ order(ctFew, colSums(rawC[, selectFew]), decreasing = F) ]
# the increment for the colsums:
incFew    <- c( rep(-1, 250), rep(1, 250))

# find exremely noisy genes, those we exclude in PCA for speed:
vmrFew <- VMRstats(rawC[, selectFew] )
coarse_pickFew <- rownames(
    vmrFew[ vmrFew$log_Gene_VMR > vmrFew$log_PoissonVMR_median, ] )
# normalize, then compute PCA: 
normFew <- log( t(t(rawC[, selectFew]) * median(colSums(rawC[, selectFew])) /
                  colSums(rawC[, selectFew])  ) + 1 ) 
pcFew  <- prcomp((t(normFew[ coarse_pickFew, ])), center = TRUE, scale. = FALSE)
```


### More PCs, popular neighbors and nUMI
We can see that at one point, the more PCs we include in the distance computation,
the less resolution we have to separate CD4 and CD8 T cells.
```{r}
sqDs <- matrix(0, nrow = nrow(pcFew$x), ncol = ncol(pcFew$x))    
few_DF <- tibble()
for(p in 1:500) {
 if(p %% 10 == 0) print(p)
 sqDs <<- sqDs + sqDeviation(pcFew$x[, p])
 nns  <- NNindices(as.matrix(sqrt(sqDs)), n_neighbors = 31)
 correct_assignments <- apply(nns, 1, function(ids)
      sum(ctFew[ids[2:ncol(nns)]] == ctFew[ids[1]]))
 tmp <- tibble(  correct_assignments, Celltype = ctFew,
                       PCs = p) %>% # number of PC
   group_by(Celltype, PCs) %>%
   summarise(Mean_correctNeighbors = mean(correct_assignments))  
 few_DF<<- bind_rows(few_DF, tmp) 
}

p_few <- ggplot(few_DF) + geom_point(aes(PCs, Mean_correctNeighbors, colour = Celltype))+
    ggtitle("(Almost) all genes -> PCA on 250 CD4 and 250 CD8 T cells\n-> eucl. distance on n PCs") + ylim(c(0,30))
p_few + geom_vline(xintercept = 9)  


tsneFew_9 <- Rtsne(pcFew$x[, 1:9])
plot(tsneFew_9$Y, pch =20, col = 2+ .5 * incFew, main = "250/250 CD4/CD8 T cells\nPCs 1-9")

tsneFew_25 <- Rtsne(pcFew$x[, 1:25])
plot(tsneFew_25$Y, pch =20, col = 2+ .5 * incFew, main = "250/250 CD4/CD8 T cells\nPCs 1-25")

tsneFew_200 <- Rtsne(pcFew$x[, 1:200])
plot(tsneFew_200$Y, pch =20, col = 2+ .5 * incFew, main = "250/250 CD4/CD8 T cells\nPCs 1-200")
```

So we see that including more cells gets us more noise and less resolution to
separate CD4 and CD8 T cells. I doubt however that 



## Kolmogorow-Smirnow statistics

### Idea
Let's walk up the density histograms from the left side and in a cumsum, we 
count upward when we encounter a CD8 cell and downward when we encounter a
CD4 cell.
Illustration:
```{r}
# pc is object computed in CoD on PCs
d <- dist(pcFew$x[, 1:30])


id_g = which(rownames(pcFew$x) == "GGAACTTAGGTGTGGT")
ggplot( data.frame(Celltype = ctFew[- id_g], Distance = as.matrix(d)[ id_g, -id_g]) ) + geom_histogram( aes(Distance, y = ..density.., fill = Celltype) , binwidth = 1, alpha = .4, position = "identity") + ggtitle("Good CD8 cell")

id_b = which(rownames(pcFew$x) == "CCGTTCACAGGCGATA")
ggplot( data.frame(Celltype = ctFew[- id_b], Distance = as.matrix(d)[ id_b, -id_b]) ) + geom_histogram( aes(Distance, y = ..density.., fill = Celltype) , binwidth = 1, alpha = .4, position = "identity") + ggtitle("Bad CD8 cell")
```


```{r}
n_PCs <- 30
d <- dist(pcFew$x[, 1:n_PCs])


id <- id_g
do <- order(as.matrix(d)[id, ])
x <- cumsum(incFew[do])
plot( cumsum(incFew[do]), xlab = "NN          <---->         furthest cells" , main = "CD8 enrichment for bad CD8 cell", ylim = c(0, 120))

```




```{r}
n_PCs <- 30
d <- dist(pcFew$x[, 1:n_PCs])

# we can use statistical test as well, but not sure that's sound:
ks.test(x = which(ctFew[do] == "CD8"), 
             y = which(ctFew[do] == "CD4") )

# for a given distance matrix d, compute nearest neighbor enrichments
    enrichments <- t(sapply(1:500, function(id) {
      
     do <- order(as.matrix(d)[id, ])
     x <- cumsum(incFew[do])
     return( c(ES = x[which.max(abs(x))],
               NN_at_max = which.max(abs(x))) )
     
    }))
# for a given distance matrix d, compute Kolmogorov-Smirnov test

ksresults <- t( sapply(1:500, function(id) {
    do <- order(as.matrix(d)[id, ])
    ks <- ks.test(x = which(ctFew[do] == "CD8"), 
                  y = which(ctFew[do] == "CD4") )
    c(D_statistic = unname(ks$statistic), pval = ks$p.value)
}) )

    
df <- data.frame(PCs = n_PCs, Celltype = ctFew, increment = incFew,
               enrichments, ksresults)


df %>% ggplot() + geom_jitter(aes(Celltype, ES, col = Celltype)) + ggtitle("CD8 enrichment\nfor 250:250 CD4:CD8 T cells, 1:30 PCs")

df %>% ggplot() + geom_point(aes(ES, NN_at_max, col = Celltype)) + ggtitle("Late peak for false-positive CD4s")
```










### enrichments Functions

```{r}


enrichments <- function(d, increms = c(rep(-1, 250), rep(1, 250))) {
# for a given distance matrix d of class 'dist' or 'matrix', compute nearest neighbor enrichments
     if(inherits(d, "dist")) d <- as.matrix(d)
     enr <- t(sapply(1:ncol(d), function(id) {
                do <- order(d[id, ])
                x <- cumsum(increms[do])
                return( c(ES = x[which.max(abs(x))],
                          NN_at_ES= which.max(abs(x))) )
              })
           )
     return( data.frame(enr, increments = increms) )
}

```


### 250:250

```{r}

 sqDs <- matrix(0, nrow = 500, ncol = 500)    
 res_250.250 <- tibble()
for(n_PCs in 1:500) {
 if(n_PCs %% 10 == 0) print(n_PCs)
  # add squared deviations of the current PC:
  sqDs <<- sqDs + sqDeviation(pcFew$x[, n_PCs])
  # compute enrichments:
  d <- sqrt(sqDs)
  foo <- data.frame(enrichments(d, incFew),
                    Celltype = ctFew)
  # summarize enrichment readouts for current number of PCs:
  foo <- foo %>%
    mutate(ES_sign = ifelse(ES>0, "positive", "negative")) %>% group_by(Celltype, ES_sign) %>% 
    summarise(Mean = mean(ES),
              sd=sd(ES),
              Number = n()) %>%
    mutate(PCs = n_PCs,
           correlation_ES_celltype = cor(foo$increments, foo$ES)) %>%
    select(PCs, correlation_ES_celltype, everything())
  
  res_250.250 <<- bind_rows(res_250.250, foo) 
}

 
 
 
p_250.250 <- ggplot(res_250.250) + geom_point(aes(PCs, correlation_ES_celltype))
p_250.250
p_250.250  + geom_vline(xintercept = 35) + 
             geom_vline(xintercept = 12) +
             geom_vline(xintercept = 65)


cowplot::plot_grid( 
   ggplot( data.frame( enrichments(dist(pcFew$x[, 1:12])), 
                       Celltype = ctFew) ) +
           geom_jitter(aes(Celltype, ES, col = Celltype)) + theme(legend.position="none") +
           ggtitle("PCs 1-12"),
   
   ggplot( data.frame( enrichments(dist(pcFew$x[, 1:35])), 
                       Celltype = ctFew) ) +
           geom_jitter(aes(Celltype, ES, col = Celltype)) + theme(legend.position="none") +
           ggtitle("PCs 1-35"),
   
   
   ggplot( data.frame( enrichments(dist(pcFew$x[, 1:65])), 
                       Celltype = ctFew) ) +
           geom_jitter(aes(Celltype, ES, col = Celltype)) + theme(legend.position="none") +
           ggtitle("PCs 1-65"),
   ncol = 3)
```
















# Unfinished





## CoD on Cells
Curse of dimensionality (CoD): let's take all cells, this should also make
it harder to tell CD4 and CD8 T cells apart.

```{r}
# vmr <- VMRstats(rawC )
norm3 <- log( t(t(rawC) * median(colSums(rawC)) /
                  colSums(rawC)  ) + 1 )

pc <- irlba::irlba(t(scale(t(norm3[rownames(vmr), ]))),
                         nv = 40)
embeddings <- pc$v %*% diag(pc$d)


d  <- dist(embeddings[, 1:28])
tmp <- Rtsne::Rtsne(d, is_distance = TRUE)  
plot(tmp$Y, pch = 20, col = rgb(.5+.5*isCD4Tcell, .5+.5*isCD8Tcell, 0),
     asp=1)

for(np in c(1:39)) {
  print(np)
  
  d <- dist(embeddings[, 1:np])
  
  nn4 <- NNindices(as.matrix(d)[isCD4Tcell, ] )
  nn4 <- apply(nn4, 1, function(drow) sum(isCD4Tcell[drow]))
  
  nn8 <- NNindices(as.matrix(d)[isCD8Tcell, ] )
  nn8 <- apply(nn8, 1, function(drow) sum(isCD8Tcell[drow]))
 
   
  df <- rbind(data.frame(Celltype = "CD4", NNsameType = unname(nn4) ),
               data.frame(Celltype = "CD8", NNsameType = unname(nn8) ) )
  df$NNsameType =  factor(df$NNsameType, levels = 0:10)
  
  
  
  cd4 <-  data.frame(table(df[df$Celltype== "CD4",
                        "NNsameType"]), Celltype = "CD4")
  cd8 <-  data.frame(table(df[df$Celltype== "CD8",
                        "NNsameType"]), Celltype = "CD8")
  cdf <- data.frame(rbind(cd4, cd8),
             prop = c( cd4$Freq / sum(isCD4Tcell),
                       cd8$Freq / sum(isCD8Tcell) ))
  
    
  # output nice plots:
   png(paste0(
        "/home/felix/Dropbox/PhD/Meetings-Talks-etc/labM_Anders/7_hvg_vmr_CD4CD8/",
        "lognormalization_allGenes_allCells_varyPC/" ,      
        "NNs_lognormAllGenes_PCs1to", np, ".png"), width = 1000, height = 600, units = "px")
  # par(mfrow = c(1, 2)) 
  # plot(vmr$log_Gene_Mean, vmr$log_Gene_VMR, pch = 20, cex=.1, col = "#00000040",
  #      ylab = "log_VMR ( log(var/mu)", xlab = "Average expression ( log(mu) )",
  #      main = ng) 
  # points(vmr$log_Gene_Mean[1:ng], vmr$log_Gene_VMR[1:ng], pch=20, col = "red")
   
  print(ggplot(cdf) + geom_bar(aes(Var1, prop, fill = Celltype),stat = "identity", position = "dodge") + xlab("NNs of same cell type")+ ylab("Proportion") +
    ggtitle(np))
   # deprecated plot:
   # hist(ns, xlab = "CD4 T cells amongst 100 NNs", main = paste0("CD4 and CD8 T cells\n", ng),
   #      breaks = -.5+0:101, ylim = c(0, 800) )
   dev.off()
 
}
```




















## CD4 vs CD8 T cells - regress nUMIs

```{r}
library(Seurat)
s <- MakeSparse(CreateSeuratObject(rawC[, c(which(isCD4Tcell), which(isCD8Tcell))]))
s <- NormalizeData(s)
s <- FindVariableGenes(s, display.progress = F)

s <- ScaleData(object = s, vars.to.regress = c("nUMI"),
               genes.use = rownames(vmr[1:4100,]), use.umi = T,
               model.use = "negbinom", do.par = T, num.cores = 4, display.progress = T)
```


# Savepoint

```{r}
# save.image(file = "/home/felix/PhDother/scAnalysis/sc_methods/hvg/savepoint/messySeurat_s.rda")
library(ggplot2)
library(pbmcapply) # parallelization
library(quantreg)  # quantile regression with rq.wfit
library(parallelDist)
library(tidyverse)
library(Rtsne)
load(file = "/home/felix/PhDother/scAnalysis/sc_methods/hvg/savepoint/messySeurat_s.rda")
```







```{r}

for(ng in 2^(6:14)) {
  print(ng)
 seurat <- RunPCA(s, pc.genes = rownames(head(vmr, n = ng)), pcs.compute = 30,
            do.print = F)
   d    <- parallelDist::parDist(seurat@dr$pca@cell.embeddings[, 1:30],
                                  threads = 4)
 
  nns <- NNindices(as.matrix(d), n_neighbors = 11 )
  correct_assignments <- apply(nns, 1, function(ids)
    sum(identities[ids[2:11]] == identities[ids[1]]))
  cdf <- (cbind.data.frame(correct_assignments, identities))
  # below, table will note empty levels only if it's a factor:
  cdf$correct_assignments <- factor(cdf$correct_assignments, levels = 0:10)
  
  cd4 <-  data.frame(table(cdf[cdf$identities == "CD4",
                        "correct_assignments"]), Celltype = "CD4")
  cd8 <-  data.frame(table(cdf[cdf$identities == "CD8",
                        "correct_assignments"]), Celltype = "CD8")
  cdf <- data.frame(rbind(cd4, cd8),
             prop = c( cd4$Freq / sum(identities == "CD4"),
                       cd8$Freq / sum(identities == "CD8") ))
  
    
  # output nice plots:
  fn <- paste0(
        "/home/felix/Dropbox/PhD/Meetings-Talks-etc/labM_Anders/7_hvg_vmr_CD4CD8/",
        "withRegressingUMIs/",
        "NNs_PCs1to30_hvgs1to", ng, ".png")
  # png(fn, width = 1000, height = 800, units = "px")
  # par(mfrow = c(1, 2)) 
  # plot(vmr$log_Gene_Mean, vmr$log_Gene_VMR, pch = 20, cex=.1, col = "#00000040",
  #      ylab = "log_VMR ( log(var/mu)", xlab = "Average expression ( log(mu) )",
  #      main = ng) 
  # points(vmr$log_Gene_Mean[1:ng], vmr$log_Gene_VMR[1:ng], pch=20, col = "red")
   
  ggplot2::ggsave(fn, ggplot(cdf) + geom_bar(aes(Var1, prop, fill = Celltype),stat = "identity", position = "dodge") + xlab("NNs of same cell type")+ ylab("Proportion")
  )
   # deprecated plot:
   # hist(ns, xlab = "CD4 T cells amongst 100 NNs", main = paste0("CD4 and CD8 T cells\n", ng),
   #      breaks = -.5+0:101, ylim = c(0, 800) )
   #dev.off()
 
}
```































# Play zone





## Covariance of Poisson estimators
```{r}
# My VMR-over-mean plot implicitly estimates the true variance and mean of each
# gene, and the vmr as well. Sveta has come up with formulas for the variances
# of the first two estimators (mean_hat and var_hat), and from these I can compute
# the variance of the vmr estimator. This follows the formula
#
#      var(log_vmr) = var(log_var) + var(-log_mean) + 2 * cov(log_var, -log_mean)
#
# It is interesting for me to see whether the covariance of log_var and log_mean
# is 0 as hoped, or if it's above.

meanvar_correl <- do.call(rbind, lapply(exp(-5:10), function(x) {

cors <- t(replicate(5, {
  mv <- t(replicate(100, expr = {
  p <- rpois(8000, x)
  c(mean(p), var(p)) } ))
  c( lambda = x, varmean_cov = cov(log(mv[,1]), log(mv[,2]) ), var_of_var = var(log(mv[, 2])),
     var_of_mean = var(log(mv[, 1])))
}))

}) )

plot(meanvar_correl, log = "x")
```

Let's try this again with realistic values, stemming from the CITEseq dataset:
```{r}
# size factors:
s_j <- colSums(rawC[, Ts])
# means (after colsum normalization):
mu_i <- rowMeans( t( t(rawC[, Ts]) / s_j))

m <- 0.008

  p <- rpois(length(s_j), lambda = m * s_j)
  p <- p / s_j
  c(mean = mean(p), var = var(p))

```









## Sveta's variance of Poisson-variance
she derived the formula for variance of variance when we include size factors in
our poisson models. Ultimately this will be helpful in picking HVGs as we do not
have to rely on poisson simulations anymore, so here I simulate data to see whether
the formula is correct.


I simulate poisson counts:
```{r}
cs <- colSums(rawC[, Ts]) 
# estimate each gene's true mean and var - for this we normalize with library size:
estMeans <- apply(t( t(rawC[ rowMeans(rawC[, Ts]) > 0.00530, Ts]) / cs), 1, mean)
estMeans <- sample(estMeans, 1000)

# simulate poisson raw counts relevant to our data (i.e. using allM):
    nc <- ncol(rawC[, Ts])
   
    
    poisson_replicates <- replicate(100, { 
    singlePoisson <- do.call(rbind,
                           lapply(estMeans, function(mu) rpois(nc, lambda = mu * cs))
                           )
    
    apply( t(singlePoisson) / cs, 2, var)} )
    
    
    
# Sveta's variance estimation:
    var_theo <- function(mu, colsums = colSums(rawC[, Ts])) {
     N   <- length(colsums)
     psi <- sum(1/colsums) / N
     
     vt <- 2/(N-1)/(N-1) * mu^2 * psi^2 +
           (2*N-4)/(N*(N-1)^2) * mu * mu * sum(1/(colsums^2)) +
           mu / N / N * sum(1/(colsums^3))
    }


plot(apply(poisson_replicates, 1, var), var_theo(estMeans, cs), log = "xy")

all.equal(apply(poisson_replicates, 1, var), var_theo(estMeans, cs))


```

```{r}
poisson_sds <- sqrt( var_theo(exp(v$log_Gene_Mean), colSums(rawC[, Ts])) )

plot(v$log_Gene_Mean, v$log_Gene_VMR, pch = 20, cex=.1, col = "#00000090")
lines(v$log_Gene_Mean, v$log_PoissonVMR_median,
      lwd = 1.5, col = "orange")
lines(v$log_Gene_Mean, )
```
































## seurat stuff








```{r}
s <- MakeSparse(CreateSeuratObject(rawC[, Ts]))
s <- NormalizeData(s)
s <- FindVariableGenes(s, display.progress = F)
s@var.genes <- names(hvgs_T$hvg_2IQR)
s <- ScaleData(object = s, vars.to.regress = c("nUMI"),
               genes.use = s@var.genes, use.umi = T,
               model.use = "negbinom", do.par = T, num.cores = 4, display.progress = T)
s <- RunPCA(s, pc.genes = s@var.genes, do.print = F)

d_seurat <- parallelDist::parDist(s@dr$pca@cell.embeddings[, 1:10],
                                  threads = 4)

ggplot(distanceDF_Tcells(d_seurat),
       aes(x = Distance, stat(density), fill = Group)) +
       geom_histogram(position = "identity", alpha = .6, binwidth = 1)
```






# weighted distance
Simon and I play around with this but find it's not very powerful
yet.
```{r}

mat <- rawC[, Ts]
cs  <- colSums(mat)
mat <- mat[rownames(mat) %in% names(hvgs_T$hvg_2IQR) ,]

w <- apply(mat, 1, function(k) 1 / mean(1/ (k+.5)))

plot(w, col =
(1 + rownames(mat) %in% names(hvgs_T$hvg_2IQR) ))

d <- parDist(t(sqrt(w) * log( t(t(mat + .5) / cs)) ),
               threads = 4 )

ggplot(distanceDF_Tcells(d),
       aes(x = Distance, stat(density), fill = Group)) +
       geom_histogram(position = "identity", alpha = .6, binwidth = 1)

```








# Seurat's means and disps

FindVariableGenes by default uses ExpMean and LogVMR, which effectively do this
with the lognormalized values x:

```{r}
x  <- s@data["poisson_1305", ]
expMean <-  log(
              x = mean(x = exp(x = x) - 1) + 1 )

disp    <-  log(
              x = var(x = exp(x = x) - 1) / mean(x = exp(x = x) - 1) )



xf <- pCounts["poisson_1305", ]  / cs 

```

I find this worrying.



# End of Script

```{r}
devtools::session_info()
```


