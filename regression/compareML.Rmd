---
title: "Compare machine learning algorithms"
output:
  html_document:
    df_print: paged
    toc: yes
    number_sections: true
    toc_float: true
    self_contained: true
    mathjax: default
    code_download: true
---

The ML algorithms are e.g. NBNB with Ridge / logistic / LASSO regression.

We believe Machine learning-based classification is a powerful alternative 
to clustering and could envision particularly good performance in certain
situations:

  * comparing multiple samples (lymphnodes from multiple cancer patients, etc):
    rather than making cells align in a tSNE, once classified we could compare
    them within each sample and then compare results across samples.
  * resolving 'tSNE blobs': some cell types are transcriptomically highly
    similar (e.g. naive CD4 T cells and CD8 T cells, ISCs and enteroblasts, ...),
    but telling them apart is possible when focusing on the right genes - typical
    classification problem where clustering on all genes will always be outperformed.
    

In this script, I test different machine learning techniques for point two:
how to discriminate transcriptomically highly similar cells. For that, I take
all CD4 and CD8 T cells from the CITEseq dataset chosen based on the
protein markers ( ~ groundtruth), give the same subsets of both to classifiers
and test their performance.


# Script setup

## Set CPU / Threads
```{r}
R.utils::setOption("mc.cores", 8)
library(pbmcapply)
library(parallelDist)
library(glmnet)
library(devtools) # for session_info()
```
## Classifier functions
```{r}
source("~/sc_methods/nb_sc/src/NBNB_functions.R")
sepscore <- function(dispersiontable) {
  g  = dispersiontable$gene
  mp = dispersiontable$meanPos 
  mn = dispersiontable$meanNeg
  dp = dispersiontable$dispPos
  dn = dispersiontable$dispNeg
  
  varp = mp + dp * mp * mp
  varn = mn + dn * mn * mn
  
  sepscore <- (mp - mn) / (sqrt(varp) + sqrt(varn))
  names(sepscore) <- g
 return(sepscore) 
}
```



## Load data from SDS
```{r readCSVs}
citeseqDIR <- "~/sds/sd17l002/p/scRNAseq_datasets/CITEseq_NatMethods_2017/"


rawC <- as.matrix(
        read.csv(paste0(citeseqDIR, "Seurat_workflow/Robjects/citeseq_RawCounts_2018Aug08.csv"),
                 header = T, row.names = 1))
anno <- read.csv(paste0(citeseqDIR, "Seurat_workflow/Robjects/citeseq_CellAnnotation_2018Aug08.csv"),
                 header = T, row.names = 1)

```

# The challenge

## Cell types (ground truth)
Using CITEseq protein data, I have found around 3000 cells for which I am certain
they are CD4 and CD8 T cells, respectively. I consider this the `ground truth` against
which I can compare performance of different ML algorithms. Here they are:
```{r}
allT <- anno$groundtruth_CD4T | anno$groundtruth_CD8T
rawT  <- rawC[, allT]
normT <- apply(rawT, 2, function(x) (sqrt(x + 3/8)-sqrt(3/8)) / sqrt(sum(x)))

groundtruth <- ifelse(anno$groundtruth_CD4T[allT], "CD4", "CD8")
idx_cd4 <- which(anno$groundtruth_CD4T[allT])
idx_cd8 <- which(anno$groundtruth_CD8T[allT])
```

## Training data
I set the first ML challenge as follows:
Which methods best classifies ~ 3000 naive T cells (cord blood) into CD4 T and CD8 T
when given 100 randomly selected CD4 and 50 randomly selected CD8 cells?

Note that this is an artificial example, in reality a researcher would probably
find training CD4 and CD8 T cells based on high mRNA expression of CD8, CD4, etc.,
and then use ML to classify the remaining T cells for which expression of these
markers was not high enough. 

I give more CD4 T cells here as there are 2727 CD4 but only 266 CD8 T cells - in
other words, I'm starting rather easy and see what we get.

```{r}
n_positive_cells <- 100
n_negative_cells <- 50

cd4training <- sample(idx_cd4, n_positive_cells)
cd8training <- sample(idx_cd8, n_negative_cells)

```


# NBNB
pre-select features for speedup during crossvalidation:
```{r}
# for NBNB we need a TRUE-FALSE vector
cd4trainingB <- cd8trainingB <- rep(FALSE, ncol(rawT))
cd4trainingB[cd4training] <- TRUE
cd8trainingB[cd8training] <- TRUE
dt <- trainNB(rawT, isPositive = cd4trainingB, isNegative = cd8trainingB)
dt$sepScore <- sepscore(dt)
nbgenes <- order(abs(dt$sepScore), decreasing = T)[1:7000] # nas and uninformative genes should now be excluded


```
Compute size factors:
```{r}
# we still want size factors to represent the entire count matrix to avoid artifacts:
sfT <- colSums(rawT) / mean(colSums(rawT))
```



## Crossvalidation for feature selection:


```{r}
# classification improves when we only use `n` features with highest separation
# scores. So we do k-fold crossvalidation to find ideal `n`:






validateNB <- function(n) {
  # for the n highest separating genes, compute classification Error
  # using 10-fold crossvalidation. Error = 1/10 sum(10 x Numer_WrongClassifications)
ktest_cd4 <- sample(cd4training, floor(n_positive_cells / 10))
ktest_cd8 <- sample(cd8training, floor(n_negative_cells / 10))

ktrain_cd4 <- setdiff(cd4training, ktest_cd4)
ktrain_cd8 <- setdiff(cd8training, ktest_cd8)

# NBNB needs TRUE/FALSE boolean, so we convert the indices:
ktrain_cd4B <- ktrain_cd8B <- rep(FALSE, ncol(rawT))
ktrain_cd4B[ktrain_cd4] <- TRUE
ktrain_cd8B[ktrain_cd8] <- TRUE

# we train NBNB on ktrain data:
dt <- trainNB(rawT[nbgenes, ], isPositive = ktrain_cd4B, isNegative = ktrain_cd8B,
              sf = sfT)
dt$sepScore <- sepscore(dt)

# score is computed on the n genes with highest separation scores:
topGenes <- (dt$gene[order(abs(dt$sepScore), decreasing = T)[1:n]])

# 
score <- NBNB(cbind(rawT[topGenes, ktest_cd4], rawT[topGenes, ktest_cd8]),
              dt[dt$gene %in% topGenes, ],
              sf = c(sfT[ktest_cd4], sfT[ktest_cd8] ))
predictedClass <- score > 0 # T = CD4, F = CD8
   actualClass <- groundtruth[c(ktest_cd4, ktest_cd8)] == "CD4"
wrongClassifications <- sum(predictedClass != actualClass)
return(wrongClassifications)
}


crossvalidation_NB <- data.frame()
for(n in round(10^(seq(1,  log10(7000), by=0.4  # .1 would be better
                       )))){ 
print(paste0("nGene =  ", n))
crossvalidation_NB <- rbind(crossvalidation_NB, 
                            data.frame(nGene = n,
# 10-fold crossvalidation:
average_wrongClassifications = mean(replicate(n = 10, validateNB(n = 5)))))
}
```

```{r saveNBcrossvalid, eval = F}
save.image("~/sc_methods/regression/compareML.RData")
```

```{r}
NBNB_optimum <- crossvalidation_NB$nGene[which.min(crossvalidation_NB$average_wrongClassifications)]
plot(crossvalidation_NB$nGene, crossvalidation_NB$average_wrongClassifications,
     log = "x", pch = 20, main = "10-fold crossvalidation to find optimal nGene for Feature selection\n(selecting genes with highest separation scores)")
abline(v = NBNB_optimum, col = "red")
```

## NBNB performance



```{r}
dispersionTbl <- trainNB(rawT,
                         isPositive = cd4trainingB,
                         isNegative = cd8trainingB,
                         sf = sfT)
dispersionTbl$sepScore <- sepscore(dispersionTbl)

NBtopGenes <- (dispersionTbl$gene[order(
            abs(dispersionTbl$sepScore), decreasing = T)[1:NBNB_optimum]])

# 
NBNBscore <- NBNB(rawT,
              dispersionTbl[dispersionTbl$gene %in% NBtopGenes, ],
              sf = sfT)

NBprediction <- NBNBscore > 0 # TRUE = CD4 T, FALSE = CD8 T cells
actualClass  <- groundtruth == "CD4"
table(NBprediction, actualClass)
```

We correctly assigned 2669 out of 2727 CD4 cells (97 %) and 254 out of 266 CD8 T cells (95 %).

Surprisingly, CD4 is not within the gene list NBtopGenes! GPR183 is, though.





# Ridge, the LASSO and elastig net regression


## Normalization and Filtering
Using regression requires normalized and variance-stabiliced data, so we use
Anscombe normalization:
```{r}
A          <- apply(rawT, 2, function(x) (sqrt(x + 3/8)-sqrt(3/8)) / sqrt(sum(x)))
```

Feature selection to speed up crossvalidation process:
```{r}
A_variances <- apply(A, 1, var)
Acutoff <- 1e-5
hist(A_variances, breaks=200, ylim = c(0, 2000))
abline(v=Acutoff, col = "red")

Aselect <- A_variances > Acutoff
```

## Crossvalidation for penalty selection

```{r}
# Default of nfold is to do 10-fold crossvalidation:
cv_ridge <- cv.glmnet( x = t(A[Aselect, c(cd4training, cd8training)]),
                   y = as.factor(groundtruth[ c(cd4training, cd8training)]),
                   family = "binomial",
                   alpha  = 0,   # ridge penalty only  ->  ridge regression
                   type.measure = "class" # gives misclassification error
                   )

cv_lasso <- cv.glmnet( x = t(A[Aselect, c(cd4training, cd8training)]),
                   y = as.factor(groundtruth[ c(cd4training, cd8training)]),
                   family = "binomial",
                   alpha  = 1,   # lasso penaly only -> LASSO regression
                   type.measure = "class" # gives misclassification error
                   )

plot(cv_ridge); log(cv_ridge$lambda.min); log(cv_ridge$lambda.1se)
```

```{r}
print("Features selected by lasso (stringent regularization):\n")
coef(cv_lasso, s = "lambda.1se")[coef(cv_lasso, s = "lambda.1se")[, 1] != 0, ]
coef(cv_lasso, s = "lambda.1se")[coef(cv_lasso, s = "lambda.min")[, 1] != 0, ]
```

## Performance
Qualitatively: did it assign the class correctly or not (when forced to assign all
cells).
```{r}
table(
  predict(cv_ridge, newx = t(A[Aselect, ]), s = "lambda.min", type = "class"),
  groundtruth
) 
table(
  predict(cv_ridge, newx = t(A[Aselect, ]), s = "lambda.1se", type = "class"),
  groundtruth)

table(
  predict(cv_lasso, newx = t(A[Aselect, ]), s = "lambda.min", type = "class"),
  groundtruth)
table(
  predict(cv_lasso, newx = t(A[Aselect, ]), s = "lambda.1se", type = "class"),
  groundtruth)
```

Lasso with lambda.min (not as strictly regularized as lambda.1se) performs best:
it correctly assigned 2719 out of 2727 CD4 T cells (99.7 %) and 191 out of 266 CD8 T cells (only 72 %).

## Detailed Performance

Let's look at how confident each cell is assigned - are the misclassifications
low-confidence? 

Hint: response gives the fitted probabilities - these you want to look at for misclassified cells.

## TO DO __ CONTINUE HERE






